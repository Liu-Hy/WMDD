/home/techt/anaconda3/envs/hl/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/techt/anaconda3/envs/hl/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
ipc_id =  0
Traceback (most recent call last):
  File "data_synthesis.py", line 337, in <module>
    main_syn(args)
  File "data_synthesis.py", line 251, in main_syn
    checkpoint = torch.load(args.ckpt_path)
  File "/home/techt/anaconda3/envs/hl/lib/python3.8/site-packages/torch/serialization.py", line 809, in load
    return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)
  File "/home/techt/anaconda3/envs/hl/lib/python3.8/site-packages/torch/serialization.py", line 1172, in _load
    result = unpickler.load()
  File "/home/techt/anaconda3/envs/hl/lib/python3.8/site-packages/torch/serialization.py", line 1142, in persistent_load
    typed_storage = load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))
  File "/home/techt/anaconda3/envs/hl/lib/python3.8/site-packages/torch/serialization.py", line 1116, in load_tensor
    wrap_storage=restore_location(storage, location),
  File "/home/techt/anaconda3/envs/hl/lib/python3.8/site-packages/torch/serialization.py", line 217, in default_restore_location
    result = fn(storage, location)
  File "/home/techt/anaconda3/envs/hl/lib/python3.8/site-packages/torch/serialization.py", line 182, in _cuda_deserialize
    device = validate_cuda_device(location)
  File "/home/techt/anaconda3/envs/hl/lib/python3.8/site-packages/torch/serialization.py", line 166, in validate_cuda_device
    raise RuntimeError('Attempting to deserialize object on a CUDA '
RuntimeError: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.
Traceback (most recent call last):
  File "generate_soft_label.py", line 250, in <module>
    main()
  File "generate_soft_label.py", line 116, in main
    ngpus_per_node = torch.cuda.device_count()
  File "/home/techt/anaconda3/envs/hl/lib/python3.8/site-packages/torch/cuda/__init__.py", line 650, in device_count
    nvml_count = _device_count_nvml()
  File "/home/techt/anaconda3/envs/hl/lib/python3.8/site-packages/torch/cuda/__init__.py", line 619, in _device_count_nvml
    visible_devices = _parse_visible_devices()
  File "/home/techt/anaconda3/envs/hl/lib/python3.8/site-packages/torch/cuda/__init__.py", line 528, in _parse_visible_devices
    x = _strtoul(elem.strip())
  File "/home/techt/anaconda3/envs/hl/lib/python3.8/site-packages/torch/cuda/__init__.py", line 506, in _strtoul
    return int(s[:idx]) if idx > 0 else -1
ValueError: invalid literal for int() with base 10: '-'
W&B disabled.
W&B online. Running your script from this directory will now sync to the cloud.
wandb: Currently logged in as: hl57. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.12 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.10
wandb: Run data is saved locally in /media/techt/One Touch/DD/SRe2L/train/wandb/run-20231005_205750-t507l7i4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run tough-serenity-386
wandb: ‚≠êÔ∏è View project at https://wandb.ai/hl57/final_rn18_fkd
wandb: üöÄ View run at https://wandb.ai/hl57/final_rn18_fkd/runs/t507l7i4
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: üöÄ View run tough-serenity-386 at: https://wandb.ai/hl57/final_rn18_fkd/runs/t507l7i4
wandb: Ô∏è‚ö° View job at https://wandb.ai/hl57/final_rn18_fkd/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjk3MzkyNjU3/version_details/v37
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231005_205750-t507l7i4/logs
Traceback (most recent call last):
  File "train_FKD.py", line 390, in <module>
    main()
  File "train_FKD.py", line 99, in main
    raise Exception("need gpu to train!")
Exception: need gpu to train!
