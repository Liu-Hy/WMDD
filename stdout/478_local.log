r_bn:  500.0
lr:  0.1
bc loaded
bc shape (200, 10, 512)
Computing BatchNorm statistics for class 0
Computing BatchNorm statistics for class 1
Computing BatchNorm statistics for class 2
Computing BatchNorm statistics for class 3
Computing BatchNorm statistics for class 4
Computing BatchNorm statistics for class 5
Computing BatchNorm statistics for class 6
Computing BatchNorm statistics for class 7
Computing BatchNorm statistics for class 8
Computing BatchNorm statistics for class 9
Computing BatchNorm statistics for class 10
Computing BatchNorm statistics for class 11
Computing BatchNorm statistics for class 12
Computing BatchNorm statistics for class 13
Computing BatchNorm statistics for class 14
Computing BatchNorm statistics for class 15
Computing BatchNorm statistics for class 16
Computing BatchNorm statistics for class 17
Computing BatchNorm statistics for class 18
Computing BatchNorm statistics for class 19
Computing BatchNorm statistics for class 20
Computing BatchNorm statistics for class 21
Computing BatchNorm statistics for class 22
Computing BatchNorm statistics for class 23
Computing BatchNorm statistics for class 24
Computing BatchNorm statistics for class 25
Computing BatchNorm statistics for class 26
Computing BatchNorm statistics for class 27
Computing BatchNorm statistics for class 28
Computing BatchNorm statistics for class 29
Computing BatchNorm statistics for class 30
Computing BatchNorm statistics for class 31
Computing BatchNorm statistics for class 32
Computing BatchNorm statistics for class 33
Computing BatchNorm statistics for class 34
Computing BatchNorm statistics for class 35
Computing BatchNorm statistics for class 36
Computing BatchNorm statistics for class 37
Computing BatchNorm statistics for class 38
Computing BatchNorm statistics for class 39
Computing BatchNorm statistics for class 40
Computing BatchNorm statistics for class 41
Computing BatchNorm statistics for class 42
Computing BatchNorm statistics for class 43
Computing BatchNorm statistics for class 44
Computing BatchNorm statistics for class 45
Computing BatchNorm statistics for class 46
Computing BatchNorm statistics for class 47
Computing BatchNorm statistics for class 48
Computing BatchNorm statistics for class 49
Computing BatchNorm statistics for class 50
Computing BatchNorm statistics for class 51
Computing BatchNorm statistics for class 52
Computing BatchNorm statistics for class 53
Computing BatchNorm statistics for class 54
Computing BatchNorm statistics for class 55
Computing BatchNorm statistics for class 56
Computing BatchNorm statistics for class 57
Computing BatchNorm statistics for class 58
Computing BatchNorm statistics for class 59
Computing BatchNorm statistics for class 60
Computing BatchNorm statistics for class 61
Computing BatchNorm statistics for class 62
Computing BatchNorm statistics for class 63
Computing BatchNorm statistics for class 64
Computing BatchNorm statistics for class 65
Computing BatchNorm statistics for class 66
Computing BatchNorm statistics for class 67
Computing BatchNorm statistics for class 68
Computing BatchNorm statistics for class 69
Computing BatchNorm statistics for class 70
Computing BatchNorm statistics for class 71
Computing BatchNorm statistics for class 72
Computing BatchNorm statistics for class 73
Computing BatchNorm statistics for class 74
Computing BatchNorm statistics for class 75
Computing BatchNorm statistics for class 76
Computing BatchNorm statistics for class 77
Computing BatchNorm statistics for class 78
Computing BatchNorm statistics for class 79
Computing BatchNorm statistics for class 80
Computing BatchNorm statistics for class 81
Computing BatchNorm statistics for class 82
Computing BatchNorm statistics for class 83
Computing BatchNorm statistics for class 84
Computing BatchNorm statistics for class 85
Computing BatchNorm statistics for class 86
Computing BatchNorm statistics for class 87
Computing BatchNorm statistics for class 88
Computing BatchNorm statistics for class 89
Computing BatchNorm statistics for class 90
Computing BatchNorm statistics for class 91
Computing BatchNorm statistics for class 92
Computing BatchNorm statistics for class 93
Computing BatchNorm statistics for class 94
Computing BatchNorm statistics for class 95
Computing BatchNorm statistics for class 96
Computing BatchNorm statistics for class 97
Computing BatchNorm statistics for class 98
Computing BatchNorm statistics for class 99
Computing BatchNorm statistics for class 100
Computing BatchNorm statistics for class 101
Computing BatchNorm statistics for class 102
Computing BatchNorm statistics for class 103
Computing BatchNorm statistics for class 104
Computing BatchNorm statistics for class 105
Computing BatchNorm statistics for class 106
Computing BatchNorm statistics for class 107
Computing BatchNorm statistics for class 108
Computing BatchNorm statistics for class 109
Computing BatchNorm statistics for class 110
Computing BatchNorm statistics for class 111
Computing BatchNorm statistics for class 112
Computing BatchNorm statistics for class 113
Computing BatchNorm statistics for class 114
Computing BatchNorm statistics for class 115
Computing BatchNorm statistics for class 116
Computing BatchNorm statistics for class 117
Computing BatchNorm statistics for class 118
Computing BatchNorm statistics for class 119
Computing BatchNorm statistics for class 120
Computing BatchNorm statistics for class 121
Computing BatchNorm statistics for class 122
Computing BatchNorm statistics for class 123
Computing BatchNorm statistics for class 124
Computing BatchNorm statistics for class 125
Computing BatchNorm statistics for class 126
Computing BatchNorm statistics for class 127
Computing BatchNorm statistics for class 128
Computing BatchNorm statistics for class 129
Computing BatchNorm statistics for class 130
Computing BatchNorm statistics for class 131
Computing BatchNorm statistics for class 132
Computing BatchNorm statistics for class 133
Computing BatchNorm statistics for class 134
Computing BatchNorm statistics for class 135
Computing BatchNorm statistics for class 136
Computing BatchNorm statistics for class 137
Computing BatchNorm statistics for class 138
Computing BatchNorm statistics for class 139
Computing BatchNorm statistics for class 140
Computing BatchNorm statistics for class 141
Computing BatchNorm statistics for class 142
Computing BatchNorm statistics for class 143
Computing BatchNorm statistics for class 144
Computing BatchNorm statistics for class 145
Computing BatchNorm statistics for class 146
Computing BatchNorm statistics for class 147
Computing BatchNorm statistics for class 148
Computing BatchNorm statistics for class 149
Computing BatchNorm statistics for class 150
Computing BatchNorm statistics for class 151
Computing BatchNorm statistics for class 152
Computing BatchNorm statistics for class 153
Computing BatchNorm statistics for class 154
Computing BatchNorm statistics for class 155
Computing BatchNorm statistics for class 156
Computing BatchNorm statistics for class 157
Computing BatchNorm statistics for class 158
Computing BatchNorm statistics for class 159
Computing BatchNorm statistics for class 160
Computing BatchNorm statistics for class 161
Computing BatchNorm statistics for class 162
Computing BatchNorm statistics for class 163
Computing BatchNorm statistics for class 164
Computing BatchNorm statistics for class 165
Computing BatchNorm statistics for class 166
Computing BatchNorm statistics for class 167
Computing BatchNorm statistics for class 168
Computing BatchNorm statistics for class 169
Computing BatchNorm statistics for class 170
Computing BatchNorm statistics for class 171
Computing BatchNorm statistics for class 172
Computing BatchNorm statistics for class 173
Computing BatchNorm statistics for class 174
Computing BatchNorm statistics for class 175
Computing BatchNorm statistics for class 176
Computing BatchNorm statistics for class 177
Computing BatchNorm statistics for class 178
Computing BatchNorm statistics for class 179
Computing BatchNorm statistics for class 180
Computing BatchNorm statistics for class 181
Computing BatchNorm statistics for class 182
Computing BatchNorm statistics for class 183
Computing BatchNorm statistics for class 184
Computing BatchNorm statistics for class 185
Computing BatchNorm statistics for class 186
Computing BatchNorm statistics for class 187
Computing BatchNorm statistics for class 188
Computing BatchNorm statistics for class 189
Computing BatchNorm statistics for class 190
Computing BatchNorm statistics for class 191
Computing BatchNorm statistics for class 192
Computing BatchNorm statistics for class 193
Computing BatchNorm statistics for class 194
Computing BatchNorm statistics for class 195
Computing BatchNorm statistics for class 196
Computing BatchNorm statistics for class 197
Computing BatchNorm statistics for class 198
Computing BatchNorm statistics for class 199
/home/techt/anaconda3/envs/hl/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/techt/anaconda3/envs/hl/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
/home/techt/anaconda3/envs/hl/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
get_images call
class_per_batch  10 batch_size  100 args.ipc  10
------------iteration 0----------
total loss 118194.92419099627
main criterion 135.05700349627026
weighted_aux_loss 118059.8671875
loss_r_bn_feature 236.11973571777344
------------iteration 100----------
total loss 51001.89309539715
main criterion 90.54934539715018
weighted_aux_loss 50911.34375
loss_r_bn_feature 101.82268524169922
------------iteration 200----------
total loss 39049.15951315419
main criterion 80.89779440418921
weighted_aux_loss 38968.26171875
loss_r_bn_feature 77.9365234375
------------iteration 300----------
total loss 69158.85966717733
main criterion 112.36747967732062
weighted_aux_loss 69046.4921875
loss_r_bn_feature 138.09298706054688
------------iteration 400----------
total loss 56747.910264724764
main criterion 103.22276472476362
weighted_aux_loss 56644.6875
loss_r_bn_feature 113.28937530517578
------------iteration 500----------
total loss 27489.224335510644
main criterion 80.41378863564378
weighted_aux_loss 27408.810546875
loss_r_bn_feature 54.81761932373047
------------iteration 600----------
total loss 67302.33894645702
main criterion 123.44832145702685
weighted_aux_loss 67178.890625
loss_r_bn_feature 134.3577880859375
------------iteration 700----------
total loss 35469.19532812216
main criterion 83.96095312215459
weighted_aux_loss 35385.234375
loss_r_bn_feature 70.77046966552734
------------iteration 800----------
total loss 36040.815898292094
main criterion 86.7807420420974
weighted_aux_loss 35954.03515625
loss_r_bn_feature 71.90807342529297
------------iteration 900----------
total loss 21872.95989159724
main criterion 76.82512597223905
weighted_aux_loss 21796.134765625
loss_r_bn_feature 43.59226989746094
------------iteration 1000----------
total loss 18233.871595971174
main criterion 78.40284597117301
weighted_aux_loss 18155.46875
loss_r_bn_feature 36.310935974121094
------------iteration 1100----------
total loss 16831.931835724143
main criterion 78.49238259914112
weighted_aux_loss 16753.439453125
loss_r_bn_feature 33.50687789916992
------------iteration 1200----------
total loss 14531.364167300304
main criterion 76.26846417530415
weighted_aux_loss 14455.095703125
loss_r_bn_feature 28.91019058227539
------------iteration 1300----------
total loss 18813.13953434931
main criterion 74.08289372430961
weighted_aux_loss 18739.056640625
loss_r_bn_feature 37.47811508178711
------------iteration 1400----------
total loss 15783.910086064314
main criterion 76.4618438768142
weighted_aux_loss 15707.4482421875
loss_r_bn_feature 31.41489601135254
------------iteration 1500----------
total loss 14707.672859353828
main criterion 75.3056718538275
weighted_aux_loss 14632.3671875
loss_r_bn_feature 29.264734268188477
------------iteration 1600----------
total loss 13279.29794602142
main criterion 75.19638352141843
weighted_aux_loss 13204.1015625
loss_r_bn_feature 26.408203125
------------iteration 1700----------
total loss 13937.795280156293
main criterion 75.75817078129263
weighted_aux_loss 13862.037109375
loss_r_bn_feature 27.72407341003418
------------iteration 1800----------
total loss 13363.792461570472
main criterion 75.50632875797308
weighted_aux_loss 13288.2861328125
loss_r_bn_feature 26.57657241821289
------------iteration 1900----------
total loss 32266.3808798589
main criterion 89.80275485890337
weighted_aux_loss 32176.578125
loss_r_bn_feature 64.35315704345703
------------iteration 0----------
total loss 118906.5372847904
main criterion 130.67009729039677
weighted_aux_loss 118775.8671875
loss_r_bn_feature 237.55172729492188
------------iteration 100----------
total loss 40205.01394440806
main criterion 78.64285065806
weighted_aux_loss 40126.37109375
loss_r_bn_feature 80.25273895263672
------------iteration 200----------
total loss 75312.36280033036
main criterion 107.07373783035473
weighted_aux_loss 75205.2890625
loss_r_bn_feature 150.41058349609375
------------iteration 300----------
total loss 64672.47714966087
main criterion 103.18808716086879
weighted_aux_loss 64569.2890625
loss_r_bn_feature 129.13858032226562
------------iteration 400----------
total loss 28942.641909992584
main criterion 76.51495686758389
weighted_aux_loss 28866.126953125
loss_r_bn_feature 57.73225402832031
------------iteration 500----------
total loss 29364.50563593901
main criterion 78.13844843901107
weighted_aux_loss 29286.3671875
loss_r_bn_feature 58.57273483276367
------------iteration 600----------
total loss 27310.25612964342
main criterion 77.3674577684229
weighted_aux_loss 27232.888671875
loss_r_bn_feature 54.46577835083008
------------iteration 700----------
total loss 22096.464601563835
main criterion 79.91186718883375
weighted_aux_loss 22016.552734375
loss_r_bn_feature 44.033103942871094
------------iteration 800----------
total loss 20999.424587478403
main criterion 75.67263435340287
weighted_aux_loss 20923.751953125
loss_r_bn_feature 41.847503662109375
------------iteration 900----------
total loss 18119.238629628584
main criterion 76.59995775358348
weighted_aux_loss 18042.638671875
loss_r_bn_feature 36.08527755737305
------------iteration 1000----------
total loss 29400.18777873527
main criterion 82.9846537352712
weighted_aux_loss 29317.203125
loss_r_bn_feature 58.63440704345703
------------iteration 1100----------
total loss 22359.471912130473
main criterion 82.91331838047459
weighted_aux_loss 22276.55859375
loss_r_bn_feature 44.55311584472656
------------iteration 1200----------
total loss 15601.810920242895
main criterion 75.97009993039437
weighted_aux_loss 15525.8408203125
loss_r_bn_feature 31.051681518554688
------------iteration 1300----------
total loss 37588.311531263775
main criterion 88.19434376377662
weighted_aux_loss 37500.1171875
loss_r_bn_feature 75.00023651123047
------------iteration 1400----------
total loss 26580.94838215151
main criterion 87.91713215151111
weighted_aux_loss 26493.03125
loss_r_bn_feature 52.986061096191406
------------iteration 1500----------
total loss 15979.849205766186
main criterion 78.19881514118563
weighted_aux_loss 15901.650390625
loss_r_bn_feature 31.803300857543945
------------iteration 1600----------
total loss 12424.711729580542
main criterion 74.76055770554113
weighted_aux_loss 12349.951171875
loss_r_bn_feature 24.699901580810547
------------iteration 1700----------
total loss 11127.0230080509
main criterion 75.37066430090059
weighted_aux_loss 11051.65234375
loss_r_bn_feature 22.103303909301758
------------iteration 1800----------
total loss 14130.515470820837
main criterion 74.86117394583768
weighted_aux_loss 14055.654296875
loss_r_bn_feature 28.111309051513672
------------iteration 1900----------
total loss 14915.135168602026
main criterion 78.61661391452547
weighted_aux_loss 14836.5185546875
loss_r_bn_feature 29.673036575317383
------------iteration 0----------
total loss 127002.89753103824
main criterion 156.2256560382321
weighted_aux_loss 126846.671875
loss_r_bn_feature 253.69334411621094
------------iteration 100----------
total loss 84224.86186295784
main criterion 117.50248795784016
weighted_aux_loss 84107.359375
loss_r_bn_feature 168.2147216796875
------------iteration 200----------
total loss 38171.25191461585
main criterion 77.64253961585344
weighted_aux_loss 38093.609375
loss_r_bn_feature 76.18721771240234
------------iteration 300----------
total loss 31215.25621882279
main criterion 79.82067194778946
weighted_aux_loss 31135.435546875
loss_r_bn_feature 62.270870208740234
------------iteration 400----------
total loss 31661.63258726316
main criterion 87.54079038816045
weighted_aux_loss 31574.091796875
loss_r_bn_feature 63.1481819152832
------------iteration 500----------
total loss 24449.00677578243
main criterion 79.76068203243031
weighted_aux_loss 24369.24609375
loss_r_bn_feature 48.73849105834961
------------iteration 600----------
total loss 28341.9161051997
main criterion 85.24618332469977
weighted_aux_loss 28256.669921875
loss_r_bn_feature 56.51333999633789
------------iteration 700----------
total loss 30637.55798462745
main criterion 84.87243775244981
weighted_aux_loss 30552.685546875
loss_r_bn_feature 61.105369567871094
------------iteration 800----------
total loss 18461.42174209327
main criterion 75.16783584326862
weighted_aux_loss 18386.25390625
loss_r_bn_feature 36.77250671386719
------------iteration 900----------
total loss 20531.01161165095
main criterion 78.95106477595009
weighted_aux_loss 20452.060546875
loss_r_bn_feature 40.90412139892578
------------iteration 1000----------
total loss 16149.102871876554
main criterion 77.91830156405419
weighted_aux_loss 16071.1845703125
loss_r_bn_feature 32.14236831665039
------------iteration 1100----------
total loss 45366.06363897304
main criterion 104.09098272304432
weighted_aux_loss 45261.97265625
loss_r_bn_feature 90.5239486694336
------------iteration 1200----------
total loss 12368.66093121324
main criterion 80.31229840074067
weighted_aux_loss 12288.3486328125
loss_r_bn_feature 24.576696395874023
------------iteration 1300----------
total loss 13297.999357646784
main criterion 76.46517795928398
weighted_aux_loss 13221.5341796875
loss_r_bn_feature 26.44306755065918
------------iteration 1400----------
total loss 15315.701044845793
main criterion 83.457880783294
weighted_aux_loss 15232.2431640625
loss_r_bn_feature 30.464487075805664
------------iteration 1500----------
total loss 11464.220796400166
main criterion 78.38681202516605
weighted_aux_loss 11385.833984375
loss_r_bn_feature 22.77166748046875
------------iteration 1600----------
total loss 12039.500497046663
main criterion 78.29151267166338
weighted_aux_loss 11961.208984375
loss_r_bn_feature 23.92241859436035
------------iteration 1700----------
total loss 32881.05315420088
main criterion 93.3734667008835
weighted_aux_loss 32787.6796875
loss_r_bn_feature 65.57535552978516
------------iteration 1800----------
total loss 12708.883213924319
main criterion 80.05801861181895
weighted_aux_loss 12628.8251953125
loss_r_bn_feature 25.25765037536621
------------iteration 1900----------
total loss 25683.48228863443
main criterion 88.49986675942866
weighted_aux_loss 25594.982421875
loss_r_bn_feature 51.189964294433594
------------iteration 0----------
total loss 115563.53032845397
main criterion 136.5928284539735
weighted_aux_loss 115426.9375
loss_r_bn_feature 230.8538818359375
------------iteration 100----------
total loss 37084.535359832116
main criterion 80.85957858211891
weighted_aux_loss 37003.67578125
loss_r_bn_feature 74.00735473632812
------------iteration 200----------
total loss 41439.53329719105
main criterion 77.29892219104896
weighted_aux_loss 41362.234375
loss_r_bn_feature 82.72447204589844
------------iteration 300----------
total loss 28737.44551029252
main criterion 78.04316654252109
weighted_aux_loss 28659.40234375
loss_r_bn_feature 57.31880569458008
------------iteration 400----------
total loss 32962.994802953974
main criterion 79.29949045397063
weighted_aux_loss 32883.6953125
loss_r_bn_feature 65.76738739013672
------------iteration 500----------
total loss 23444.867007547906
main criterion 80.10528879790593
weighted_aux_loss 23364.76171875
loss_r_bn_feature 46.729522705078125
------------iteration 600----------
total loss 23064.153667315135
main criterion 77.47007356513429
weighted_aux_loss 22986.68359375
loss_r_bn_feature 45.973365783691406
------------iteration 700----------
total loss 19434.862621434724
main criterion 77.47004330972321
weighted_aux_loss 19357.392578125
loss_r_bn_feature 38.714786529541016
------------iteration 800----------
total loss 19866.586523644095
main criterion 80.3990236440939
weighted_aux_loss 19786.1875
loss_r_bn_feature 39.5723762512207
------------iteration 900----------
total loss 16182.737551998503
main criterion 78.99145824850315
weighted_aux_loss 16103.74609375
loss_r_bn_feature 32.20749282836914
------------iteration 1000----------
total loss 41468.780154135384
main criterion 97.65906038538719
weighted_aux_loss 41371.12109375
loss_r_bn_feature 82.74224090576172
------------iteration 1100----------
total loss 18198.420949497668
main criterion 83.66899637266853
weighted_aux_loss 18114.751953125
loss_r_bn_feature 36.2295036315918
------------iteration 1200----------
total loss 13937.093873860087
main criterion 77.04797542258744
weighted_aux_loss 13860.0458984375
loss_r_bn_feature 27.720090866088867
------------iteration 1300----------
total loss 13847.208972805243
main criterion 78.30076968024352
weighted_aux_loss 13768.908203125
loss_r_bn_feature 27.537817001342773
------------iteration 1400----------
total loss 45434.08296043595
main criterion 101.65327293595615
weighted_aux_loss 45332.4296875
loss_r_bn_feature 90.66485595703125
------------iteration 1500----------
total loss 37005.56386608282
main criterion 95.46230358281925
weighted_aux_loss 36910.1015625
loss_r_bn_feature 73.82020568847656
------------iteration 1600----------
total loss 19174.880430187422
main criterion 84.17925831242114
weighted_aux_loss 19090.701171875
loss_r_bn_feature 38.18140411376953
------------iteration 1700----------
total loss 11430.984752552955
main criterion 77.03748692795476
weighted_aux_loss 11353.947265625
loss_r_bn_feature 22.707895278930664
------------iteration 1800----------
total loss 18150.46711228141
main criterion 82.96515915641184
weighted_aux_loss 18067.501953125
loss_r_bn_feature 36.13500213623047
------------iteration 1900----------
total loss 19060.87981900372
main criterion 85.33880337871851
weighted_aux_loss 18975.541015625
loss_r_bn_feature 37.951080322265625
------------iteration 0----------
total loss 116893.10941547854
main criterion 131.06254047853201
weighted_aux_loss 116762.046875
loss_r_bn_feature 233.5240936279297
------------iteration 100----------
total loss 55343.77497289553
main criterion 91.3296603955324
weighted_aux_loss 55252.4453125
loss_r_bn_feature 110.50489044189453
------------iteration 200----------
total loss 36505.121066617954
main criterion 77.01950411795441
weighted_aux_loss 36428.1015625
loss_r_bn_feature 72.856201171875
------------iteration 300----------
total loss 30093.465424971775
main criterion 76.81503434677505
weighted_aux_loss 30016.650390625
loss_r_bn_feature 60.033302307128906
------------iteration 400----------
total loss 50258.0859243469
main criterion 90.51170559690235
weighted_aux_loss 50167.57421875
loss_r_bn_feature 100.33515167236328
------------iteration 500----------
total loss 25222.361263597206
main criterion 79.44329484720414
weighted_aux_loss 25142.91796875
loss_r_bn_feature 50.28583526611328
------------iteration 600----------
total loss 21829.087634143012
main criterion 75.73411851801276
weighted_aux_loss 21753.353515625
loss_r_bn_feature 43.50670623779297
------------iteration 700----------
total loss 19992.75840674168
main criterion 74.05528174167758
weighted_aux_loss 19918.703125
loss_r_bn_feature 39.837406158447266
------------iteration 800----------
total loss 21004.236882524074
main criterion 77.25055439907254
weighted_aux_loss 20926.986328125
loss_r_bn_feature 41.853973388671875
------------iteration 900----------
total loss 18040.82617989686
main criterion 74.85938302185832
weighted_aux_loss 17965.966796875
loss_r_bn_feature 35.93193435668945
------------iteration 1000----------
total loss 16126.7637379312
main criterion 75.79010511870018
weighted_aux_loss 16050.9736328125
loss_r_bn_feature 32.10194778442383
------------iteration 1100----------
total loss 15955.036876291986
main criterion 75.43922004198592
weighted_aux_loss 15879.59765625
loss_r_bn_feature 31.75919532775879
------------iteration 1200----------
total loss 17243.167710534854
main criterion 76.39231990985385
weighted_aux_loss 17166.775390625
loss_r_bn_feature 34.33354949951172
------------iteration 1300----------
total loss 15176.69572232128
main criterion 76.14787075877989
weighted_aux_loss 15100.5478515625
loss_r_bn_feature 30.201095581054688
------------iteration 1400----------
total loss 21473.99190478359
main criterion 88.0133891585919
weighted_aux_loss 21385.978515625
loss_r_bn_feature 42.77195739746094
------------iteration 1500----------
total loss 13499.423612200611
main criterion 77.05642470061228
weighted_aux_loss 13422.3671875
loss_r_bn_feature 26.84473419189453
------------iteration 1600----------
total loss 12235.601860107514
main criterion 76.18389135751404
weighted_aux_loss 12159.41796875
loss_r_bn_feature 24.318836212158203
------------iteration 1700----------
total loss 16129.022901032906
main criterion 80.24458072040508
weighted_aux_loss 16048.7783203125
loss_r_bn_feature 32.097557067871094
------------iteration 1800----------
total loss 16701.48253881968
main criterion 78.13878881968046
weighted_aux_loss 16623.34375
loss_r_bn_feature 33.24668884277344
------------iteration 1900----------
total loss 10674.307790804192
main criterion 76.02947049169319
weighted_aux_loss 10598.2783203125
loss_r_bn_feature 21.196556091308594
------------iteration 0----------
total loss 111604.56762913078
main criterion 135.74731663078265
weighted_aux_loss 111468.8203125
loss_r_bn_feature 222.93763732910156
------------iteration 100----------
total loss 37985.527433766576
main criterion 80.66024626657273
weighted_aux_loss 37904.8671875
loss_r_bn_feature 75.80973815917969
------------iteration 200----------
total loss 42175.25121230579
main criterion 84.57543105578695
weighted_aux_loss 42090.67578125
loss_r_bn_feature 84.18135070800781
------------iteration 300----------
total loss 29539.019347563837
main criterion 86.27520693883699
weighted_aux_loss 29452.744140625
loss_r_bn_feature 58.905487060546875
------------iteration 400----------
total loss 29902.12976290596
main criterion 81.45984103095797
weighted_aux_loss 29820.669921875
loss_r_bn_feature 59.64133834838867
------------iteration 500----------
total loss 29080.743452356626
main criterion 78.23173360662744
weighted_aux_loss 29002.51171875
loss_r_bn_feature 58.00502395629883
------------iteration 600----------
total loss 21095.61864794861
main criterion 78.33544482360912
weighted_aux_loss 21017.283203125
loss_r_bn_feature 42.03456497192383
------------iteration 700----------
total loss 22800.314572152092
main criterion 81.0216034020934
weighted_aux_loss 22719.29296875
loss_r_bn_feature 45.4385871887207
------------iteration 800----------
total loss 20860.133743196206
main criterion 77.09077444620581
weighted_aux_loss 20783.04296875
loss_r_bn_feature 41.56608581542969
------------iteration 900----------
total loss 19637.402382776552
main criterion 81.11917965155132
weighted_aux_loss 19556.283203125
loss_r_bn_feature 39.11256790161133
------------iteration 1000----------
total loss 16315.76830194641
main criterion 77.40892694641012
weighted_aux_loss 16238.359375
loss_r_bn_feature 32.47671890258789
------------iteration 1100----------
total loss 18323.469774707923
main criterion 82.70219658292211
weighted_aux_loss 18240.767578125
loss_r_bn_feature 36.481536865234375
------------iteration 1200----------
total loss 14710.060061873106
main criterion 78.27978843560598
weighted_aux_loss 14631.7802734375
loss_r_bn_feature 29.263561248779297
------------iteration 1300----------
total loss 16656.47993342599
main criterion 85.15571467599274
weighted_aux_loss 16571.32421875
loss_r_bn_feature 33.14264678955078
------------iteration 1400----------
total loss 15171.710504417528
main criterion 78.96441066752718
weighted_aux_loss 15092.74609375
loss_r_bn_feature 30.18549156188965
------------iteration 1500----------
total loss 12424.620391791052
main criterion 75.84402460355213
weighted_aux_loss 12348.7763671875
loss_r_bn_feature 24.697553634643555
------------iteration 1600----------
total loss 18185.819187225276
main criterion 83.61215597527486
weighted_aux_loss 18102.20703125
loss_r_bn_feature 36.20441436767578
------------iteration 1700----------
total loss 15609.143282416306
main criterion 77.42746210380656
weighted_aux_loss 15531.7158203125
loss_r_bn_feature 31.063430786132812
------------iteration 1800----------
total loss 16025.233409654189
main criterion 82.78712059168907
weighted_aux_loss 15942.4462890625
loss_r_bn_feature 31.8848934173584
------------iteration 1900----------
total loss 31419.80865732404
main criterion 92.3750635740385
weighted_aux_loss 31327.43359375
loss_r_bn_feature 62.654869079589844
------------iteration 0----------
total loss 127085.34811307573
main criterion 136.644988075722
weighted_aux_loss 126948.703125
loss_r_bn_feature 253.89739990234375
------------iteration 100----------
total loss 101932.5083580246
main criterion 118.21929552460175
weighted_aux_loss 101814.2890625
loss_r_bn_feature 203.6285858154297
------------iteration 200----------
total loss 41778.8180476032
main criterion 75.92742260320013
weighted_aux_loss 41702.890625
loss_r_bn_feature 83.4057846069336
------------iteration 300----------
total loss 33077.9739786544
main criterion 74.4231974044009
weighted_aux_loss 33003.55078125
loss_r_bn_feature 66.0071029663086
------------iteration 400----------
total loss 27853.2660811019
main criterion 77.24654985190226
weighted_aux_loss 27776.01953125
loss_r_bn_feature 55.552040100097656
------------iteration 500----------
total loss 29662.579242379335
main criterion 75.87025800433476
weighted_aux_loss 29586.708984375
loss_r_bn_feature 59.17341613769531
------------iteration 600----------
total loss 24335.659206091947
main criterion 73.9736592169461
weighted_aux_loss 24261.685546875
loss_r_bn_feature 48.523372650146484
------------iteration 700----------
total loss 25495.67261704928
main criterion 76.15894517428033
weighted_aux_loss 25419.513671875
loss_r_bn_feature 50.839027404785156
------------iteration 800----------
total loss 24470.650125356202
main criterion 76.68528160620241
weighted_aux_loss 24393.96484375
loss_r_bn_feature 48.78792953491211
------------iteration 900----------
total loss 17782.28357579882
main criterion 73.59021642382213
weighted_aux_loss 17708.693359375
loss_r_bn_feature 35.41738510131836
------------iteration 1000----------
total loss 23832.0809024255
main criterion 80.02230867550048
weighted_aux_loss 23752.05859375
loss_r_bn_feature 47.50411605834961
------------iteration 1100----------
total loss 23563.994237729476
main criterion 77.1153314794739
weighted_aux_loss 23486.87890625
loss_r_bn_feature 46.973758697509766
------------iteration 1200----------
total loss 27618.592012559708
main criterion 79.44748130970765
weighted_aux_loss 27539.14453125
loss_r_bn_feature 55.07828903198242
------------iteration 1300----------
total loss 14102.582629414303
main criterion 73.52598878930274
weighted_aux_loss 14029.056640625
loss_r_bn_feature 28.05811309814453
------------iteration 1400----------
total loss 15236.262456813736
main criterion 72.41870681373533
weighted_aux_loss 15163.84375
loss_r_bn_feature 30.327688217163086
------------iteration 1500----------
total loss 16318.538399279154
main criterion 75.68293052915342
weighted_aux_loss 16242.85546875
loss_r_bn_feature 32.48571014404297
------------iteration 1600----------
total loss 16996.407864520817
main criterion 74.69497389581836
weighted_aux_loss 16921.712890625
loss_r_bn_feature 33.84342575073242
------------iteration 1700----------
total loss 21095.304725441074
main criterion 79.55863169107319
weighted_aux_loss 21015.74609375
loss_r_bn_feature 42.031490325927734
------------iteration 1800----------
total loss 20406.298038966932
main criterion 77.08514834193068
weighted_aux_loss 20329.212890625
loss_r_bn_feature 40.658424377441406
------------iteration 1900----------
total loss 23299.366854750773
main criterion 83.26724537577333
weighted_aux_loss 23216.099609375
loss_r_bn_feature 46.43219757080078
------------iteration 0----------
total loss 124117.82396642602
main criterion 131.65990392601498
weighted_aux_loss 123986.1640625
loss_r_bn_feature 247.9723358154297
------------iteration 100----------
total loss 45801.448245186926
main criterion 80.66308893692872
weighted_aux_loss 45720.78515625
loss_r_bn_feature 91.44157409667969
------------iteration 200----------
total loss 37443.53343712823
main criterion 78.20140587822804
weighted_aux_loss 37365.33203125
loss_r_bn_feature 74.73066711425781
------------iteration 300----------
total loss 72557.4673600391
main criterion 104.5611100391018
weighted_aux_loss 72452.90625
loss_r_bn_feature 144.9058074951172
------------iteration 400----------
total loss 33653.2749061678
main criterion 76.02099991779592
weighted_aux_loss 33577.25390625
loss_r_bn_feature 67.15451049804688
------------iteration 500----------
total loss 26147.861627439244
main criterion 78.74248681424505
weighted_aux_loss 26069.119140625
loss_r_bn_feature 52.13823699951172
------------iteration 600----------
total loss 36935.76502622942
main criterion 87.16346372941855
weighted_aux_loss 36848.6015625
loss_r_bn_feature 73.69720458984375
------------iteration 700----------
total loss 22171.49008877523
main criterion 80.61313565023067
weighted_aux_loss 22090.876953125
loss_r_bn_feature 44.18175506591797
------------iteration 800----------
total loss 19876.576579310953
main criterion 76.50040743595405
weighted_aux_loss 19800.076171875
loss_r_bn_feature 39.60015106201172
------------iteration 900----------
total loss 22417.166054826914
main criterion 78.19730482691217
weighted_aux_loss 22338.96875
loss_r_bn_feature 44.67793655395508
------------iteration 1000----------
total loss 19887.550941599595
main criterion 76.9532853495953
weighted_aux_loss 19810.59765625
loss_r_bn_feature 39.62119674682617
------------iteration 1100----------
total loss 16633.012931343394
main criterion 78.08519696839363
weighted_aux_loss 16554.927734375
loss_r_bn_feature 33.10985565185547
------------iteration 1200----------
total loss 23466.71588990928
main criterion 80.91901490928016
weighted_aux_loss 23385.796875
loss_r_bn_feature 46.7715950012207
------------iteration 1300----------
total loss 15858.672501271247
main criterion 77.93910283374652
weighted_aux_loss 15780.7333984375
loss_r_bn_feature 31.561466217041016
------------iteration 1400----------
total loss 17429.240765190232
main criterion 81.9809995652329
weighted_aux_loss 17347.259765625
loss_r_bn_feature 34.69451904296875
------------iteration 1500----------
total loss 18717.51926270061
main criterion 82.97043457560912
weighted_aux_loss 18634.548828125
loss_r_bn_feature 37.26909637451172
------------iteration 1600----------
total loss 12667.71924665359
main criterion 76.63428571609072
weighted_aux_loss 12591.0849609375
loss_r_bn_feature 25.182170867919922
------------iteration 1700----------
total loss 13855.45242500615
main criterion 78.29519844364985
weighted_aux_loss 13777.1572265625
loss_r_bn_feature 27.55431365966797
------------iteration 1800----------
total loss 12200.317824542328
main criterion 78.97016829232842
weighted_aux_loss 12121.34765625
loss_r_bn_feature 24.242694854736328
------------iteration 1900----------
total loss 12583.480291332326
main criterion 78.26154133232629
weighted_aux_loss 12505.21875
loss_r_bn_feature 25.01043701171875
------------iteration 0----------
total loss 123716.45266967935
main criterion 133.5386071793456
weighted_aux_loss 123582.9140625
loss_r_bn_feature 247.16583251953125
------------iteration 100----------
total loss 45324.25686108353
main criterion 79.29201733353129
weighted_aux_loss 45244.96484375
loss_r_bn_feature 90.48992919921875
------------iteration 200----------
total loss 38625.41561137771
main criterion 77.39217387771262
weighted_aux_loss 38548.0234375
loss_r_bn_feature 77.0960464477539
------------iteration 300----------
total loss 37858.85996442364
main criterion 82.02012067364133
weighted_aux_loss 37776.83984375
loss_r_bn_feature 75.55368041992188
------------iteration 400----------
total loss 35124.75601639394
main criterion 76.67789139394354
weighted_aux_loss 35048.078125
loss_r_bn_feature 70.09615325927734
------------iteration 500----------
total loss 48368.400575129446
main criterion 87.97870012944317
weighted_aux_loss 48280.421875
loss_r_bn_feature 96.56084442138672
------------iteration 600----------
total loss 33236.35870132792
main criterion 85.64776382791725
weighted_aux_loss 33150.7109375
loss_r_bn_feature 66.30142211914062
------------iteration 700----------
total loss 22527.274599339777
main criterion 73.63006808977508
weighted_aux_loss 22453.64453125
loss_r_bn_feature 44.90728759765625
------------iteration 800----------
total loss 19265.40202399426
main criterion 77.1891333692592
weighted_aux_loss 19188.212890625
loss_r_bn_feature 38.376426696777344
------------iteration 900----------
total loss 20605.772951348965
main criterion 73.7670919739658
weighted_aux_loss 20532.005859375
loss_r_bn_feature 41.06401062011719
------------iteration 1000----------
total loss 26289.29914686545
main criterion 81.82063124045165
weighted_aux_loss 26207.478515625
loss_r_bn_feature 52.414955139160156
------------iteration 1100----------
total loss 29129.30992811893
main criterion 82.27867811893142
weighted_aux_loss 29047.03125
loss_r_bn_feature 58.09406280517578
------------iteration 1200----------
total loss 15001.92714434823
main criterion 73.41835528573078
weighted_aux_loss 14928.5087890625
loss_r_bn_feature 29.857017517089844
------------iteration 1300----------
total loss 14222.501364607007
main criterion 77.10780991950743
weighted_aux_loss 14145.3935546875
loss_r_bn_feature 28.290786743164062
------------iteration 1400----------
total loss 18015.34689659306
main criterion 76.18478721805727
weighted_aux_loss 17939.162109375
loss_r_bn_feature 35.87832260131836
------------iteration 1500----------
total loss 15361.571841765151
main criterion 75.3833652026515
weighted_aux_loss 15286.1884765625
loss_r_bn_feature 30.572376251220703
------------iteration 1600----------
total loss 15713.770980554767
main criterion 73.63621492976726
weighted_aux_loss 15640.134765625
loss_r_bn_feature 31.280269622802734
------------iteration 1700----------
total loss 20421.1576125038
main criterion 82.10683125380007
weighted_aux_loss 20339.05078125
loss_r_bn_feature 40.6781005859375
------------iteration 1800----------
total loss 20543.160086584638
main criterion 80.06243033463672
weighted_aux_loss 20463.09765625
loss_r_bn_feature 40.92619705200195
------------iteration 1900----------
total loss 12678.871017852634
main criterion 75.69035379013528
weighted_aux_loss 12603.1806640625
loss_r_bn_feature 25.206361770629883
------------iteration 0----------
total loss 126527.8981190619
main criterion 133.85124406189777
weighted_aux_loss 126394.046875
loss_r_bn_feature 252.7880859375
------------iteration 100----------
total loss 45211.70537731567
main criterion 75.92022106567482
weighted_aux_loss 45135.78515625
loss_r_bn_feature 90.27156829833984
------------iteration 200----------
total loss 36158.8874268726
main criterion 74.85617687259594
weighted_aux_loss 36084.03125
loss_r_bn_feature 72.16806030273438
------------iteration 300----------
total loss 36064.58418690156
main criterion 73.13887440155312
weighted_aux_loss 35991.4453125
loss_r_bn_feature 71.9828872680664
------------iteration 400----------
total loss 48118.055282459965
main criterion 84.38731370996327
weighted_aux_loss 48033.66796875
loss_r_bn_feature 96.06733703613281
------------iteration 500----------
total loss 41583.31783106366
main criterion 81.18111231366014
weighted_aux_loss 41502.13671875
loss_r_bn_feature 83.0042724609375
------------iteration 600----------
total loss 46674.91921177725
main criterion 92.9153055272478
weighted_aux_loss 46582.00390625
loss_r_bn_feature 93.16400909423828
------------iteration 700----------
total loss 25012.889127846403
main criterion 72.00631534640213
weighted_aux_loss 24940.8828125
loss_r_bn_feature 49.88176727294922
------------iteration 800----------
total loss 21302.791041404966
main criterion 72.68947890496402
weighted_aux_loss 21230.1015625
loss_r_bn_feature 42.460201263427734
------------iteration 900----------
total loss 20535.934467541385
main criterion 72.47743629138488
weighted_aux_loss 20463.45703125
loss_r_bn_feature 40.92691421508789
------------iteration 1000----------
total loss 19191.801270680004
main criterion 74.3305675550022
weighted_aux_loss 19117.470703125
loss_r_bn_feature 38.23493957519531
------------iteration 1100----------
total loss 22072.241761865964
main criterion 76.51519936596203
weighted_aux_loss 21995.7265625
loss_r_bn_feature 43.991455078125
------------iteration 1200----------
total loss 15401.21751567094
main criterion 72.03489848344003
weighted_aux_loss 15329.1826171875
loss_r_bn_feature 30.65836524963379
------------iteration 1300----------
total loss 16915.998099771085
main criterion 75.7188028960837
weighted_aux_loss 16840.279296875
loss_r_bn_feature 33.68055725097656
------------iteration 1400----------
total loss 14347.562192840227
main criterion 72.69305221522713
weighted_aux_loss 14274.869140625
loss_r_bn_feature 28.54973793029785
------------iteration 1500----------
total loss 14509.820458368416
main criterion 71.94448180591591
weighted_aux_loss 14437.8759765625
loss_r_bn_feature 28.875751495361328
------------iteration 1600----------
total loss 14907.481579777574
main criterion 71.71302509007452
weighted_aux_loss 14835.7685546875
loss_r_bn_feature 29.671537399291992
------------iteration 1700----------
total loss 12909.39034998568
main criterion 71.50167811067972
weighted_aux_loss 12837.888671875
loss_r_bn_feature 25.675777435302734
------------iteration 1800----------
total loss 12761.168033318398
main criterion 72.61139269339688
weighted_aux_loss 12688.556640625
loss_r_bn_feature 25.377113342285156
------------iteration 1900----------
total loss 12485.143843475687
main criterion 73.06571847568668
weighted_aux_loss 12412.078125
loss_r_bn_feature 24.824155807495117
------------iteration 0----------
total loss 124825.66745622235
main criterion 130.3940187223485
weighted_aux_loss 124695.2734375
loss_r_bn_feature 249.3905487060547
------------iteration 100----------
total loss 48841.21491071192
main criterion 80.00787946191647
weighted_aux_loss 48761.20703125
loss_r_bn_feature 97.52241516113281
------------iteration 200----------
total loss 41792.596881494566
main criterion 76.37031899456835
weighted_aux_loss 41716.2265625
loss_r_bn_feature 83.43245697021484
------------iteration 300----------
total loss 37814.77673384066
main criterion 77.03454634066264
weighted_aux_loss 37737.7421875
loss_r_bn_feature 75.4754867553711
------------iteration 400----------
total loss 35051.446561227705
main criterion 76.6028112277085
weighted_aux_loss 34974.84375
loss_r_bn_feature 69.9496841430664
------------iteration 500----------
total loss 28875.493141234554
main criterion 75.5087662345556
weighted_aux_loss 28799.984375
loss_r_bn_feature 57.59996795654297
------------iteration 600----------
total loss 26583.126996677063
main criterion 78.07816855206211
weighted_aux_loss 26505.048828125
loss_r_bn_feature 53.01009750366211
------------iteration 700----------
total loss 34541.13367284532
main criterion 82.29773534531938
weighted_aux_loss 34458.8359375
loss_r_bn_feature 68.91767120361328
------------iteration 800----------
total loss 20148.022138274864
main criterion 76.76823202486256
weighted_aux_loss 20071.25390625
loss_r_bn_feature 40.14250946044922
------------iteration 900----------
total loss 26595.39117032833
main criterion 81.40093595333163
weighted_aux_loss 26513.990234375
loss_r_bn_feature 53.02798080444336
------------iteration 1000----------
total loss 19506.072283024117
main criterion 76.20509552411681
weighted_aux_loss 19429.8671875
loss_r_bn_feature 38.85973358154297
------------iteration 1100----------
total loss 20283.23524855257
main criterion 77.99110792757159
weighted_aux_loss 20205.244140625
loss_r_bn_feature 40.41048812866211
------------iteration 1200----------
total loss 15333.363724878265
main criterion 76.60200612826574
weighted_aux_loss 15256.76171875
loss_r_bn_feature 30.51352310180664
------------iteration 1300----------
total loss 14671.26114807685
main criterion 77.1888824518493
weighted_aux_loss 14594.072265625
loss_r_bn_feature 29.18814468383789
------------iteration 1400----------
total loss 17204.924674596798
main criterion 77.83483084679602
weighted_aux_loss 17127.08984375
loss_r_bn_feature 34.254180908203125
------------iteration 1500----------
total loss 12044.552503175555
main criterion 77.06324536305584
weighted_aux_loss 11967.4892578125
loss_r_bn_feature 23.934978485107422
------------iteration 1600----------
total loss 18887.787165031677
main criterion 78.85552440667682
weighted_aux_loss 18808.931640625
loss_r_bn_feature 37.617862701416016
------------iteration 1700----------
total loss 14876.730611217994
main criterion 78.91127528049434
weighted_aux_loss 14797.8193359375
loss_r_bn_feature 29.595638275146484
------------iteration 1800----------
total loss 16130.501950784377
main criterion 77.10937265937778
weighted_aux_loss 16053.392578125
loss_r_bn_feature 32.10678482055664
------------iteration 1900----------
total loss 13058.134620183288
main criterion 76.36997174578784
weighted_aux_loss 12981.7646484375
loss_r_bn_feature 25.963529586791992
------------iteration 0----------
total loss 128714.72918118983
main criterion 140.06511868982912
weighted_aux_loss 128574.6640625
loss_r_bn_feature 257.1493225097656
------------iteration 100----------
total loss 45754.95262033561
main criterion 83.42918283560988
weighted_aux_loss 45671.5234375
loss_r_bn_feature 91.34304809570312
------------iteration 200----------
total loss 43760.149371749016
main criterion 78.0634342490178
weighted_aux_loss 43682.0859375
loss_r_bn_feature 87.36417388916016
------------iteration 300----------
total loss 35345.742021505765
main criterion 84.77327150576745
weighted_aux_loss 35260.96875
loss_r_bn_feature 70.52193450927734
------------iteration 400----------
total loss 32446.13355843291
main criterion 76.73707405791072
weighted_aux_loss 32369.396484375
loss_r_bn_feature 64.7387924194336
------------iteration 500----------
total loss 31024.930782414383
main criterion 77.34679803938513
weighted_aux_loss 30947.583984375
loss_r_bn_feature 61.89516830444336
------------iteration 600----------
total loss 25970.559229929535
main criterion 76.05727680453542
weighted_aux_loss 25894.501953125
loss_r_bn_feature 51.789005279541016
------------iteration 700----------
total loss 28080.446872022105
main criterion 81.89804389710453
weighted_aux_loss 27998.548828125
loss_r_bn_feature 55.99709701538086
------------iteration 800----------
total loss 29637.2201146747
main criterion 74.22206779970077
weighted_aux_loss 29562.998046875
loss_r_bn_feature 59.12599563598633
------------iteration 900----------
total loss 20962.531038170848
main criterion 79.26931942084836
weighted_aux_loss 20883.26171875
loss_r_bn_feature 41.76652526855469
------------iteration 1000----------
total loss 17482.648652411975
main criterion 76.16037116197398
weighted_aux_loss 17406.48828125
loss_r_bn_feature 34.8129768371582
------------iteration 1100----------
total loss 21145.78774625565
main criterion 78.14126188065063
weighted_aux_loss 21067.646484375
loss_r_bn_feature 42.135292053222656
------------iteration 1200----------
total loss 19634.0340986536
main criterion 75.07706740360076
weighted_aux_loss 19558.95703125
loss_r_bn_feature 39.11791229248047
------------iteration 1300----------
total loss 19269.280728121663
main criterion 80.7748687466623
weighted_aux_loss 19188.505859375
loss_r_bn_feature 38.377010345458984
------------iteration 1400----------
total loss 21166.414591194516
main criterion 78.04740369451457
weighted_aux_loss 21088.3671875
loss_r_bn_feature 42.176734924316406
------------iteration 1500----------
total loss 12688.827761584773
main criterion 75.01526158477306
weighted_aux_loss 12613.8125
loss_r_bn_feature 25.227624893188477
------------iteration 1600----------
total loss 31066.675312699004
main criterion 85.46632832400554
weighted_aux_loss 30981.208984375
loss_r_bn_feature 61.96241760253906
------------iteration 1700----------
total loss 14729.284940530153
main criterion 75.81716709265231
weighted_aux_loss 14653.4677734375
loss_r_bn_feature 29.306936264038086
------------iteration 1800----------
total loss 12377.57113067974
main criterion 74.98226349223947
weighted_aux_loss 12302.5888671875
loss_r_bn_feature 24.60517692565918
------------iteration 1900----------
total loss 16518.29695414916
main criterion 76.15242289915697
weighted_aux_loss 16442.14453125
loss_r_bn_feature 32.8842887878418
------------iteration 0----------
total loss 125660.35780095661
main criterion 127.61561345661075
weighted_aux_loss 125532.7421875
loss_r_bn_feature 251.06549072265625
------------iteration 100----------
total loss 48822.83967898608
main criterion 80.37483523607703
weighted_aux_loss 48742.46484375
loss_r_bn_feature 97.48493194580078
------------iteration 200----------
total loss 43057.45446705412
main criterion 76.0638420541194
weighted_aux_loss 42981.390625
loss_r_bn_feature 85.96278381347656
------------iteration 300----------
total loss 36660.508446716565
main criterion 77.78188421656164
weighted_aux_loss 36582.7265625
loss_r_bn_feature 73.16545104980469
------------iteration 400----------
total loss 67527.72683499222
main criterion 92.33620999222833
weighted_aux_loss 67435.390625
loss_r_bn_feature 134.87078857421875
------------iteration 500----------
total loss 27786.541587238684
main criterion 76.04354036368207
weighted_aux_loss 27710.498046875
loss_r_bn_feature 55.420997619628906
------------iteration 600----------
total loss 36204.78553621765
main criterion 92.7308487176497
weighted_aux_loss 36112.0546875
loss_r_bn_feature 72.22410583496094
------------iteration 700----------
total loss 27039.35405431342
main criterion 75.38335118841803
weighted_aux_loss 26963.970703125
loss_r_bn_feature 53.927940368652344
------------iteration 800----------
total loss 25321.515527686388
main criterion 79.68740268638899
weighted_aux_loss 25241.828125
loss_r_bn_feature 50.48365783691406
------------iteration 900----------
total loss 22580.420268065303
main criterion 75.56089306530406
weighted_aux_loss 22504.859375
loss_r_bn_feature 45.00971984863281
------------iteration 1000----------
total loss 23201.187142014867
main criterion 75.88440763986789
weighted_aux_loss 23125.302734375
loss_r_bn_feature 46.250606536865234
------------iteration 1100----------
total loss 21693.041513764692
main criterion 77.29346688969183
weighted_aux_loss 21615.748046875
loss_r_bn_feature 43.23149490356445
------------iteration 1200----------
total loss 28408.644247801196
main criterion 78.66573217619771
weighted_aux_loss 28329.978515625
loss_r_bn_feature 56.65995788574219
------------iteration 1300----------
total loss 15172.807445134835
main criterion 75.82307013483481
weighted_aux_loss 15096.984375
loss_r_bn_feature 30.1939697265625
------------iteration 1400----------
total loss 13810.720620012402
main criterion 76.08976063740222
weighted_aux_loss 13734.630859375
loss_r_bn_feature 27.469261169433594
------------iteration 1500----------
total loss 16178.496419633151
main criterion 76.84016963315035
weighted_aux_loss 16101.65625
loss_r_bn_feature 32.203311920166016
------------iteration 1600----------
total loss 15939.533554850907
main criterion 75.22886735090742
weighted_aux_loss 15864.3046875
loss_r_bn_feature 31.728609085083008
------------iteration 1700----------
total loss 15270.519345011098
main criterion 75.46465751109739
weighted_aux_loss 15195.0546875
loss_r_bn_feature 30.39011001586914
------------iteration 1800----------
total loss 16299.624390705807
main criterion 79.94177351830606
weighted_aux_loss 16219.6826171875
loss_r_bn_feature 32.43936538696289
------------iteration 1900----------
total loss 13598.901257663878
main criterion 75.3270389138773
weighted_aux_loss 13523.57421875
loss_r_bn_feature 27.047147750854492
------------iteration 0----------
total loss 125590.15716333082
main criterion 130.93841333081437
weighted_aux_loss 125459.21875
loss_r_bn_feature 250.91844177246094
------------iteration 100----------
total loss 48145.88350090672
main criterion 78.75068840672235
weighted_aux_loss 48067.1328125
loss_r_bn_feature 96.13426208496094
------------iteration 200----------
total loss 42748.4824196549
main criterion 75.865232154899
weighted_aux_loss 42672.6171875
loss_r_bn_feature 85.3452377319336
------------iteration 300----------
total loss 36829.453494191774
main criterion 82.31677544177276
weighted_aux_loss 36747.13671875
loss_r_bn_feature 73.49427032470703
------------iteration 400----------
total loss 31864.927232396072
main criterion 77.04637302107399
weighted_aux_loss 31787.880859375
loss_r_bn_feature 63.57575988769531
------------iteration 500----------
total loss 29622.49584075652
main criterion 76.37084075651919
weighted_aux_loss 29546.125
loss_r_bn_feature 59.09225082397461
------------iteration 600----------
total loss 25930.313957265433
main criterion 77.98973851543413
weighted_aux_loss 25852.32421875
loss_r_bn_feature 51.704647064208984
------------iteration 700----------
total loss 22432.981686262956
main criterion 76.38403001295747
weighted_aux_loss 22356.59765625
loss_r_bn_feature 44.71319580078125
------------iteration 800----------
total loss 19554.525554919714
main criterion 74.53727366971394
weighted_aux_loss 19479.98828125
loss_r_bn_feature 38.95997619628906
------------iteration 900----------
total loss 17521.434219166367
main criterion 77.63539104136812
weighted_aux_loss 17443.798828125
loss_r_bn_feature 34.887596130371094
------------iteration 1000----------
total loss 16819.05532865745
main criterion 76.08657865745003
weighted_aux_loss 16742.96875
loss_r_bn_feature 33.485939025878906
------------iteration 1100----------
total loss 16588.985464318204
main criterion 76.48155806820542
weighted_aux_loss 16512.50390625
loss_r_bn_feature 33.02500915527344
------------iteration 1200----------
total loss 14922.629255940616
main criterion 76.21812312811535
weighted_aux_loss 14846.4111328125
loss_r_bn_feature 29.692821502685547
------------iteration 1300----------
total loss 15656.53816739295
main criterion 74.37703458044905
weighted_aux_loss 15582.1611328125
loss_r_bn_feature 31.164321899414062
------------iteration 1400----------
total loss 19567.04431335539
main criterion 78.65564148038999
weighted_aux_loss 19488.388671875
loss_r_bn_feature 38.976776123046875
------------iteration 1500----------
total loss 13455.090339015715
main criterion 74.15088589071382
weighted_aux_loss 13380.939453125
loss_r_bn_feature 26.761878967285156
------------iteration 1600----------
total loss 17790.221944964254
main criterion 77.88210121425358
weighted_aux_loss 17712.33984375
loss_r_bn_feature 35.424678802490234
------------iteration 1700----------
total loss 16012.543243449061
main criterion 79.75418094906121
weighted_aux_loss 15932.7890625
loss_r_bn_feature 31.865577697753906
------------iteration 1800----------
total loss 12284.458391568678
main criterion 75.81971969367761
weighted_aux_loss 12208.638671875
loss_r_bn_feature 24.417278289794922
------------iteration 1900----------
total loss 18076.264711572087
main criterion 80.99713344708636
weighted_aux_loss 17995.267578125
loss_r_bn_feature 35.990535736083984
------------iteration 0----------
total loss 127685.40391100485
main criterion 134.78672350484257
weighted_aux_loss 127550.6171875
loss_r_bn_feature 255.1012420654297
------------iteration 100----------
total loss 45795.0931445378
main criterion 78.32361328780073
weighted_aux_loss 45716.76953125
loss_r_bn_feature 91.43354034423828
------------iteration 200----------
total loss 40828.202298148455
main criterion 74.92495439845862
weighted_aux_loss 40753.27734375
loss_r_bn_feature 81.50655364990234
------------iteration 300----------
total loss 39038.26833545518
main criterion 74.0300542051754
weighted_aux_loss 38964.23828125
loss_r_bn_feature 77.92847442626953
------------iteration 400----------
total loss 37386.23034558174
main criterion 73.99206433174183
weighted_aux_loss 37312.23828125
loss_r_bn_feature 74.62447357177734
------------iteration 500----------
total loss 27093.649345324735
main criterion 78.23137657473366
weighted_aux_loss 27015.41796875
loss_r_bn_feature 54.03083419799805
------------iteration 600----------
total loss 28567.660334501896
main criterion 74.4591626268973
weighted_aux_loss 28493.201171875
loss_r_bn_feature 56.98640060424805
------------iteration 700----------
total loss 23144.853666978433
main criterion 76.88687010343351
weighted_aux_loss 23067.966796875
loss_r_bn_feature 46.13593292236328
------------iteration 800----------
total loss 23305.256194867896
main criterion 77.02377299289746
weighted_aux_loss 23228.232421875
loss_r_bn_feature 46.45646667480469
------------iteration 900----------
total loss 21102.571335993558
main criterion 77.15141411855821
weighted_aux_loss 21025.419921875
loss_r_bn_feature 42.050838470458984
------------iteration 1000----------
total loss 20053.50154318817
main criterion 75.38435568817107
weighted_aux_loss 19978.1171875
loss_r_bn_feature 39.956233978271484
------------iteration 1100----------
total loss 21927.227143998178
main criterion 78.48690962317714
weighted_aux_loss 21848.740234375
loss_r_bn_feature 43.697479248046875
------------iteration 1200----------
total loss 18398.01898150568
main criterion 75.42327838068138
weighted_aux_loss 18322.595703125
loss_r_bn_feature 36.64519119262695
------------iteration 1300----------
total loss 14389.21002206722
main criterion 75.1729126922201
weighted_aux_loss 14314.037109375
loss_r_bn_feature 28.628074645996094
------------iteration 1400----------
total loss 20739.558088662292
main criterion 80.77879178729333
weighted_aux_loss 20658.779296875
loss_r_bn_feature 41.31755828857422
------------iteration 1500----------
total loss 46656.88831765577
main criterion 93.03675515577235
weighted_aux_loss 46563.8515625
loss_r_bn_feature 93.12770080566406
------------iteration 1600----------
total loss 16783.297865841683
main criterion 75.16895959168373
weighted_aux_loss 16708.12890625
loss_r_bn_feature 33.416259765625
------------iteration 1700----------
total loss 13302.79589459778
main criterion 75.24120709778076
weighted_aux_loss 13227.5546875
loss_r_bn_feature 26.455108642578125
------------iteration 1800----------
total loss 11730.506543195896
main criterion 75.23994163339576
weighted_aux_loss 11655.2666015625
loss_r_bn_feature 23.31053352355957
------------iteration 1900----------
total loss 14004.331454942201
main criterion 75.74356431720035
weighted_aux_loss 13928.587890625
loss_r_bn_feature 27.857175827026367
------------iteration 0----------
total loss 121553.84991816492
main criterion 153.87335566493033
weighted_aux_loss 121399.9765625
loss_r_bn_feature 242.79995727539062
------------iteration 100----------
total loss 45674.116723830375
main criterion 80.93313008037308
weighted_aux_loss 45593.18359375
loss_r_bn_feature 91.18637084960938
------------iteration 200----------
total loss 58324.33811343824
main criterion 94.25217593824696
weighted_aux_loss 58230.0859375
loss_r_bn_feature 116.46017456054688
------------iteration 300----------
total loss 45017.587858998486
main criterion 76.23629649848576
weighted_aux_loss 44941.3515625
loss_r_bn_feature 89.88270568847656
------------iteration 400----------
total loss 36807.12253460744
main criterion 77.80222210744553
weighted_aux_loss 36729.3203125
loss_r_bn_feature 73.4586410522461
------------iteration 500----------
total loss 29454.27716740326
main criterion 76.5584174032607
weighted_aux_loss 29377.71875
loss_r_bn_feature 58.755435943603516
------------iteration 600----------
total loss 22534.54084208346
main criterion 79.74006083346069
weighted_aux_loss 22454.80078125
loss_r_bn_feature 44.909603118896484
------------iteration 700----------
total loss 22467.145379098973
main criterion 78.37194159897338
weighted_aux_loss 22388.7734375
loss_r_bn_feature 44.77754592895508
------------iteration 800----------
total loss 36314.411052153795
main criterion 85.69230215379845
weighted_aux_loss 36228.71875
loss_r_bn_feature 72.45743560791016
------------iteration 900----------
total loss 33619.28767011538
main criterion 88.89313886538056
weighted_aux_loss 33530.39453125
loss_r_bn_feature 67.060791015625
------------iteration 1000----------
total loss 16677.488158748343
main criterion 79.81042437334459
weighted_aux_loss 16597.677734375
loss_r_bn_feature 33.19535446166992
------------iteration 1100----------
total loss 31980.922583576896
main criterion 89.37375545189394
weighted_aux_loss 31891.548828125
loss_r_bn_feature 63.78309631347656
------------iteration 1200----------
total loss 15340.747564060195
main criterion 77.58643124769542
weighted_aux_loss 15263.1611328125
loss_r_bn_feature 30.526321411132812
------------iteration 1300----------
total loss 15351.726448183048
main criterion 78.2381669330485
weighted_aux_loss 15273.48828125
loss_r_bn_feature 30.54697608947754
------------iteration 1400----------
total loss 23482.921629278
main criterion 83.06030115299978
weighted_aux_loss 23399.861328125
loss_r_bn_feature 46.79972457885742
------------iteration 1500----------
total loss 14399.714823683922
main criterion 76.66892524642248
weighted_aux_loss 14323.0458984375
loss_r_bn_feature 28.64609146118164
------------iteration 1600----------
total loss 20929.748935464464
main criterion 83.86416983946323
weighted_aux_loss 20845.884765625
loss_r_bn_feature 41.691768646240234
------------iteration 1700----------
total loss 26250.142459175768
main criterion 80.54870917576746
weighted_aux_loss 26169.59375
loss_r_bn_feature 52.33918762207031
------------iteration 1800----------
total loss 12610.028247118538
main criterion 77.7938721185387
weighted_aux_loss 12532.234375
loss_r_bn_feature 25.064468383789062
------------iteration 1900----------
total loss 13968.480857320545
main criterion 77.34413857054408
weighted_aux_loss 13891.13671875
loss_r_bn_feature 27.78227424621582
------------iteration 0----------
total loss 123243.37417650777
main criterion 136.42886400777644
weighted_aux_loss 123106.9453125
loss_r_bn_feature 246.21389770507812
------------iteration 100----------
total loss 46875.80108291265
main criterion 82.37139541265043
weighted_aux_loss 46793.4296875
loss_r_bn_feature 93.58686065673828
------------iteration 200----------
total loss 36215.89362412589
main criterion 83.06159287588957
weighted_aux_loss 36132.83203125
loss_r_bn_feature 72.26566314697266
------------iteration 300----------
total loss 36274.73390508068
main criterion 79.21437383067607
weighted_aux_loss 36195.51953125
loss_r_bn_feature 72.39103698730469
------------iteration 400----------
total loss 58743.71394647154
main criterion 95.26863397153382
weighted_aux_loss 58648.4453125
loss_r_bn_feature 117.29689025878906
------------iteration 500----------
total loss 25000.071620549108
main criterion 79.49544867410955
weighted_aux_loss 24920.576171875
loss_r_bn_feature 49.84115219116211
------------iteration 600----------
total loss 22505.234620308016
main criterion 80.20922968301622
weighted_aux_loss 22425.025390625
loss_r_bn_feature 44.85005187988281
------------iteration 700----------
total loss 21080.650901291305
main criterion 78.03957316630334
weighted_aux_loss 21002.611328125
loss_r_bn_feature 42.00522232055664
------------iteration 800----------
total loss 36810.58559422668
main criterion 90.49575047668121
weighted_aux_loss 36720.08984375
loss_r_bn_feature 73.44017791748047
------------iteration 900----------
total loss 24950.072273674497
main criterion 78.89258617449728
weighted_aux_loss 24871.1796875
loss_r_bn_feature 49.74235916137695
------------iteration 1000----------
total loss 24309.453852208073
main criterion 86.7976022080734
weighted_aux_loss 24222.65625
loss_r_bn_feature 48.4453125
------------iteration 1100----------
total loss 16110.355453700262
main criterion 79.38182088776267
weighted_aux_loss 16030.9736328125
loss_r_bn_feature 32.061946868896484
------------iteration 1200----------
total loss 25479.02565867647
main criterion 84.23268992646845
weighted_aux_loss 25394.79296875
loss_r_bn_feature 50.78958511352539
------------iteration 1300----------
total loss 15804.774948782162
main criterion 76.72514409466199
weighted_aux_loss 15728.0498046875
loss_r_bn_feature 31.456100463867188
------------iteration 1400----------
total loss 14935.606820332167
main criterion 79.59607814466736
weighted_aux_loss 14856.0107421875
loss_r_bn_feature 29.712020874023438
------------iteration 1500----------
total loss 14423.441575562481
main criterion 78.27360681248219
weighted_aux_loss 14345.16796875
loss_r_bn_feature 28.690336227416992
------------iteration 1600----------
total loss 13462.69360764813
main criterion 76.15942796063081
weighted_aux_loss 13386.5341796875
loss_r_bn_feature 26.773067474365234
------------iteration 1700----------
total loss 14506.969132041922
main criterion 81.13905391692167
weighted_aux_loss 14425.830078125
loss_r_bn_feature 28.851659774780273
------------iteration 1800----------
total loss 11636.120215839106
main criterion 77.40341896410519
weighted_aux_loss 11558.716796875
loss_r_bn_feature 23.117433547973633
------------iteration 1900----------
total loss 11844.574237597199
main criterion 77.67287040969858
weighted_aux_loss 11766.9013671875
loss_r_bn_feature 23.533802032470703
------------iteration 0----------
total loss 127670.84479612698
main criterion 136.28229612699192
weighted_aux_loss 127534.5625
loss_r_bn_feature 255.06912231445312
------------iteration 100----------
total loss 68393.39787696602
main criterion 94.0775644660118
weighted_aux_loss 68299.3203125
loss_r_bn_feature 136.59864807128906
------------iteration 200----------
total loss 36783.89521564503
main criterion 81.46552814502951
weighted_aux_loss 36702.4296875
loss_r_bn_feature 73.40486145019531
------------iteration 300----------
total loss 32742.73555742871
main criterion 79.969932428711
weighted_aux_loss 32662.765625
loss_r_bn_feature 65.32553100585938
------------iteration 400----------
total loss 45278.07529076897
main criterion 94.78622826896726
weighted_aux_loss 45183.2890625
loss_r_bn_feature 90.3665771484375
------------iteration 500----------
total loss 29151.21967247786
main criterion 75.93451622785828
weighted_aux_loss 29075.28515625
loss_r_bn_feature 58.150569915771484
------------iteration 600----------
total loss 48569.98258466191
main criterion 93.73258466191439
weighted_aux_loss 48476.25
loss_r_bn_feature 96.95249938964844
------------iteration 700----------
total loss 21984.79839721512
main criterion 76.6480065901185
weighted_aux_loss 21908.150390625
loss_r_bn_feature 43.81629943847656
------------iteration 800----------
total loss 17353.586643985742
main criterion 77.62765961074051
weighted_aux_loss 17275.958984375
loss_r_bn_feature 34.551918029785156
------------iteration 900----------
total loss 16313.292177584164
main criterion 75.66522445916429
weighted_aux_loss 16237.626953125
loss_r_bn_feature 32.47525405883789
------------iteration 1000----------
total loss 15523.67983171212
main criterion 76.0392067121199
weighted_aux_loss 15447.640625
loss_r_bn_feature 30.895280838012695
------------iteration 1100----------
total loss 39182.223344995466
main criterion 89.69600124546677
weighted_aux_loss 39092.52734375
loss_r_bn_feature 78.18505096435547
------------iteration 1200----------
total loss 26923.071311145563
main criterion 83.85451427056306
weighted_aux_loss 26839.216796875
loss_r_bn_feature 53.67843246459961
------------iteration 1300----------
total loss 20565.800873396638
main criterion 82.7188421466395
weighted_aux_loss 20483.08203125
loss_r_bn_feature 40.966163635253906
------------iteration 1400----------
total loss 14232.694293623168
main criterion 73.5487858106672
weighted_aux_loss 14159.1455078125
loss_r_bn_feature 28.31829071044922
------------iteration 1500----------
total loss 11830.321300173518
main criterion 74.93360486101808
weighted_aux_loss 11755.3876953125
loss_r_bn_feature 23.510774612426758
------------iteration 1600----------
total loss 12107.536528835108
main criterion 75.18301321010779
weighted_aux_loss 12032.353515625
loss_r_bn_feature 24.064706802368164
------------iteration 1700----------
total loss 20521.218890213626
main criterion 83.71303083862585
weighted_aux_loss 20437.505859375
loss_r_bn_feature 40.8750114440918
------------iteration 1800----------
total loss 13905.946420514934
main criterion 75.64466270243446
weighted_aux_loss 13830.3017578125
loss_r_bn_feature 27.660602569580078
------------iteration 1900----------
total loss 10879.133108934131
main criterion 74.32353862163197
weighted_aux_loss 10804.8095703125
loss_r_bn_feature 21.609619140625
------------iteration 0----------
total loss 132238.97069650292
main criterion 137.5800715029086
weighted_aux_loss 132101.390625
loss_r_bn_feature 264.2027893066406
------------iteration 100----------
total loss 46976.06209524557
main criterion 83.8042827455694
weighted_aux_loss 46892.2578125
loss_r_bn_feature 93.78451538085938
------------iteration 200----------
total loss 37543.348602874496
main criterion 76.58688412449598
weighted_aux_loss 37466.76171875
loss_r_bn_feature 74.93352508544922
------------iteration 300----------
total loss 33950.904829421495
main criterion 78.99467317149697
weighted_aux_loss 33871.91015625
loss_r_bn_feature 67.74382019042969
------------iteration 400----------
total loss 46399.491092774726
main criterion 87.39734277472556
weighted_aux_loss 46312.09375
loss_r_bn_feature 92.62419128417969
------------iteration 500----------
total loss 27104.034513158742
main criterion 76.47201315874278
weighted_aux_loss 27027.5625
loss_r_bn_feature 54.05512619018555
------------iteration 600----------
total loss 26171.744046275115
main criterion 75.83193690011615
weighted_aux_loss 26095.912109375
loss_r_bn_feature 52.19182586669922
------------iteration 700----------
total loss 20113.790351619777
main criterion 78.04230474477582
weighted_aux_loss 20035.748046875
loss_r_bn_feature 40.071495056152344
------------iteration 800----------
total loss 27869.60079082046
main criterion 77.9250095704602
weighted_aux_loss 27791.67578125
loss_r_bn_feature 55.583351135253906
------------iteration 900----------
total loss 19167.40868485728
main criterion 75.79930985727972
weighted_aux_loss 19091.609375
loss_r_bn_feature 38.18321990966797
------------iteration 1000----------
total loss 18230.796013851796
main criterion 75.36632635179419
weighted_aux_loss 18155.4296875
loss_r_bn_feature 36.31085968017578
------------iteration 1100----------
total loss 15059.038953589738
main criterion 74.77528171473747
weighted_aux_loss 14984.263671875
loss_r_bn_feature 29.96852684020996
------------iteration 1200----------
total loss 14513.140119901143
main criterion 75.32469021364405
weighted_aux_loss 14437.8154296875
loss_r_bn_feature 28.87563133239746
------------iteration 1300----------
total loss 13525.622707863433
main criterion 75.15395786343313
weighted_aux_loss 13450.46875
loss_r_bn_feature 26.900938034057617
------------iteration 1400----------
total loss 19459.10025398696
main criterion 77.24869148695947
weighted_aux_loss 19381.8515625
loss_r_bn_feature 38.763702392578125
------------iteration 1500----------
total loss 14388.868406477644
main criterion 74.68578929014438
weighted_aux_loss 14314.1826171875
loss_r_bn_feature 28.62836456298828
------------iteration 1600----------
total loss 34142.20128135433
main criterion 90.02159385433188
weighted_aux_loss 34052.1796875
loss_r_bn_feature 68.10436248779297
------------iteration 1700----------
total loss 11645.219920537402
main criterion 76.59785022490179
weighted_aux_loss 11568.6220703125
loss_r_bn_feature 23.137243270874023
------------iteration 1800----------
total loss 12041.886590088277
main criterion 75.18346508827813
weighted_aux_loss 11966.703125
loss_r_bn_feature 23.933406829833984
------------iteration 1900----------
total loss 19622.615925776638
main criterion 80.28389452663733
weighted_aux_loss 19542.33203125
loss_r_bn_feature 39.08466339111328
------------iteration 0----------
total loss 117316.2472208653
main criterion 126.67690836530065
weighted_aux_loss 117189.5703125
loss_r_bn_feature 234.37913513183594
------------iteration 100----------
total loss 42324.79068168303
main criterion 74.27505668303152
weighted_aux_loss 42250.515625
loss_r_bn_feature 84.50102996826172
------------iteration 200----------
total loss 61140.02357420279
main criterion 96.60951170278605
weighted_aux_loss 61043.4140625
loss_r_bn_feature 122.08683013916016
------------iteration 300----------
total loss 31810.671379461197
main criterion 71.3901294611971
weighted_aux_loss 31739.28125
loss_r_bn_feature 63.47856140136719
------------iteration 400----------
total loss 27777.209587368852
main criterion 72.51818111885167
weighted_aux_loss 27704.69140625
loss_r_bn_feature 55.40938186645508
------------iteration 500----------
total loss 24724.589456070393
main criterion 70.25156544539125
weighted_aux_loss 24654.337890625
loss_r_bn_feature 49.308677673339844
------------iteration 600----------
total loss 24495.676801656235
main criterion 72.99320790623597
weighted_aux_loss 24422.68359375
loss_r_bn_feature 48.845367431640625
------------iteration 700----------
total loss 25598.805408324704
main criterion 68.71361144970288
weighted_aux_loss 25530.091796875
loss_r_bn_feature 51.060184478759766
------------iteration 800----------
total loss 26915.183028040366
main criterion 71.20060616536477
weighted_aux_loss 26843.982421875
loss_r_bn_feature 53.687965393066406
------------iteration 900----------
total loss 44284.730001206444
main criterion 85.91750120644116
weighted_aux_loss 44198.8125
loss_r_bn_feature 88.39762878417969
------------iteration 1000----------
total loss 17743.07171549117
main criterion 70.94476236616845
weighted_aux_loss 17672.126953125
loss_r_bn_feature 35.34425354003906
------------iteration 1100----------
total loss 16305.184122257899
main criterion 70.86478632039925
weighted_aux_loss 16234.3193359375
loss_r_bn_feature 32.4686393737793
------------iteration 1200----------
total loss 15789.573694158005
main criterion 70.21627228300584
weighted_aux_loss 15719.357421875
loss_r_bn_feature 31.4387149810791
------------iteration 1300----------
total loss 12402.646103441619
main criterion 70.99473625411837
weighted_aux_loss 12331.6513671875
loss_r_bn_feature 24.66330337524414
------------iteration 1400----------
total loss 31662.173564384564
main criterion 81.51145500956298
weighted_aux_loss 31580.662109375
loss_r_bn_feature 63.16132354736328
------------iteration 1500----------
total loss 12209.218222433423
main criterion 69.38130837092308
weighted_aux_loss 12139.8369140625
loss_r_bn_feature 24.279674530029297
------------iteration 1600----------
total loss 13505.109795899325
main criterion 70.7347958993249
weighted_aux_loss 13434.375
loss_r_bn_feature 26.868749618530273
------------iteration 1700----------
total loss 20685.41895673186
main criterion 74.25684735686114
weighted_aux_loss 20611.162109375
loss_r_bn_feature 41.22232437133789
------------iteration 1800----------
total loss 11999.07623235886
main criterion 71.47466985886024
weighted_aux_loss 11927.6015625
loss_r_bn_feature 23.85520362854004
------------iteration 1900----------
total loss 11748.28439725989
main criterion 71.0588113223897
weighted_aux_loss 11677.2255859375
loss_r_bn_feature 23.354450225830078
=> using pytorch pre-trained model 'resnet18'
process data from ../recover/syn_data/tiny-imagenet/478

  0%|          | 0/300 [00:00<?, ?it/s]
  0%|          | 1/300 [00:04<23:57,  4.81s/it]
  1%|          | 2/300 [00:07<16:20,  3.29s/it]
  1%|          | 3/300 [00:09<14:12,  2.87s/it]
  1%|▏         | 4/300 [00:11<12:57,  2.63s/it]
  2%|▏         | 5/300 [00:13<12:19,  2.51s/it]
  2%|▏         | 6/300 [00:16<12:06,  2.47s/it]
  2%|▏         | 7/300 [00:18<11:43,  2.40s/it]
  3%|▎         | 8/300 [00:20<11:33,  2.37s/it]
  3%|▎         | 9/300 [00:23<11:29,  2.37s/it]
  3%|▎         | 10/300 [00:25<11:22,  2.35s/it]
  4%|▎         | 11/300 [00:27<11:21,  2.36s/it]
  4%|▍         | 12/300 [00:30<11:20,  2.36s/it]
  4%|▍         | 13/300 [00:32<11:12,  2.34s/it]
  5%|▍         | 14/300 [00:34<11:02,  2.32s/it]
  5%|▌         | 15/300 [00:37<10:53,  2.29s/it]
  5%|▌         | 16/300 [00:39<10:52,  2.30s/it]
  6%|▌         | 17/300 [00:41<10:44,  2.28s/it]
  6%|▌         | 18/300 [00:43<10:37,  2.26s/it]
  6%|▋         | 19/300 [00:46<10:42,  2.29s/it]
  7%|▋         | 20/300 [00:48<10:38,  2.28s/it]
  7%|▋         | 21/300 [00:50<10:39,  2.29s/it]
  7%|▋         | 22/300 [00:53<10:30,  2.27s/it]
  8%|▊         | 23/300 [00:55<10:36,  2.30s/it]
  8%|▊         | 24/300 [00:57<10:27,  2.27s/it]
  8%|▊         | 25/300 [00:59<10:23,  2.27s/it]
  9%|▊         | 26/300 [01:02<10:19,  2.26s/it]
  9%|▉         | 27/300 [01:04<10:26,  2.29s/it]
  9%|▉         | 28/300 [01:06<10:27,  2.31s/it]
 10%|▉         | 29/300 [01:09<10:29,  2.32s/it]
 10%|█         | 30/300 [01:11<10:24,  2.31s/it]
 10%|█         | 31/300 [01:13<10:23,  2.32s/it]
 11%|█         | 32/300 [01:16<10:26,  2.34s/it]
 11%|█         | 33/300 [01:18<10:26,  2.35s/it]
 11%|█▏        | 34/300 [01:20<10:13,  2.31s/it]
 12%|█▏        | 35/300 [01:23<10:06,  2.29s/it]
 12%|█▏        | 36/300 [01:25<10:10,  2.31s/it]
 12%|█▏        | 37/300 [01:27<10:05,  2.30s/it]
 13%|█▎        | 38/300 [01:30<10:06,  2.31s/it]
 13%|█▎        | 39/300 [01:32<10:09,  2.33s/it]
 13%|█▎        | 40/300 [01:34<10:08,  2.34s/it]
 14%|█▎        | 41/300 [01:37<10:09,  2.35s/it]
 14%|█▍        | 42/300 [01:39<10:02,  2.34s/it]
 14%|█▍        | 43/300 [01:41<09:54,  2.31s/it]
 15%|█▍        | 44/300 [01:44<09:51,  2.31s/it]
 15%|█▌        | 45/300 [01:46<09:54,  2.33s/it]
 15%|█▌        | 46/300 [01:48<09:44,  2.30s/it]
 16%|█▌        | 47/300 [01:50<09:38,  2.29s/it]
 16%|█▌        | 48/300 [01:53<09:30,  2.26s/it]
 16%|█▋        | 49/300 [01:55<09:33,  2.28s/it]
 17%|█▋        | 50/300 [01:57<09:32,  2.29s/it]
 17%|█▋        | 51/300 [01:59<09:27,  2.28s/it]
 17%|█▋        | 52/300 [02:02<09:25,  2.28s/it]
 18%|█▊        | 53/300 [02:04<09:29,  2.31s/it]
 18%|█▊        | 54/300 [02:06<09:22,  2.29s/it]
 18%|█▊        | 55/300 [02:09<09:21,  2.29s/it]
 19%|█▊        | 56/300 [02:11<09:21,  2.30s/it]
 19%|█▉        | 57/300 [02:13<09:16,  2.29s/it]
 19%|█▉        | 58/300 [02:16<09:14,  2.29s/it]
 20%|█▉        | 59/300 [02:18<09:22,  2.33s/it]
 20%|██        | 60/300 [02:20<09:19,  2.33s/it]
 20%|██        | 61/300 [02:23<09:17,  2.33s/it]
 21%|██        | 62/300 [02:25<09:11,  2.32s/it]
 21%|██        | 63/300 [02:27<09:12,  2.33s/it]
 21%|██▏       | 64/300 [02:30<09:10,  2.33s/it]
 22%|██▏       | 65/300 [02:32<09:05,  2.32s/it]
 22%|██▏       | 66/300 [02:34<09:04,  2.33s/it]
 22%|██▏       | 67/300 [02:37<08:58,  2.31s/it]
 23%|██▎       | 68/300 [02:39<08:48,  2.28s/it]
 23%|██▎       | 69/300 [02:41<08:43,  2.27s/it]
 23%|██▎       | 70/300 [02:43<08:43,  2.28s/it]
 24%|██▎       | 71/300 [02:46<08:45,  2.29s/it]
 24%|██▍       | 72/300 [02:48<08:48,  2.32s/it]
 24%|██▍       | 73/300 [02:50<08:42,  2.30s/it]
 25%|██▍       | 74/300 [02:53<08:45,  2.33s/it]
 25%|██▌       | 75/300 [02:55<08:39,  2.31s/it]
 25%|██▌       | 76/300 [02:57<08:35,  2.30s/it]
 26%|██▌       | 77/300 [03:00<08:38,  2.33s/it]
 26%|██▌       | 78/300 [03:02<08:31,  2.30s/it]
 26%|██▋       | 79/300 [03:04<08:26,  2.29s/it]
 27%|██▋       | 80/300 [03:06<08:24,  2.29s/it]
 27%|██▋       | 81/300 [03:09<08:27,  2.32s/it]
 27%|██▋       | 82/300 [03:11<08:26,  2.32s/it]
 28%|██▊       | 83/300 [03:13<08:27,  2.34s/it]
 28%|██▊       | 84/300 [03:16<08:25,  2.34s/it]
 28%|██▊       | 85/300 [03:18<08:23,  2.34s/it]
 29%|██▊       | 86/300 [03:20<08:21,  2.35s/it]
 29%|██▉       | 87/300 [03:23<08:19,  2.34s/it]
 29%|██▉       | 88/300 [03:25<08:15,  2.34s/it]
 30%|██▉       | 89/300 [03:27<08:12,  2.33s/it]
 30%|███       | 90/300 [03:30<08:13,  2.35s/it]
 30%|███       | 91/300 [03:32<08:06,  2.33s/it]
 31%|███       | 92/300 [03:34<08:00,  2.31s/it]
 31%|███       | 93/300 [03:37<07:55,  2.30s/it]
 31%|███▏      | 94/300 [03:39<08:00,  2.33s/it]
 32%|███▏      | 95/300 [03:41<07:54,  2.32s/it]
 32%|███▏      | 96/300 [03:44<07:49,  2.30s/it]
 32%|███▏      | 97/300 [03:46<07:51,  2.32s/it]
 33%|███▎      | 98/300 [03:48<07:45,  2.31s/it]
 33%|███▎      | 99/300 [03:51<07:39,  2.29s/it]
 33%|███▎      | 100/300 [03:53<07:38,  2.29s/it]
 34%|███▎      | 101/300 [03:55<07:38,  2.30s/it]
 34%|███▍      | 102/300 [03:57<07:35,  2.30s/it]
 34%|███▍      | 103/300 [04:00<07:38,  2.33s/it]
 35%|███▍      | 104/300 [04:02<07:30,  2.30s/it]
 35%|███▌      | 105/300 [04:04<07:28,  2.30s/it]
 35%|███▌      | 106/300 [04:07<07:23,  2.29s/it]
 36%|███▌      | 107/300 [04:09<07:27,  2.32s/it]
 36%|███▌      | 108/300 [04:11<07:28,  2.34s/it]
 36%|███▋      | 109/300 [04:14<07:29,  2.35s/it]
 37%|███▋      | 110/300 [04:16<07:28,  2.36s/it]
 37%|███▋      | 111/300 [04:18<07:23,  2.34s/it]
 37%|███▋      | 112/300 [04:21<07:20,  2.34s/it]
 38%|███▊      | 113/300 [04:23<07:14,  2.32s/it]
 38%|███▊      | 114/300 [04:25<07:09,  2.31s/it]
 38%|███▊      | 115/300 [04:28<07:02,  2.28s/it]
 39%|███▊      | 116/300 [04:30<07:03,  2.30s/it]
 39%|███▉      | 117/300 [04:32<06:57,  2.28s/it]
 39%|███▉      | 118/300 [04:35<06:57,  2.30s/it]
 40%|███▉      | 119/300 [04:37<06:55,  2.29s/it]
 40%|████      | 120/300 [04:39<06:49,  2.28s/it]
 40%|████      | 121/300 [04:41<06:46,  2.27s/it]
 41%|████      | 122/300 [04:44<06:45,  2.28s/it]
 41%|████      | 123/300 [04:46<06:38,  2.25s/it]
 41%|████▏     | 124/300 [04:48<06:42,  2.29s/it]
 42%|████▏     | 125/300 [04:51<06:44,  2.31s/it]
 42%|████▏     | 126/300 [04:53<06:39,  2.30s/it]
 42%|████▏     | 127/300 [04:55<06:34,  2.28s/it]
 43%|████▎     | 128/300 [04:57<06:35,  2.30s/it]
 43%|████▎     | 129/300 [05:00<06:31,  2.29s/it]
 43%|████▎     | 130/300 [05:02<06:31,  2.30s/it]
 44%|████▎     | 131/300 [05:04<06:27,  2.29s/it]
 44%|████▍     | 132/300 [05:07<06:25,  2.30s/it]
 44%|████▍     | 133/300 [05:09<06:26,  2.31s/it]
 45%|████▍     | 134/300 [05:11<06:26,  2.33s/it]
 45%|████▌     | 135/300 [05:14<06:26,  2.34s/it]
 45%|████▌     | 136/300 [05:16<06:20,  2.32s/it]
 46%|████▌     | 137/300 [05:18<06:17,  2.32s/it]
 46%|████▌     | 138/300 [05:20<06:09,  2.28s/it]
 46%|████▋     | 139/300 [05:23<06:08,  2.29s/it]
 47%|████▋     | 140/300 [05:25<06:07,  2.30s/it]
 47%|████▋     | 141/300 [05:27<06:06,  2.30s/it]
 47%|████▋     | 142/300 [05:30<06:01,  2.29s/it]
 48%|████▊     | 143/300 [05:32<05:57,  2.28s/it]
 48%|████▊     | 144/300 [05:34<05:51,  2.25s/it]
 48%|████▊     | 145/300 [05:36<05:51,  2.27s/it]
 49%|████▊     | 146/300 [05:39<05:50,  2.27s/it]
 49%|████▉     | 147/300 [05:41<05:48,  2.27s/it]
 49%|████▉     | 148/300 [05:43<05:49,  2.30s/it]
 50%|████▉     | 149/300 [05:46<05:46,  2.29s/it]
 50%|█████     | 150/300 [05:48<05:46,  2.31s/it]
 50%|█████     | 151/300 [05:50<05:47,  2.33s/it]
 51%|█████     | 152/300 [05:52<05:39,  2.30s/it]
 51%|█████     | 153/300 [05:55<05:39,  2.31s/it]
 51%|█████▏    | 154/300 [05:57<05:32,  2.28s/it]
 52%|█████▏    | 155/300 [05:59<05:34,  2.31s/it]
 52%|█████▏    | 156/300 [06:02<05:27,  2.28s/it]
 52%|█████▏    | 157/300 [06:04<05:27,  2.29s/it]
 53%|█████▎    | 158/300 [06:06<05:28,  2.31s/it]
 53%|█████▎    | 159/300 [06:09<05:27,  2.32s/it]
 53%|█████▎    | 160/300 [06:11<05:28,  2.35s/it]
 54%|█████▎    | 161/300 [06:13<05:21,  2.31s/it]
 54%|█████▍    | 162/300 [06:16<05:18,  2.31s/it]
 54%|█████▍    | 163/300 [06:18<05:11,  2.27s/it]
 55%|█████▍    | 164/300 [06:20<05:13,  2.30s/it]
 55%|█████▌    | 165/300 [06:22<05:09,  2.29s/it]
 55%|█████▌    | 166/300 [06:25<05:05,  2.28s/it]
 56%|█████▌    | 167/300 [06:27<05:01,  2.27s/it]
 56%|█████▌    | 168/300 [06:29<05:01,  2.28s/it]
 56%|█████▋    | 169/300 [06:31<04:59,  2.28s/it]
 57%|█████▋    | 170/300 [06:34<04:54,  2.26s/it]
 57%|█████▋    | 171/300 [06:36<04:53,  2.28s/it]
 57%|█████▋    | 172/300 [06:38<04:52,  2.29s/it]
 58%|█████▊    | 173/300 [06:41<04:49,  2.28s/it]
 58%|█████▊    | 174/300 [06:43<04:49,  2.30s/it]
 58%|█████▊    | 175/300 [06:45<04:50,  2.33s/it]
 59%|█████▊    | 176/300 [06:48<04:47,  2.32s/it]
 59%|█████▉    | 177/300 [06:50<04:46,  2.33s/it]
 59%|█████▉    | 178/300 [06:52<04:40,  2.30s/it]
 60%|█████▉    | 179/300 [06:55<04:39,  2.31s/it]
 60%|██████    | 180/300 [06:57<04:36,  2.30s/it]
 60%|██████    | 181/300 [06:59<04:31,  2.28s/it]
 61%|██████    | 182/300 [07:01<04:28,  2.28s/it]
 61%|██████    | 183/300 [07:04<04:27,  2.28s/it]
 61%|██████▏   | 184/300 [07:06<04:26,  2.30s/it]
 62%|██████▏   | 185/300 [07:08<04:25,  2.31s/it]
 62%|██████▏   | 186/300 [07:11<04:24,  2.32s/it]
 62%|██████▏   | 187/300 [07:13<04:19,  2.30s/it]
 63%|██████▎   | 188/300 [07:15<04:15,  2.28s/it]
 63%|██████▎   | 189/300 [07:17<04:12,  2.27s/it]
 63%|██████▎   | 190/300 [07:20<04:10,  2.28s/it]
 64%|██████▎   | 191/300 [07:22<04:09,  2.29s/it]
 64%|██████▍   | 192/300 [07:24<04:06,  2.29s/it]
 64%|██████▍   | 193/300 [07:27<04:06,  2.30s/it]
 65%|██████▍   | 194/300 [07:29<04:03,  2.30s/it]
 65%|██████▌   | 195/300 [07:31<04:01,  2.30s/it]
 65%|██████▌   | 196/300 [07:33<03:58,  2.30s/it]
 66%|██████▌   | 197/300 [07:36<03:57,  2.30s/it]
 66%|██████▌   | 198/300 [07:38<03:53,  2.29s/it]
 66%|██████▋   | 199/300 [07:40<03:51,  2.29s/it]
 67%|██████▋   | 200/300 [07:43<03:51,  2.31s/it]
 67%|██████▋   | 201/300 [07:45<03:51,  2.34s/it]
 67%|██████▋   | 202/300 [07:47<03:45,  2.30s/it]
 68%|██████▊   | 203/300 [07:50<03:44,  2.31s/it]
 68%|██████▊   | 204/300 [07:52<03:39,  2.29s/it]
 68%|██████▊   | 205/300 [07:54<03:36,  2.28s/it]
 69%|██████▊   | 206/300 [07:56<03:34,  2.28s/it]
 69%|██████▉   | 207/300 [07:59<03:30,  2.26s/it]
 69%|██████▉   | 208/300 [08:01<03:31,  2.30s/it]
 70%|██████▉   | 209/300 [08:03<03:30,  2.32s/it]
 70%|███████   | 210/300 [08:06<03:28,  2.31s/it]
 70%|███████   | 211/300 [08:08<03:24,  2.30s/it]
 71%|███████   | 212/300 [08:10<03:19,  2.27s/it]
 71%|███████   | 213/300 [08:12<03:15,  2.24s/it]
 71%|███████▏  | 214/300 [08:15<03:15,  2.28s/it]
 72%|███████▏  | 215/300 [08:17<03:14,  2.29s/it]
 72%|███████▏  | 216/300 [08:19<03:13,  2.30s/it]
 72%|███████▏  | 217/300 [08:22<03:12,  2.31s/it]
 73%|███████▎  | 218/300 [08:24<03:12,  2.34s/it]
 73%|███████▎  | 219/300 [08:27<03:12,  2.38s/it]
 73%|███████▎  | 220/300 [08:29<03:12,  2.40s/it]
 74%|███████▎  | 221/300 [08:31<03:08,  2.39s/it]
 74%|███████▍  | 222/300 [08:34<03:03,  2.35s/it]
 74%|███████▍  | 223/300 [08:36<02:58,  2.32s/it]
 75%|███████▍  | 224/300 [08:38<02:54,  2.30s/it]
 75%|███████▌  | 225/300 [08:41<02:53,  2.31s/it]
 75%|███████▌  | 226/300 [08:43<02:50,  2.31s/it]
 76%|███████▌  | 227/300 [08:45<02:47,  2.30s/it]
 76%|███████▌  | 228/300 [08:47<02:46,  2.31s/it]
 76%|███████▋  | 229/300 [08:50<02:44,  2.32s/it]
 77%|███████▋  | 230/300 [08:52<02:41,  2.30s/it]
 77%|███████▋  | 231/300 [08:54<02:38,  2.29s/it]
 77%|███████▋  | 232/300 [08:57<02:35,  2.29s/it]
 78%|███████▊  | 233/300 [08:59<02:32,  2.28s/it]
 78%|███████▊  | 234/300 [09:01<02:29,  2.26s/it]
 78%|███████▊  | 235/300 [09:03<02:28,  2.28s/it]
 79%|███████▊  | 236/300 [09:06<02:26,  2.29s/it]
 79%|███████▉  | 237/300 [09:08<02:24,  2.30s/it]
 79%|███████▉  | 238/300 [09:10<02:22,  2.30s/it]
 80%|███████▉  | 239/300 [09:13<02:18,  2.28s/it]
 80%|████████  | 240/300 [09:15<02:16,  2.27s/it]
 80%|████████  | 241/300 [09:17<02:13,  2.27s/it]
 81%|████████  | 242/300 [09:19<02:13,  2.30s/it]
 81%|████████  | 243/300 [09:22<02:12,  2.32s/it]
 81%|████████▏ | 244/300 [09:24<02:09,  2.32s/it]
 82%|████████▏ | 245/300 [09:26<02:08,  2.34s/it]
 82%|████████▏ | 246/300 [09:29<02:06,  2.35s/it]
 82%|████████▏ | 247/300 [09:31<02:04,  2.36s/it]
 83%|████████▎ | 248/300 [09:34<02:03,  2.38s/it]
 83%|████████▎ | 249/300 [09:36<02:00,  2.37s/it]
 83%|████████▎ | 250/300 [09:38<01:58,  2.36s/it]
 84%|████████▎ | 251/300 [09:41<01:55,  2.36s/it]
 84%|████████▍ | 252/300 [09:43<01:52,  2.33s/it]
 84%|████████▍ | 253/300 [09:45<01:50,  2.34s/it]
 85%|████████▍ | 254/300 [09:48<01:49,  2.37s/it]
 85%|████████▌ | 255/300 [09:50<01:46,  2.36s/it]
 85%|████████▌ | 256/300 [09:53<01:44,  2.37s/it]
 86%|████████▌ | 257/300 [09:55<01:40,  2.33s/it]
 86%|████████▌ | 258/300 [09:57<01:36,  2.31s/it]
 86%|████████▋ | 259/300 [09:59<01:35,  2.33s/it]
 87%|████████▋ | 260/300 [10:02<01:33,  2.34s/it]
 87%|████████▋ | 261/300 [10:04<01:30,  2.32s/it]
 87%|████████▋ | 262/300 [10:06<01:27,  2.31s/it]
 88%|████████▊ | 263/300 [10:09<01:24,  2.30s/it]
 88%|████████▊ | 264/300 [10:11<01:23,  2.31s/it]
 88%|████████▊ | 265/300 [10:13<01:22,  2.34s/it]
 89%|████████▊ | 266/300 [10:16<01:18,  2.32s/it]
 89%|████████▉ | 267/300 [10:18<01:16,  2.33s/it]
 89%|████████▉ | 268/300 [10:20<01:14,  2.34s/it]
 90%|████████▉ | 269/300 [10:23<01:12,  2.33s/it]
 90%|█████████ | 270/300 [10:25<01:09,  2.33s/it]
 90%|█████████ | 271/300 [10:27<01:07,  2.32s/it]
 91%|█████████ | 272/300 [10:30<01:05,  2.34s/it]
 91%|█████████ | 273/300 [10:32<01:02,  2.33s/it]
 91%|█████████▏| 274/300 [10:34<01:01,  2.35s/it]
 92%|█████████▏| 275/300 [10:37<00:58,  2.33s/it]
 92%|█████████▏| 276/300 [10:39<00:55,  2.33s/it]
 92%|█████████▏| 277/300 [10:41<00:53,  2.33s/it]
 93%|█████████▎| 278/300 [10:44<00:50,  2.30s/it]
 93%|█████████▎| 279/300 [10:46<00:48,  2.31s/it]
 93%|█████████▎| 280/300 [10:48<00:46,  2.31s/it]
 94%|█████████▎| 281/300 [10:50<00:43,  2.30s/it]
 94%|█████████▍| 282/300 [10:53<00:41,  2.30s/it]
 94%|█████████▍| 283/300 [10:55<00:38,  2.29s/it]
 95%|█████████▍| 284/300 [10:57<00:36,  2.30s/it]
 95%|█████████▌| 285/300 [11:00<00:34,  2.30s/it]
 95%|█████████▌| 286/300 [11:02<00:32,  2.32s/it]
 96%|█████████▌| 287/300 [11:04<00:30,  2.33s/it]
 96%|█████████▌| 288/300 [11:07<00:27,  2.32s/it]
 96%|█████████▋| 289/300 [11:09<00:25,  2.33s/it]
 97%|█████████▋| 290/300 [11:11<00:23,  2.34s/it]
 97%|█████████▋| 291/300 [11:14<00:20,  2.30s/it]
 97%|█████████▋| 292/300 [11:16<00:18,  2.32s/it]
 98%|█████████▊| 293/300 [11:18<00:16,  2.30s/it]
 98%|█████████▊| 294/300 [11:20<00:13,  2.27s/it]
 98%|█████████▊| 295/300 [11:23<00:11,  2.26s/it]
 99%|█████████▊| 296/300 [11:25<00:09,  2.29s/it]
 99%|█████████▉| 297/300 [11:27<00:06,  2.31s/it]
 99%|█████████▉| 298/300 [11:30<00:04,  2.29s/it]
100%|█████████▉| 299/300 [11:32<00:02,  2.27s/it]
100%|██████████| 300/300 [11:34<00:00,  2.24s/it]
100%|██████████| 300/300 [11:34<00:00,  2.32s/it]
W&B disabled.
W&B online. Running your script from this directory will now sync to the cloud.
wandb: Currently logged in as: hl57. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.12 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.10
wandb: Run data is saved locally in /media/techt/One Touch/DD/SRe2L/train/wandb/run-20231101_074329-vs6pa7ll
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run polished-cosmos-608
wandb: ⭐️ View project at https://wandb.ai/hl57/final_rn18_fkd
wandb: 🚀 View run at https://wandb.ai/hl57/final_rn18_fkd/runs/vs6pa7ll
======= FKD: dataset info ======
path: ../relabel/FKD_cutmix_fp16/tiny-imagenet/478/
num img: 2000
batch size: 100
max epoch: 300
================================
load data successfully
=> loading student model 'resnet18'
TRAIN Iter 0: lr = 0.001000,	loss = 0.009722,	Top-1 err = 98.850000,	Top-5 err = 96.250000,	train_time = 16.835319
TEST Iter 0: loss = 6.727718,	Top-1 err = 99.420000,	Top-5 err = 95.820000,	val_time = 19.140801
TRAIN Iter 10: lr = 0.000997,	loss = 0.007373,	Top-1 err = 97.500000,	Top-5 err = 88.750000,	train_time = 15.409250
TEST Iter 10: loss = 5.501831,	Top-1 err = 97.400000,	Top-5 err = 89.460000,	val_time = 19.218046
TRAIN Iter 20: lr = 0.000989,	loss = 0.007025,	Top-1 err = 95.450000,	Top-5 err = 82.100000,	train_time = 15.464231
TEST Iter 20: loss = 5.047865,	Top-1 err = 95.510000,	Top-5 err = 83.250000,	val_time = 19.559510
TRAIN Iter 30: lr = 0.000976,	loss = 0.006236,	Top-1 err = 93.600000,	Top-5 err = 81.000000,	train_time = 15.543191
TEST Iter 30: loss = 4.567906,	Top-1 err = 92.590000,	Top-5 err = 75.930000,	val_time = 19.190652
TRAIN Iter 40: lr = 0.000957,	loss = 0.005887,	Top-1 err = 93.250000,	Top-5 err = 78.650000,	train_time = 15.471816
TEST Iter 40: loss = 4.910592,	Top-1 err = 91.670000,	Top-5 err = 75.520000,	val_time = 19.129143
TRAIN Iter 50: lr = 0.000933,	loss = 0.005679,	Top-1 err = 90.300000,	Top-5 err = 73.300000,	train_time = 15.507677
TEST Iter 50: loss = 4.559182,	Top-1 err = 89.260000,	Top-5 err = 70.830000,	val_time = 19.449670
TRAIN Iter 60: lr = 0.000905,	loss = 0.005584,	Top-1 err = 86.050000,	Top-5 err = 66.100000,	train_time = 15.473752
TEST Iter 60: loss = 4.180168,	Top-1 err = 86.950000,	Top-5 err = 65.790000,	val_time = 19.543088
TRAIN Iter 70: lr = 0.000872,	loss = 0.005136,	Top-1 err = 84.550000,	Top-5 err = 61.800000,	train_time = 15.429327
TEST Iter 70: loss = 4.355643,	Top-1 err = 86.610000,	Top-5 err = 64.020000,	val_time = 19.511933
TRAIN Iter 80: lr = 0.000835,	loss = 0.004781,	Top-1 err = 85.900000,	Top-5 err = 65.750000,	train_time = 15.448996
TEST Iter 80: loss = 4.012448,	Top-1 err = 83.560000,	Top-5 err = 59.770000,	val_time = 19.258970
TRAIN Iter 90: lr = 0.000794,	loss = 0.004487,	Top-1 err = 80.200000,	Top-5 err = 56.650000,	train_time = 15.466556
TEST Iter 90: loss = 3.811099,	Top-1 err = 81.840000,	Top-5 err = 57.030000,	val_time = 19.234698
TRAIN Iter 100: lr = 0.000750,	loss = 0.004330,	Top-1 err = 81.800000,	Top-5 err = 62.000000,	train_time = 15.557610
TEST Iter 100: loss = 3.559011,	Top-1 err = 79.030000,	Top-5 err = 52.620000,	val_time = 19.516669
TRAIN Iter 110: lr = 0.000703,	loss = 0.004277,	Top-1 err = 70.450000,	Top-5 err = 44.000000,	train_time = 15.512779
TEST Iter 110: loss = 3.758915,	Top-1 err = 79.810000,	Top-5 err = 53.380000,	val_time = 19.536882
TRAIN Iter 120: lr = 0.000655,	loss = 0.003970,	Top-1 err = 72.950000,	Top-5 err = 48.150000,	train_time = 15.505990
TEST Iter 120: loss = 3.523423,	Top-1 err = 77.320000,	Top-5 err = 50.590000,	val_time = 19.502754
TRAIN Iter 130: lr = 0.000604,	loss = 0.003645,	Top-1 err = 74.350000,	Top-5 err = 52.250000,	train_time = 15.590491
TEST Iter 130: loss = 3.486949,	Top-1 err = 75.430000,	Top-5 err = 47.830000,	val_time = 19.357008
TRAIN Iter 140: lr = 0.000552,	loss = 0.003492,	Top-1 err = 75.700000,	Top-5 err = 55.000000,	train_time = 15.499509
TEST Iter 140: loss = 3.130759,	Top-1 err = 71.610000,	Top-5 err = 42.590000,	val_time = 19.332318
TRAIN Iter 150: lr = 0.000500,	loss = 0.003508,	Top-1 err = 70.750000,	Top-5 err = 47.950000,	train_time = 15.449596
TEST Iter 150: loss = 3.191905,	Top-1 err = 70.670000,	Top-5 err = 42.040000,	val_time = 19.626876
TRAIN Iter 160: lr = 0.000448,	loss = 0.003260,	Top-1 err = 74.100000,	Top-5 err = 53.150000,	train_time = 15.522414
TEST Iter 160: loss = 3.080063,	Top-1 err = 69.330000,	Top-5 err = 41.220000,	val_time = 19.398934
TRAIN Iter 170: lr = 0.000396,	loss = 0.003139,	Top-1 err = 62.200000,	Top-5 err = 38.250000,	train_time = 15.431759
TEST Iter 170: loss = 2.989510,	Top-1 err = 67.550000,	Top-5 err = 39.000000,	val_time = 19.423532
TRAIN Iter 180: lr = 0.000345,	loss = 0.002980,	Top-1 err = 69.400000,	Top-5 err = 43.800000,	train_time = 15.483639
TEST Iter 180: loss = 2.943667,	Top-1 err = 67.290000,	Top-5 err = 38.460000,	val_time = 19.480141
TRAIN Iter 190: lr = 0.000297,	loss = 0.002930,	Top-1 err = 67.150000,	Top-5 err = 43.950000,	train_time = 15.705966
TEST Iter 190: loss = 2.838384,	Top-1 err = 65.950000,	Top-5 err = 36.450000,	val_time = 19.578879
TRAIN Iter 200: lr = 0.000250,	loss = 0.002798,	Top-1 err = 63.150000,	Top-5 err = 40.450000,	train_time = 15.502139
TEST Iter 200: loss = 2.739129,	Top-1 err = 63.840000,	Top-5 err = 34.370000,	val_time = 19.365098
TRAIN Iter 210: lr = 0.000206,	loss = 0.002750,	Top-1 err = 66.250000,	Top-5 err = 41.950000,	train_time = 15.452873
TEST Iter 210: loss = 2.666332,	Top-1 err = 62.710000,	Top-5 err = 33.310000,	val_time = 19.483022
TRAIN Iter 220: lr = 0.000165,	loss = 0.002682,	Top-1 err = 63.850000,	Top-5 err = 42.100000,	train_time = 15.507039
TEST Iter 220: loss = 2.578964,	Top-1 err = 61.360000,	Top-5 err = 31.670000,	val_time = 19.422502
TRAIN Iter 230: lr = 0.000128,	loss = 0.002695,	Top-1 err = 59.850000,	Top-5 err = 38.700000,	train_time = 15.542928
TEST Iter 230: loss = 2.565645,	Top-1 err = 61.180000,	Top-5 err = 31.520000,	val_time = 19.425621
TRAIN Iter 240: lr = 0.000095,	loss = 0.002637,	Top-1 err = 63.550000,	Top-5 err = 40.700000,	train_time = 15.416321
TEST Iter 240: loss = 2.540908,	Top-1 err = 60.310000,	Top-5 err = 31.070000,	val_time = 19.565890
TRAIN Iter 250: lr = 0.000067,	loss = 0.002499,	Top-1 err = 72.300000,	Top-5 err = 52.600000,	train_time = 15.494253
TEST Iter 250: loss = 2.496178,	Top-1 err = 59.690000,	Top-5 err = 30.270000,	val_time = 19.492591
TRAIN Iter 260: lr = 0.000043,	loss = 0.002488,	Top-1 err = 81.450000,	Top-5 err = 59.400000,	train_time = 15.577609
TEST Iter 260: loss = 2.474144,	Top-1 err = 59.140000,	Top-5 err = 29.820000,	val_time = 19.305064
TRAIN Iter 270: lr = 0.000024,	loss = 0.002542,	Top-1 err = 70.900000,	Top-5 err = 49.050000,	train_time = 15.608490
TEST Iter 270: loss = 2.478756,	Top-1 err = 59.210000,	Top-5 err = 30.050000,	val_time = 19.335803
TRAIN Iter 280: lr = 0.000011,	loss = 0.002512,	Top-1 err = 55.250000,	Top-5 err = 32.250000,	train_time = 15.452732
TEST Iter 280: loss = 2.465735,	Top-1 err = 58.760000,	Top-5 err = 29.710000,	val_time = 19.517920
TRAIN Iter 290: lr = 0.000003,	loss = 0.002504,	Top-1 err = 67.250000,	Top-5 err = 44.600000,	train_time = 15.524090
TEST Iter 290: loss = 2.473302,	Top-1 err = 58.890000,	Top-5 err = 29.940000,	val_time = 19.628120
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)
wandb: \ 0.004 MB of 0.026 MB uploaded (0.000 MB deduped)
wandb: | 0.004 MB of 0.026 MB uploaded (0.000 MB deduped)
wandb: 
wandb: Run history:
wandb:  train/Top1 ▁▁▁▂▂▂▃▂▃▃▄▄▄▄▅▅▄▇▅▆▆▆▇▇▇██▅▇▆▇▇▇▅▇▅▆▇██
wandb:  train/Top5 ▁▁▂▂▃▃▄▃▅▄▅▅▅▆▆▅▅▇▅▆▇▆▇▇▇██▆▇▆▇▇▇▅█▆▆▇█▇
wandb: train/epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  train/loss █▇▇▆▆▅▅▅▅▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    train/lr ███████▇▇▇▇▇▇▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁
wandb:   val/epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███
wandb:    val/loss █▆▅▄▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁
wandb:    val/top1 ▁▁▂▂▂▃▃▃▄▄▅▄▅▅▆▆▆▆▇▇▇▇█████████
wandb:    val/top5 ▁▂▂▃▃▄▄▄▅▅▆▅▆▆▇▇▇▇▇▇███████████
wandb: 
wandb: Run summary:
wandb:  train/Top1 40.35
wandb:  train/Top5 59.95
wandb: train/epoch 299
wandb:  train/loss 0.00253
wandb:    train/lr 0.0
wandb:   val/epoch 299
wandb:    val/loss 2.47141
wandb:    val/top1 40.98
wandb:    val/top5 70.16
wandb: 
wandb: 🚀 View run polished-cosmos-608 at: https://wandb.ai/hl57/final_rn18_fkd/runs/vs6pa7ll
wandb: ️⚡ View job at https://wandb.ai/hl57/final_rn18_fkd/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjk3MzkyNjU3/version_details/v55
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231101_074329-vs6pa7ll/logs
TEST Iter 299: loss = 2.471409,	Top-1 err = 59.020000,	Top-5 err = 29.840000,	val_time = 19.489580

# renamed wit '_local' to avoid conflict with an experiment on the galaxy server, about baseline ImageNet-1k 50 ipc.