/home/techt/anaconda3/envs/hl/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/techt/anaconda3/envs/hl/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
/home/techt/anaconda3/envs/hl/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
r_bn:  0.01
lr:  0.25
ipc_id =  0
get_images call
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
------------iteration 0----------
total loss 25.02414894104004
main criterion 3.0069849491119385
weighted_aux_loss 22.01716423034668
loss_r_bn_feature 2201.716552734375
------------iteration 100----------
total loss 4.844557285308838
main criterion 0.024203244596719742
weighted_aux_loss 4.820353984832764
loss_r_bn_feature 482.035400390625
------------iteration 200----------
total loss 3.1852681636810303
main criterion 0.008712021633982658
weighted_aux_loss 3.17655611038208
loss_r_bn_feature 317.6556091308594
------------iteration 300----------
total loss 3.012070417404175
main criterion 0.03247401863336563
weighted_aux_loss 2.9795963764190674
loss_r_bn_feature 297.95965576171875
------------iteration 400----------
total loss 2.5227413177490234
main criterion 0.0006419173441827297
weighted_aux_loss 2.522099494934082
loss_r_bn_feature 252.2099609375
------------iteration 500----------
total loss 3.9742376804351807
main criterion 0.00552776176482439
weighted_aux_loss 3.968709945678711
loss_r_bn_feature 396.8710021972656
------------iteration 600----------
total loss 4.225304126739502
main criterion 0.40934890508651733
weighted_aux_loss 3.81595516204834
loss_r_bn_feature 381.59552001953125
------------iteration 700----------
total loss 2.7391343116760254
main criterion 0.0023539941757917404
weighted_aux_loss 2.7367804050445557
loss_r_bn_feature 273.67803955078125
------------iteration 800----------
total loss 2.595994472503662
main criterion 0.025568461045622826
weighted_aux_loss 2.5704259872436523
loss_r_bn_feature 257.0426025390625
------------iteration 900----------
total loss 1.9789342880249023
main criterion 0.0012298393994569778
weighted_aux_loss 1.977704405784607
loss_r_bn_feature 197.77044677734375
------------iteration 1000----------
total loss 2.085103750228882
main criterion 0.002653190866112709
weighted_aux_loss 2.0824506282806396
loss_r_bn_feature 208.2450714111328
------------iteration 1100----------
total loss 1.7507153749465942
main criterion 0.0003359528782311827
weighted_aux_loss 1.7503794431686401
loss_r_bn_feature 175.03794860839844
------------iteration 1200----------
total loss 2.3279991149902344
main criterion 0.0011117650428786874
weighted_aux_loss 2.326887369155884
loss_r_bn_feature 232.68875122070312
------------iteration 1300----------
total loss 1.7358249425888062
main criterion 0.010876702144742012
weighted_aux_loss 1.7249482870101929
loss_r_bn_feature 172.4948272705078
------------iteration 1400----------
total loss 1.2638393640518188
main criterion 0.0005917457165196538
weighted_aux_loss 1.2632476091384888
loss_r_bn_feature 126.32476043701172
------------iteration 1500----------
total loss 0.8813522458076477
main criterion 0.0017859057988971472
weighted_aux_loss 0.8795663118362427
loss_r_bn_feature 87.95663452148438
------------iteration 1600----------
total loss 0.8690248131752014
main criterion 0.0008998399716801941
weighted_aux_loss 0.8681249618530273
loss_r_bn_feature 86.8125
------------iteration 1700----------
total loss 0.9020567536354065
main criterion 0.000807237986009568
weighted_aux_loss 0.9012495279312134
loss_r_bn_feature 90.12495422363281
------------iteration 1800----------
total loss 2.5526530742645264
main criterion 0.022424966096878052
weighted_aux_loss 2.5302281379699707
loss_r_bn_feature 253.0228271484375
------------iteration 1900----------
total loss 0.6013997197151184
main criterion 0.00021982347243465483
weighted_aux_loss 0.6011798977851868
loss_r_bn_feature 60.11798858642578
ipc_id =  1
get_images call
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
------------iteration 0----------
total loss 24.773441314697266
main criterion 3.0312960147857666
weighted_aux_loss 21.742145538330078
loss_r_bn_feature 2174.214599609375
------------iteration 100----------
total loss 4.449574947357178
main criterion 0.40918198227882385
weighted_aux_loss 4.040392875671387
loss_r_bn_feature 404.039306640625
------------iteration 200----------
total loss 4.290410995483398
main criterion 0.05865404009819031
weighted_aux_loss 4.231757164001465
loss_r_bn_feature 423.17572021484375
------------iteration 300----------
total loss 3.2826013565063477
main criterion 0.12615899741649628
weighted_aux_loss 3.156442403793335
loss_r_bn_feature 315.6442565917969
------------iteration 400----------
total loss 2.3903651237487793
main criterion 0.0014051119796931744
weighted_aux_loss 2.388960123062134
loss_r_bn_feature 238.89601135253906
------------iteration 500----------
total loss 2.4415738582611084
main criterion 0.005918459035456181
weighted_aux_loss 2.435655355453491
loss_r_bn_feature 243.56553649902344
------------iteration 600----------
total loss 2.5674328804016113
main criterion 0.0012434950331225991
weighted_aux_loss 2.5661892890930176
loss_r_bn_feature 256.6189270019531
------------iteration 700----------
total loss 3.0037150382995605
main criterion 0.11555049568414688
weighted_aux_loss 2.888164520263672
loss_r_bn_feature 288.81646728515625
------------iteration 800----------
total loss 3.2556893825531006
main criterion 0.004154276102781296
weighted_aux_loss 3.251535177230835
loss_r_bn_feature 325.1535339355469
------------iteration 900----------
total loss 1.7424066066741943
main criterion 0.004818163346499205
weighted_aux_loss 1.7375884056091309
loss_r_bn_feature 173.75885009765625
------------iteration 1000----------
total loss 2.3355553150177
main criterion 0.007656170520931482
weighted_aux_loss 2.327899217605591
loss_r_bn_feature 232.78993225097656
------------iteration 1100----------
total loss 4.6032233238220215
main criterion 0.23168770968914032
weighted_aux_loss 4.371535778045654
loss_r_bn_feature 437.153564453125
------------iteration 1200----------
total loss 1.3261076211929321
main criterion 0.0005583731108345091
weighted_aux_loss 1.3255492448806763
loss_r_bn_feature 132.554931640625
------------iteration 1300----------
total loss 1.1865181922912598
main criterion 0.0004812431870959699
weighted_aux_loss 1.1860369443893433
loss_r_bn_feature 118.60369873046875
------------iteration 1400----------
total loss 1.5479387044906616
main criterion 0.0008744829101487994
weighted_aux_loss 1.547064185142517
loss_r_bn_feature 154.7064208984375
------------iteration 1500----------
total loss 1.296531319618225
main criterion 0.0029046335257589817
weighted_aux_loss 1.2936266660690308
loss_r_bn_feature 129.3626708984375
------------iteration 1600----------
total loss 0.9412661194801331
main criterion 0.0010327966883778572
weighted_aux_loss 0.9402333498001099
loss_r_bn_feature 94.0233383178711
------------iteration 1700----------
total loss 0.777118444442749
main criterion 0.0012885924661532044
weighted_aux_loss 0.7758298516273499
loss_r_bn_feature 77.5829849243164
------------iteration 1800----------
total loss 0.9274290204048157
main criterion 0.001112871803343296
weighted_aux_loss 0.9263161420822144
loss_r_bn_feature 92.6316146850586
------------iteration 1900----------
total loss 0.7639524936676025
main criterion 0.003294680267572403
weighted_aux_loss 0.760657787322998
loss_r_bn_feature 76.06578063964844
ipc_id =  2
get_images call
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
------------iteration 0----------
total loss 26.26667594909668
main criterion 3.2661309242248535
weighted_aux_loss 23.000545501708984
loss_r_bn_feature 2300.0546875
------------iteration 100----------
total loss 3.832181692123413
main criterion 0.029871677979826927
weighted_aux_loss 3.802309989929199
loss_r_bn_feature 380.23101806640625
------------iteration 200----------
total loss 2.884336233139038
main criterion 0.0025619198568165302
weighted_aux_loss 2.881774425506592
loss_r_bn_feature 288.1774597167969
------------iteration 300----------
total loss 3.1763930320739746
main criterion 0.0011749317636713386
weighted_aux_loss 3.175218105316162
loss_r_bn_feature 317.5218200683594
------------iteration 400----------
total loss 2.2372429370880127
main criterion 0.0011011280585080385
weighted_aux_loss 2.2361419200897217
loss_r_bn_feature 223.61419677734375
------------iteration 500----------
total loss 2.39139723777771
main criterion 0.005036638118326664
weighted_aux_loss 2.3863606452941895
loss_r_bn_feature 238.6360626220703
------------iteration 600----------
total loss 4.03084659576416
main criterion 0.0834977775812149
weighted_aux_loss 3.9473485946655273
loss_r_bn_feature 394.73486328125
------------iteration 700----------
total loss 2.3410871028900146
main criterion 0.020392082631587982
weighted_aux_loss 2.320694923400879
loss_r_bn_feature 232.0695037841797
------------iteration 800----------
total loss 2.1375834941864014
main criterion 0.003190046176314354
weighted_aux_loss 2.1343934535980225
loss_r_bn_feature 213.43934631347656
------------iteration 900----------
total loss 3.928591728210449
main criterion 0.08165334910154343
weighted_aux_loss 3.846938371658325
loss_r_bn_feature 384.69384765625
------------iteration 1000----------
total loss 1.8389171361923218
main criterion 0.0037260353565216064
weighted_aux_loss 1.8351911306381226
loss_r_bn_feature 183.5191192626953
------------iteration 1100----------
total loss 1.2628170251846313
main criterion 0.006966319866478443
weighted_aux_loss 1.2558506727218628
loss_r_bn_feature 125.58507537841797
------------iteration 1200----------
total loss 1.3100411891937256
main criterion 0.0009582781349308789
weighted_aux_loss 1.3090828657150269
loss_r_bn_feature 130.90829467773438
------------iteration 1300----------
total loss 1.6312789916992188
main criterion 0.0015665965620428324
weighted_aux_loss 1.6297123432159424
loss_r_bn_feature 162.9712371826172
------------iteration 1400----------
total loss 2.870277166366577
main criterion 0.06560564786195755
weighted_aux_loss 2.8046715259552
loss_r_bn_feature 280.4671630859375
------------iteration 1500----------
total loss 0.860514760017395
main criterion 0.0012439470738172531
weighted_aux_loss 0.8592708110809326
loss_r_bn_feature 85.92708587646484
------------iteration 1600----------
total loss 1.1593791246414185
main criterion 0.0008582526934333146
weighted_aux_loss 1.1585208177566528
loss_r_bn_feature 115.85208129882812
------------iteration 1700----------
total loss 2.1457695960998535
main criterion 0.10136441886425018
weighted_aux_loss 2.044405221939087
loss_r_bn_feature 204.44053649902344
------------iteration 1800----------
total loss 2.9729490280151367
main criterion 0.06345333904027939
weighted_aux_loss 2.9094955921173096
loss_r_bn_feature 290.9495544433594
------------iteration 1900----------
total loss 0.6540921330451965
main criterion 0.002009873976930976
weighted_aux_loss 0.6520822644233704
loss_r_bn_feature 65.2082290649414
ipc_id =  3
get_images call
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
------------iteration 0----------
total loss 27.36099624633789
main criterion 4.02249813079834
weighted_aux_loss 23.338497161865234
loss_r_bn_feature 2333.849853515625
Traceback (most recent call last):
  File "data_synthesis.py", line 395, in <module>
    main_syn(args)
  File "data_synthesis.py", line 293, in main_syn
    get_images(args, model_teacher, hook_for_display, ipc_id, bc_i=bc_i)
  File "data_synthesis.py", line 116, in get_images
    outputs = model_teacher(inputs_jit)
  File "/home/techt/anaconda3/envs/hl/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/techt/anaconda3/envs/hl/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 169, in forward
    return self.module(*inputs[0], **kwargs[0])
  File "/home/techt/anaconda3/envs/hl/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/media/techt/One Touch/DD/SRe2L/recover/../models/resnet.py", line 215, in forward
    x = self.layer3(x, labels=labels)
  File "/home/techt/anaconda3/envs/hl/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/media/techt/One Touch/DD/SRe2L/recover/../models/resnet.py", line 250, in forward
    input = module(input, labels=labels)
  File "/home/techt/anaconda3/envs/hl/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/media/techt/One Touch/DD/SRe2L/recover/../models/resnet.py", line 61, in forward
    identity = self.downsample(x)
  File "/home/techt/anaconda3/envs/hl/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/media/techt/One Touch/DD/SRe2L/recover/../models/resnet.py", line 235, in forward
    x = self.norm(x)
  File "/home/techt/anaconda3/envs/hl/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/techt/anaconda3/envs/hl/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py", line 171, in forward
    return F.batch_norm(
  File "/home/techt/anaconda3/envs/hl/lib/python3.8/site-packages/torch/nn/functional.py", line 2450, in batch_norm
    return torch.batch_norm(
KeyboardInterrupt
