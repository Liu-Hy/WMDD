/home/techt/anaconda3/envs/hl/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/techt/anaconda3/envs/hl/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
/home/techt/anaconda3/envs/hl/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
r_bn:  0.01
lr:  0.25
bc shape torch.Size([10, 10, 512])
get_images call
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 116.36260608738431
main criterion 92.5704212576724
weighted_aux_loss 23.792184829711914
loss_r_bn_feature 2379.218505859375
------------iteration 100----------
total loss 54.27863432498998
main criterion 35.955035463295644
weighted_aux_loss 18.323598861694336
loss_r_bn_feature 1832.3599853515625
------------iteration 200----------
total loss 56.78741348375016
main criterion 39.360756760727696
weighted_aux_loss 17.42665672302246
loss_r_bn_feature 1742.665771484375
------------iteration 300----------
total loss 46.73365735892639
main criterion 30.835230349343874
weighted_aux_loss 15.89842700958252
loss_r_bn_feature 1589.8427734375
------------iteration 400----------
total loss 65.7106753388426
main criterion 50.3414412537596
weighted_aux_loss 15.369234085083008
loss_r_bn_feature 1536.9234619140625
------------iteration 500----------
total loss 44.18057875424034
main criterion 30.044760272795024
weighted_aux_loss 14.135818481445312
loss_r_bn_feature 1413.5819091796875
------------iteration 600----------
total loss 43.3003726450357
main criterion 30.498595759049373
weighted_aux_loss 12.801776885986328
loss_r_bn_feature 1280.177734375
------------iteration 700----------
total loss 44.237980020978675
main criterion 31.327895296552406
weighted_aux_loss 12.91008472442627
loss_r_bn_feature 1291.008544921875
------------iteration 800----------
total loss 39.72681529869426
main criterion 27.323184088367114
weighted_aux_loss 12.403631210327148
loss_r_bn_feature 1240.3631591796875
------------iteration 900----------
total loss 34.902850730446644
main criterion 24.004814727333855
weighted_aux_loss 10.898036003112793
loss_r_bn_feature 1089.8035888671875
------------iteration 1000----------
total loss 50.36434353112312
main criterion 40.26849735497566
weighted_aux_loss 10.095846176147461
loss_r_bn_feature 1009.5845947265625
------------iteration 1100----------
total loss 35.54076902138718
main criterion 26.075617234582978
weighted_aux_loss 9.4651517868042
loss_r_bn_feature 946.5151977539062
------------iteration 1200----------
total loss 30.392675125211582
main criterion 21.912529670804844
weighted_aux_loss 8.480145454406738
loss_r_bn_feature 848.0145263671875
------------iteration 1300----------
total loss 32.36979601411268
main criterion 25.34241840867445
weighted_aux_loss 7.027377605438232
loss_r_bn_feature 702.73779296875
------------iteration 1400----------
total loss 26.5107396981323
main criterion 20.48147238703611
weighted_aux_loss 6.029267311096191
loss_r_bn_feature 602.9267578125
------------iteration 1500----------
total loss 39.48879273571176
main criterion 33.69450648464365
weighted_aux_loss 5.794286251068115
loss_r_bn_feature 579.4286499023438
------------iteration 1600----------
total loss 20.964330127414296
main criterion 15.828182628329824
weighted_aux_loss 5.136147499084473
loss_r_bn_feature 513.61474609375
------------iteration 1700----------
total loss 31.675527942718634
main criterion 26.960112941803107
weighted_aux_loss 4.715415000915527
loss_r_bn_feature 471.54150390625
------------iteration 1800----------
total loss 19.438194316498404
main criterion 15.060043376556997
weighted_aux_loss 4.378150939941406
loss_r_bn_feature 437.8150939941406
------------iteration 1900----------
total loss 22.19838918343252
main criterion 16.975400577933254
weighted_aux_loss 5.222988605499268
loss_r_bn_feature 522.2988891601562
=> using pytorch pre-trained model 'resnet18'
process data from ../recover/syn_data/imagenette/242
Traceback (most recent call last):
  File "generate_soft_label.py", line 256, in <module>
    main()
  File "generate_soft_label.py", line 129, in main
    main_worker(args.gpu, ngpus_per_node, args)
  File "generate_soft_label.py", line 201, in main_worker
    bary_weights_map = load_weights_file(args.data, os.path.join(args.data, 'barycenter_weights.txt'))
  File "/media/techt/One Touch/DD/SRe2L/relabel/utils_fkd.py", line 314, in load_weights_file
    with open(weight_path, 'r') as f:
FileNotFoundError: [Errno 2] No such file or directory: '../recover/syn_data/imagenette/242/barycenter_weights.txt'
W&B disabled.
W&B online. Running your script from this directory will now sync to the cloud.
wandb: Currently logged in as: hl57. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.12 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.10
wandb: Run data is saved locally in /media/techt/One Touch/DD/SRe2L/train/wandb/run-20231012_232015-c0fq8tgl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run legendary-butterfly-408
wandb: ‚≠êÔ∏è View project at https://wandb.ai/hl57/final_rn18_fkd
wandb: üöÄ View run at https://wandb.ai/hl57/final_rn18_fkd/runs/c0fq8tgl
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: üöÄ View run legendary-butterfly-408 at: https://wandb.ai/hl57/final_rn18_fkd/runs/c0fq8tgl
wandb: Ô∏è‚ö° View job at https://wandb.ai/hl57/final_rn18_fkd/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjk3MzkyNjU3/version_details/v37
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231012_232015-c0fq8tgl/logs
Traceback (most recent call last):
  File "train_FKD.py", line 398, in <module>
    main()
  File "train_FKD.py", line 110, in main
    bary_weights_map = load_weights_file(args.train_dir, os.path.join(args.train_dir, 'barycenter_weights.txt'))
  File "/media/techt/One Touch/DD/SRe2L/train/../relabel/utils_fkd.py", line 314, in load_weights_file
    with open(weight_path, 'r') as f:
FileNotFoundError: [Errno 2] No such file or directory: '../recover/syn_data/imagenette/242/barycenter_weights.txt'
