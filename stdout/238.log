/home/techt/anaconda3/envs/hl/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/techt/anaconda3/envs/hl/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
/home/techt/anaconda3/envs/hl/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
r_bn:  0.1
lr:  0.1
bc shape torch.Size([200, 10, 512])
get_images call
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 178.60516926104324
main criterion 140.52002903277176
weighted_aux_loss 38.085140228271484
loss_r_bn_feature 380.85137939453125
------------iteration 100----------
total loss 124.03725663611341
main criterion 94.96201745459486
weighted_aux_loss 29.075239181518555
loss_r_bn_feature 290.75238037109375
------------iteration 200----------
total loss 73.46491727270231
main criterion 48.70881375707731
weighted_aux_loss 24.756103515625
loss_r_bn_feature 247.56103515625
------------iteration 300----------
total loss 70.9539871344446
main criterion 45.53529549931277
weighted_aux_loss 25.418691635131836
loss_r_bn_feature 254.18690490722656
------------iteration 400----------
total loss 67.46381833974883
main criterion 42.101511744900186
weighted_aux_loss 25.362306594848633
loss_r_bn_feature 253.62306213378906
------------iteration 500----------
total loss 86.58337355521934
main criterion 63.2468180742623
weighted_aux_loss 23.33655548095703
loss_r_bn_feature 233.3655548095703
------------iteration 600----------
total loss 58.44767210275394
main criterion 36.466196272675816
weighted_aux_loss 21.981475830078125
loss_r_bn_feature 219.81475830078125
------------iteration 700----------
total loss 55.69046320877468
main criterion 34.338334226230735
weighted_aux_loss 21.352128982543945
loss_r_bn_feature 213.5212860107422
------------iteration 800----------
total loss 56.18441360499838
main criterion 34.980977707171235
weighted_aux_loss 21.20343589782715
loss_r_bn_feature 212.0343475341797
------------iteration 900----------
total loss 56.29340298627429
main criterion 36.33843358014636
weighted_aux_loss 19.95496940612793
loss_r_bn_feature 199.5496826171875
------------iteration 1000----------
total loss 51.47058220006533
main criterion 32.5322200784833
weighted_aux_loss 18.93836212158203
loss_r_bn_feature 189.3836212158203
------------iteration 1100----------
total loss 56.32832148982022
main criterion 38.38910487605069
weighted_aux_loss 17.93921661376953
loss_r_bn_feature 179.3921661376953
------------iteration 1200----------
total loss 45.925260705686654
main criterion 28.4635469147931
weighted_aux_loss 17.461713790893555
loss_r_bn_feature 174.6171417236328
------------iteration 1300----------
total loss 45.51592627823711
main criterion 28.217840111855274
weighted_aux_loss 17.298086166381836
loss_r_bn_feature 172.98086547851562
------------iteration 1400----------
total loss 43.06530331799931
main criterion 27.437504283880653
weighted_aux_loss 15.627799034118652
loss_r_bn_feature 156.27798461914062
------------iteration 1500----------
total loss 41.89190546039385
main criterion 26.24843278888506
weighted_aux_loss 15.643472671508789
loss_r_bn_feature 156.43472290039062
------------iteration 1600----------
total loss 39.56158092177371
main criterion 24.37467696822146
weighted_aux_loss 15.186903953552246
loss_r_bn_feature 151.86903381347656
------------iteration 1700----------
total loss 53.61711798067329
main criterion 38.75169763918159
weighted_aux_loss 14.8654203414917
loss_r_bn_feature 148.65420532226562
------------iteration 1800----------
total loss 39.9848951910396
main criterion 25.105123958434614
weighted_aux_loss 14.87977123260498
loss_r_bn_feature 148.79771423339844
------------iteration 1900----------
total loss 49.51191113463805
main criterion 35.20180485717223
weighted_aux_loss 14.31010627746582
loss_r_bn_feature 143.10105895996094
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 173.51583560654495
main criterion 130.47414096543167
weighted_aux_loss 43.04169464111328
loss_r_bn_feature 430.41693115234375
------------iteration 100----------
total loss 83.9475411512534
main criterion 54.730603132454576
weighted_aux_loss 29.216938018798828
loss_r_bn_feature 292.16937255859375
------------iteration 200----------
total loss 76.40279978138146
main criterion 48.69739549976525
weighted_aux_loss 27.70540428161621
loss_r_bn_feature 277.0540466308594
------------iteration 300----------
total loss 67.23393922687757
main criterion 42.00894456745373
weighted_aux_loss 25.224994659423828
loss_r_bn_feature 252.24993896484375
------------iteration 400----------
total loss 93.70165953389584
main criterion 71.19081816426693
weighted_aux_loss 22.510841369628906
loss_r_bn_feature 225.10841369628906
------------iteration 500----------
total loss 63.07866607916916
main criterion 37.883668291693574
weighted_aux_loss 25.194997787475586
loss_r_bn_feature 251.94996643066406
------------iteration 600----------
total loss 60.92326895973303
main criterion 38.41870467445471
weighted_aux_loss 22.50456428527832
loss_r_bn_feature 225.04563903808594
------------iteration 700----------
total loss 65.21706549224126
main criterion 42.15304342803227
weighted_aux_loss 23.064022064208984
loss_r_bn_feature 230.6402130126953
------------iteration 800----------
total loss 69.93573173030245
main criterion 49.44063933833467
weighted_aux_loss 20.495092391967773
loss_r_bn_feature 204.950927734375
------------iteration 900----------
total loss 57.163768000053246
main criterion 36.068410105155785
weighted_aux_loss 21.09535789489746
loss_r_bn_feature 210.95358276367188
------------iteration 1000----------
total loss 49.94051347686777
main criterion 30.659326419372654
weighted_aux_loss 19.281187057495117
loss_r_bn_feature 192.81187438964844
------------iteration 1100----------
total loss 57.46937631968395
main criterion 38.847632163800164
weighted_aux_loss 18.62174415588379
loss_r_bn_feature 186.21743774414062
------------iteration 1200----------
total loss 46.85688052089466
main criterion 28.997955655172007
weighted_aux_loss 17.858924865722656
loss_r_bn_feature 178.58924865722656
------------iteration 1300----------
total loss 55.134057682899666
main criterion 38.53292433634205
weighted_aux_loss 16.601133346557617
loss_r_bn_feature 166.01132202148438
------------iteration 1400----------
total loss 43.83357375463558
main criterion 27.731425698605307
weighted_aux_loss 16.102148056030273
loss_r_bn_feature 161.021484375
------------iteration 1500----------
total loss 48.381517949958244
main criterion 33.471602979560295
weighted_aux_loss 14.90991497039795
loss_r_bn_feature 149.09915161132812
------------iteration 1600----------
total loss 55.44994257771084
main criterion 39.004441250196194
weighted_aux_loss 16.44550132751465
loss_r_bn_feature 164.45501708984375
------------iteration 1700----------
total loss 83.62435844865246
main criterion 63.886571201948364
weighted_aux_loss 19.7377872467041
loss_r_bn_feature 197.37786865234375
------------iteration 1800----------
total loss 40.86191784032535
main criterion 26.387120591485015
weighted_aux_loss 14.474797248840332
loss_r_bn_feature 144.7479705810547
------------iteration 1900----------
total loss 72.55677797887502
main criterion 53.9866504916924
weighted_aux_loss 18.570127487182617
loss_r_bn_feature 185.70126342773438
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 174.06989379083188
main criterion 134.0176896587518
weighted_aux_loss 40.05220413208008
loss_r_bn_feature 400.52203369140625
------------iteration 100----------
total loss 98.13560170104854
main criterion 68.19051236083858
weighted_aux_loss 29.94508934020996
loss_r_bn_feature 299.4508972167969
------------iteration 200----------
total loss 130.04952651675723
main criterion 98.20173293765568
weighted_aux_loss 31.847793579101562
loss_r_bn_feature 318.4779357910156
------------iteration 300----------
total loss 66.83795808473279
main criterion 39.47966264405896
weighted_aux_loss 27.358295440673828
loss_r_bn_feature 273.58294677734375
------------iteration 400----------
total loss 76.55492612094635
main criterion 48.64534589023346
weighted_aux_loss 27.90958023071289
loss_r_bn_feature 279.0957946777344
------------iteration 500----------
total loss 67.06659507788092
main criterion 41.118791580566466
weighted_aux_loss 25.947803497314453
loss_r_bn_feature 259.47802734375
------------iteration 600----------
total loss 62.96957048306935
main criterion 37.9760898006963
weighted_aux_loss 24.993480682373047
loss_r_bn_feature 249.93479919433594
------------iteration 700----------
total loss 65.19422483190147
main criterion 41.192258355461036
weighted_aux_loss 24.00196647644043
loss_r_bn_feature 240.0196533203125
------------iteration 800----------
total loss 55.3596429129577
main criterion 32.261561324212586
weighted_aux_loss 23.098081588745117
loss_r_bn_feature 230.98080444335938
------------iteration 900----------
total loss 97.27864875514075
main criterion 74.82253493983313
weighted_aux_loss 22.456113815307617
loss_r_bn_feature 224.56112670898438
------------iteration 1000----------
total loss 87.2519641072974
main criterion 66.82730628564212
weighted_aux_loss 20.424657821655273
loss_r_bn_feature 204.24656677246094
------------iteration 1100----------
total loss 50.517354832813965
main criterion 29.889808522389163
weighted_aux_loss 20.627546310424805
loss_r_bn_feature 206.2754669189453
------------iteration 1200----------
total loss 67.63344998797555
main criterion 48.138056234923795
weighted_aux_loss 19.495393753051758
loss_r_bn_feature 194.9539337158203
------------iteration 1300----------
total loss 48.53452046103458
main criterion 29.308623625707433
weighted_aux_loss 19.22589683532715
loss_r_bn_feature 192.25897216796875
------------iteration 1400----------
total loss 48.31716276531356
main criterion 30.436103118707116
weighted_aux_loss 17.881059646606445
loss_r_bn_feature 178.8105926513672
------------iteration 1500----------
total loss 45.3645880519135
main criterion 27.97604778396916
weighted_aux_loss 17.388540267944336
loss_r_bn_feature 173.88540649414062
------------iteration 1600----------
total loss 44.13880674831554
main criterion 27.405032563134874
weighted_aux_loss 16.733774185180664
loss_r_bn_feature 167.33773803710938
------------iteration 1700----------
total loss 48.08947099292199
main criterion 30.182555328249133
weighted_aux_loss 17.90691566467285
loss_r_bn_feature 179.06915283203125
------------iteration 1800----------
total loss 47.74157304712845
main criterion 30.683419894662627
weighted_aux_loss 17.05815315246582
loss_r_bn_feature 170.58152770996094
------------iteration 1900----------
total loss 50.923391622183736
main criterion 34.465505879996236
weighted_aux_loss 16.4578857421875
loss_r_bn_feature 164.578857421875
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 178.49524678370395
main criterion 137.120807544202
weighted_aux_loss 41.37443923950195
loss_r_bn_feature 413.744384765625
------------iteration 100----------
total loss 83.29998442567972
main criterion 54.01315543092875
weighted_aux_loss 29.286828994750977
loss_r_bn_feature 292.8682861328125
------------iteration 200----------
total loss 71.77683142879219
main criterion 44.43579177119942
weighted_aux_loss 27.341039657592773
loss_r_bn_feature 273.410400390625
------------iteration 300----------
total loss 69.41085198097821
main criterion 43.6253237979948
weighted_aux_loss 25.7855281829834
loss_r_bn_feature 257.85528564453125
------------iteration 400----------
total loss 83.03167065766323
main criterion 57.38258847382533
weighted_aux_loss 25.64908218383789
loss_r_bn_feature 256.4908142089844
------------iteration 500----------
total loss 62.3851233658223
main criterion 38.0567656692891
weighted_aux_loss 24.328357696533203
loss_r_bn_feature 243.2835693359375
------------iteration 600----------
total loss 70.47644589327483
main criterion 46.898109177576586
weighted_aux_loss 23.578336715698242
loss_r_bn_feature 235.7833709716797
------------iteration 700----------
total loss 106.366155999214
main criterion 82.99058951667006
weighted_aux_loss 23.375566482543945
loss_r_bn_feature 233.7556610107422
------------iteration 800----------
total loss 64.73940404072246
main criterion 41.71015103474101
weighted_aux_loss 23.029253005981445
loss_r_bn_feature 230.2925262451172
------------iteration 900----------
total loss 57.883703429230565
main criterion 36.36267681790244
weighted_aux_loss 21.521026611328125
loss_r_bn_feature 215.21026611328125
------------iteration 1000----------
total loss 59.2941186569923
main criterion 38.3660352371925
weighted_aux_loss 20.928083419799805
loss_r_bn_feature 209.2808380126953
------------iteration 1100----------
total loss 53.06658002463701
main criterion 33.42018338767412
weighted_aux_loss 19.64639663696289
loss_r_bn_feature 196.46395874023438
------------iteration 1200----------
total loss 97.81789419530591
main criterion 78.8451559293147
weighted_aux_loss 18.97273826599121
loss_r_bn_feature 189.7273712158203
------------iteration 1300----------
total loss 61.956487240813146
main criterion 43.237878384611975
weighted_aux_loss 18.718608856201172
loss_r_bn_feature 187.1860809326172
------------iteration 1400----------
total loss 59.305246408008244
main criterion 43.40644841911664
weighted_aux_loss 15.898797988891602
loss_r_bn_feature 158.98797607421875
------------iteration 1500----------
total loss 46.81302650980133
main criterion 30.40282639078278
weighted_aux_loss 16.410200119018555
loss_r_bn_feature 164.1020050048828
------------iteration 1600----------
total loss 48.90172614145872
main criterion 32.926390066629615
weighted_aux_loss 15.975336074829102
loss_r_bn_feature 159.75335693359375
------------iteration 1700----------
total loss 66.58488991526951
main criterion 51.4727177884995
weighted_aux_loss 15.11217212677002
loss_r_bn_feature 151.12171936035156
------------iteration 1800----------
total loss 54.1871595643456
main criterion 37.51644613966298
weighted_aux_loss 16.670713424682617
loss_r_bn_feature 166.70712280273438
------------iteration 1900----------
total loss 51.985871440768264
main criterion 37.54211819827559
weighted_aux_loss 14.443753242492676
loss_r_bn_feature 144.43753051757812
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 172.76288153917554
main criterion 131.26995017320874
weighted_aux_loss 41.4929313659668
loss_r_bn_feature 414.9293212890625
------------iteration 100----------
total loss 70.86854734290006
main criterion 42.65898313391569
weighted_aux_loss 28.209564208984375
loss_r_bn_feature 282.09564208984375
------------iteration 200----------
total loss 99.92707799053943
main criterion 70.38892720318592
weighted_aux_loss 29.538150787353516
loss_r_bn_feature 295.3815002441406
------------iteration 300----------
total loss 73.79681274028871
main criterion 47.35295936199282
weighted_aux_loss 26.4438533782959
loss_r_bn_feature 264.43853759765625
------------iteration 400----------
total loss 72.75312926936186
main criterion 46.61051490473785
weighted_aux_loss 26.142614364624023
loss_r_bn_feature 261.4261474609375
------------iteration 500----------
total loss 58.043192389562414
main criterion 34.13704347903995
weighted_aux_loss 23.90614891052246
loss_r_bn_feature 239.06149291992188
------------iteration 600----------
total loss 70.51964014927351
main criterion 47.20165652195418
weighted_aux_loss 23.317983627319336
loss_r_bn_feature 233.17982482910156
------------iteration 700----------
total loss 55.83485785576814
main criterion 33.29163542839998
weighted_aux_loss 22.543222427368164
loss_r_bn_feature 225.43222045898438
------------iteration 800----------
total loss 55.26072982349635
main criterion 33.28970054188014
weighted_aux_loss 21.97102928161621
loss_r_bn_feature 219.7102813720703
------------iteration 900----------
total loss 55.466730532369915
main criterion 34.37325328322441
weighted_aux_loss 21.093477249145508
loss_r_bn_feature 210.9347686767578
------------iteration 1000----------
total loss 110.80796116010762
main criterion 82.00260799543477
weighted_aux_loss 28.80535316467285
loss_r_bn_feature 288.05352783203125
------------iteration 1100----------
total loss 53.562666323227184
main criterion 33.51453438047816
weighted_aux_loss 20.048131942749023
loss_r_bn_feature 200.4813232421875
------------iteration 1200----------
total loss 54.90895575488954
main criterion 36.63911932910829
weighted_aux_loss 18.26983642578125
loss_r_bn_feature 182.6983642578125
------------iteration 1300----------
total loss 57.63006184576586
main criterion 41.39669011114672
weighted_aux_loss 16.23337173461914
loss_r_bn_feature 162.33370971679688
------------iteration 1400----------
total loss 42.69714318417235
main criterion 25.67660485409422
weighted_aux_loss 17.020538330078125
loss_r_bn_feature 170.20538330078125
------------iteration 1500----------
total loss 43.31660310584243
main criterion 27.00281373816665
weighted_aux_loss 16.31378936767578
loss_r_bn_feature 163.1378936767578
------------iteration 1600----------
total loss 59.863118292441754
main criterion 43.9427977815775
weighted_aux_loss 15.920320510864258
loss_r_bn_feature 159.2032012939453
------------iteration 1700----------
total loss 80.57226138691401
main criterion 63.013265186352484
weighted_aux_loss 17.558996200561523
loss_r_bn_feature 175.5899658203125
------------iteration 1800----------
total loss 40.69400697150576
main criterion 24.736327222970598
weighted_aux_loss 15.957679748535156
loss_r_bn_feature 159.57679748535156
------------iteration 1900----------
total loss 51.37440413683297
main criterion 35.57592410295846
weighted_aux_loss 15.798480033874512
loss_r_bn_feature 157.98480224609375
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 172.01722551135956
main criterion 136.199949510383
weighted_aux_loss 35.81727600097656
loss_r_bn_feature 358.1727600097656
------------iteration 100----------
total loss 78.23537061912825
main criterion 51.35868261558821
weighted_aux_loss 26.87668800354004
loss_r_bn_feature 268.7668762207031
------------iteration 200----------
total loss 69.64828292858425
main criterion 43.66449348461453
weighted_aux_loss 25.983789443969727
loss_r_bn_feature 259.837890625
------------iteration 300----------
total loss 70.945300223998
main criterion 45.01341736572164
weighted_aux_loss 25.931882858276367
loss_r_bn_feature 259.3188171386719
------------iteration 400----------
total loss 66.2949891438175
main criterion 41.089296852557744
weighted_aux_loss 25.205692291259766
loss_r_bn_feature 252.05691528320312
------------iteration 500----------
total loss 72.76287819180011
main criterion 49.4058683041048
weighted_aux_loss 23.357009887695312
loss_r_bn_feature 233.57009887695312
------------iteration 600----------
total loss 61.058320668416336
main criterion 38.44323697395833
weighted_aux_loss 22.615083694458008
loss_r_bn_feature 226.1508331298828
------------iteration 700----------
total loss 56.78186194673983
main criterion 35.00164763704745
weighted_aux_loss 21.780214309692383
loss_r_bn_feature 217.80213928222656
------------iteration 800----------
total loss 72.1693524099484
main criterion 50.44787490140349
weighted_aux_loss 21.721477508544922
loss_r_bn_feature 217.2147674560547
------------iteration 900----------
total loss 57.78269666636844
main criterion 36.876166286119414
weighted_aux_loss 20.906530380249023
loss_r_bn_feature 209.06529235839844
------------iteration 1000----------
total loss 70.68154987882691
main criterion 51.30311092924195
weighted_aux_loss 19.37843894958496
loss_r_bn_feature 193.78439331054688
------------iteration 1100----------
total loss 53.746075842548834
main criterion 34.639312002827154
weighted_aux_loss 19.10676383972168
loss_r_bn_feature 191.067626953125
------------iteration 1200----------
total loss 56.61482152375347
main criterion 39.20048627289898
weighted_aux_loss 17.414335250854492
loss_r_bn_feature 174.1433563232422
------------iteration 1300----------
total loss 48.92265132931131
main criterion 31.778289833339624
weighted_aux_loss 17.14436149597168
loss_r_bn_feature 171.443603515625
------------iteration 1400----------
total loss 67.95795454331795
main criterion 50.7726956779615
weighted_aux_loss 17.185258865356445
loss_r_bn_feature 171.8525848388672
------------iteration 1500----------
total loss 59.24619368616495
main criterion 44.83781031671915
weighted_aux_loss 14.4083833694458
loss_r_bn_feature 144.08383178710938
------------iteration 1600----------
total loss 40.7628361846311
main criterion 25.618274417052973
weighted_aux_loss 15.144561767578125
loss_r_bn_feature 151.44561767578125
------------iteration 1700----------
total loss 39.86748069648998
main criterion 24.42789787178295
weighted_aux_loss 15.439582824707031
loss_r_bn_feature 154.3958282470703
------------iteration 1800----------
total loss 44.2429184014442
main criterion 29.485366254532583
weighted_aux_loss 14.757552146911621
loss_r_bn_feature 147.5755157470703
------------iteration 1900----------
total loss 41.45415168960506
main criterion 25.854226648406332
weighted_aux_loss 15.59992504119873
loss_r_bn_feature 155.99925231933594
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 176.2716807599983
main criterion 135.5943469282112
weighted_aux_loss 40.67733383178711
loss_r_bn_feature 406.7733154296875
------------iteration 100----------
total loss 80.01436503384745
main criterion 48.50981982205546
weighted_aux_loss 31.504545211791992
loss_r_bn_feature 315.0454406738281
------------iteration 200----------
total loss 85.95858708525527
main criterion 56.50162640715469
weighted_aux_loss 29.456960678100586
loss_r_bn_feature 294.5696105957031
------------iteration 300----------
total loss 81.51872032387539
main criterion 53.66105049355313
weighted_aux_loss 27.857669830322266
loss_r_bn_feature 278.5766906738281
------------iteration 400----------
total loss 64.62646810763815
main criterion 36.73652403110005
weighted_aux_loss 27.889944076538086
loss_r_bn_feature 278.8994445800781
------------iteration 500----------
total loss 77.32809158362733
main criterion 51.91302772559509
weighted_aux_loss 25.415063858032227
loss_r_bn_feature 254.150634765625
------------iteration 600----------
total loss 66.19967326380043
main criterion 41.8522363588932
weighted_aux_loss 24.347436904907227
loss_r_bn_feature 243.474365234375
------------iteration 700----------
total loss 62.66043109554637
main criterion 39.06128139156688
weighted_aux_loss 23.599149703979492
loss_r_bn_feature 235.99148559570312
------------iteration 800----------
total loss 59.41430837737455
main criterion 35.488345929254436
weighted_aux_loss 23.925962448120117
loss_r_bn_feature 239.25962829589844
------------iteration 900----------
total loss 70.11345463984199
main criterion 48.52291852228827
weighted_aux_loss 21.59053611755371
loss_r_bn_feature 215.9053497314453
------------iteration 1000----------
total loss 53.35303045474573
main criterion 31.985806715121704
weighted_aux_loss 21.367223739624023
loss_r_bn_feature 213.67222595214844
------------iteration 1100----------
total loss 52.158120415465774
main criterion 31.84770514752144
weighted_aux_loss 20.310415267944336
loss_r_bn_feature 203.10415649414062
------------iteration 1200----------
total loss 50.56808305675088
main criterion 31.171373614612204
weighted_aux_loss 19.396709442138672
loss_r_bn_feature 193.9670867919922
------------iteration 1300----------
total loss 47.67019892494449
main criterion 27.830227382586095
weighted_aux_loss 19.8399715423584
loss_r_bn_feature 198.3997039794922
------------iteration 1400----------
total loss 46.11692004596209
main criterion 28.196992448916195
weighted_aux_loss 17.9199275970459
loss_r_bn_feature 179.1992645263672
------------iteration 1500----------
total loss 61.51247339231778
main criterion 45.52273778898526
weighted_aux_loss 15.98973560333252
loss_r_bn_feature 159.89735412597656
------------iteration 1600----------
total loss 45.76089389096042
main criterion 29.793631622894008
weighted_aux_loss 15.967262268066406
loss_r_bn_feature 159.67262268066406
------------iteration 1700----------
total loss 65.66716810429193
main criterion 47.306040250898384
weighted_aux_loss 18.361127853393555
loss_r_bn_feature 183.61126708984375
------------iteration 1800----------
total loss 54.60314807298132
main criterion 39.3943544328446
weighted_aux_loss 15.208793640136719
loss_r_bn_feature 152.0879364013672
------------iteration 1900----------
total loss 44.17501807659275
main criterion 28.558753733331027
weighted_aux_loss 15.616264343261719
loss_r_bn_feature 156.1626434326172
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 176.31012303881883
main criterion 131.3485675578618
weighted_aux_loss 44.96155548095703
loss_r_bn_feature 449.61553955078125
------------iteration 100----------
total loss 126.9403845678546
main criterion 90.72301167356748
weighted_aux_loss 36.21737289428711
loss_r_bn_feature 362.1737060546875
------------iteration 200----------
total loss 75.09826470195404
main criterion 43.88789368449798
weighted_aux_loss 31.210371017456055
loss_r_bn_feature 312.10369873046875
------------iteration 300----------
total loss 75.0815353624358
main criterion 45.60116960926197
weighted_aux_loss 29.480365753173828
loss_r_bn_feature 294.80364990234375
------------iteration 400----------
total loss 102.58299740432766
main criterion 71.4705153334048
weighted_aux_loss 31.11248207092285
loss_r_bn_feature 311.12481689453125
------------iteration 500----------
total loss 65.47809190428231
main criterion 37.17439050352548
weighted_aux_loss 28.303701400756836
loss_r_bn_feature 283.0370178222656
------------iteration 600----------
total loss 62.259671845693276
main criterion 35.30566183592765
weighted_aux_loss 26.954010009765625
loss_r_bn_feature 269.54010009765625
------------iteration 700----------
total loss 66.58421532681785
main criterion 40.56870667508445
weighted_aux_loss 26.0155086517334
loss_r_bn_feature 260.15509033203125
------------iteration 800----------
total loss 120.0908906081557
main criterion 90.6429097274184
weighted_aux_loss 29.447980880737305
loss_r_bn_feature 294.47979736328125
------------iteration 900----------
total loss 125.71496847741032
main criterion 92.23788336388493
weighted_aux_loss 33.47708511352539
loss_r_bn_feature 334.7708435058594
------------iteration 1000----------
total loss 77.66219112288502
main criterion 52.91133281600026
weighted_aux_loss 24.750858306884766
loss_r_bn_feature 247.50857543945312
------------iteration 1100----------
total loss 56.56708295985291
main criterion 35.62216146632264
weighted_aux_loss 20.944921493530273
loss_r_bn_feature 209.44920349121094
------------iteration 1200----------
total loss 73.24085412671963
main criterion 50.77444825818936
weighted_aux_loss 22.466405868530273
loss_r_bn_feature 224.66404724121094
------------iteration 1300----------
total loss 55.45902175087961
main criterion 35.258195105982146
weighted_aux_loss 20.20082664489746
loss_r_bn_feature 202.00827026367188
------------iteration 1400----------
total loss 59.44442133133836
main criterion 40.46609453385301
weighted_aux_loss 18.97832679748535
loss_r_bn_feature 189.78326416015625
------------iteration 1500----------
total loss 49.76354656176978
main criterion 31.913118934206302
weighted_aux_loss 17.850427627563477
loss_r_bn_feature 178.5042724609375
------------iteration 1600----------
total loss 64.68696213531044
main criterion 44.23371697234656
weighted_aux_loss 20.453245162963867
loss_r_bn_feature 204.53245544433594
------------iteration 1700----------
total loss 49.678335047997344
main criterion 31.29874215248953
weighted_aux_loss 18.379592895507812
loss_r_bn_feature 183.79592895507812
------------iteration 1800----------
total loss 63.61771400926696
main criterion 44.66299637315856
weighted_aux_loss 18.9547176361084
loss_r_bn_feature 189.54718017578125
------------iteration 1900----------
total loss 51.36488122946415
main criterion 33.325717639986614
weighted_aux_loss 18.03916358947754
loss_r_bn_feature 180.39163208007812
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 179.90639332239775
main criterion 133.69375828211454
weighted_aux_loss 46.2126350402832
loss_r_bn_feature 462.1263427734375
------------iteration 100----------
total loss 86.39189411417897
main criterion 54.25467563883716
weighted_aux_loss 32.1372184753418
loss_r_bn_feature 321.3721923828125
------------iteration 200----------
total loss 69.7060960381627
main criterion 39.46127830439805
weighted_aux_loss 30.24481773376465
loss_r_bn_feature 302.44818115234375
------------iteration 300----------
total loss 126.6139283352196
main criterion 94.13524104884264
weighted_aux_loss 32.47868728637695
loss_r_bn_feature 324.786865234375
------------iteration 400----------
total loss 89.38377626501259
main criterion 60.378796177732305
weighted_aux_loss 29.004980087280273
loss_r_bn_feature 290.0498046875
------------iteration 500----------
total loss 67.05827120769017
main criterion 38.533706464648176
weighted_aux_loss 28.524564743041992
loss_r_bn_feature 285.2456359863281
------------iteration 600----------
total loss 80.54835245664864
main criterion 54.17581865843086
weighted_aux_loss 26.372533798217773
loss_r_bn_feature 263.725341796875
------------iteration 700----------
total loss 61.552312033541774
main criterion 36.07991709030447
weighted_aux_loss 25.472394943237305
loss_r_bn_feature 254.72393798828125
------------iteration 800----------
total loss 57.539806560873316
main criterion 33.05126400411062
weighted_aux_loss 24.488542556762695
loss_r_bn_feature 244.8854217529297
------------iteration 900----------
total loss 64.18170997137565
main criterion 41.05294295782585
weighted_aux_loss 23.128767013549805
loss_r_bn_feature 231.28765869140625
------------iteration 1000----------
total loss 75.6223792958985
main criterion 55.39896964318853
weighted_aux_loss 20.22340965270996
loss_r_bn_feature 202.23410034179688
------------iteration 1100----------
total loss 59.4361868341283
main criterion 38.10044769838611
weighted_aux_loss 21.335739135742188
loss_r_bn_feature 213.35739135742188
------------iteration 1200----------
total loss 53.6632856726064
main criterion 32.95591491454976
weighted_aux_loss 20.70737075805664
loss_r_bn_feature 207.07369995117188
------------iteration 1300----------
total loss 76.06663090207297
main criterion 54.9465613982888
weighted_aux_loss 21.12006950378418
loss_r_bn_feature 211.20069885253906
------------iteration 1400----------
total loss 53.056025685797444
main criterion 34.87269514990877
weighted_aux_loss 18.183330535888672
loss_r_bn_feature 181.8332977294922
------------iteration 1500----------
total loss 44.888204584772176
main criterion 26.169672022516316
weighted_aux_loss 18.71853256225586
loss_r_bn_feature 187.18531799316406
------------iteration 1600----------
total loss 61.01113491728662
main criterion 43.062726790455564
weighted_aux_loss 17.948408126831055
loss_r_bn_feature 179.48406982421875
------------iteration 1700----------
total loss 64.75159430800434
main criterion 48.451135399923295
weighted_aux_loss 16.300458908081055
loss_r_bn_feature 163.0045928955078
------------iteration 1800----------
total loss 56.73002743584181
main criterion 39.967652557912125
weighted_aux_loss 16.762374877929688
loss_r_bn_feature 167.62374877929688
------------iteration 1900----------
total loss 45.27060077104224
main criterion 27.759433244797126
weighted_aux_loss 17.511167526245117
loss_r_bn_feature 175.11166381835938
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 176.02113394993083
main criterion 130.67963080662028
weighted_aux_loss 45.34150314331055
loss_r_bn_feature 453.4150390625
------------iteration 100----------
total loss 78.41484251099953
main criterion 46.77283086854347
weighted_aux_loss 31.642011642456055
loss_r_bn_feature 316.42010498046875
------------iteration 200----------
total loss 93.1337591975043
main criterion 63.13737553051212
weighted_aux_loss 29.996383666992188
loss_r_bn_feature 299.9638366699219
------------iteration 300----------
total loss 67.53140729348512
main criterion 38.88883870522828
weighted_aux_loss 28.642568588256836
loss_r_bn_feature 286.4256896972656
------------iteration 400----------
total loss 83.36275527777377
main criterion 54.85031364264194
weighted_aux_loss 28.512441635131836
loss_r_bn_feature 285.1244201660156
------------iteration 500----------
total loss 65.30542578810883
main criterion 38.63106360549164
weighted_aux_loss 26.674362182617188
loss_r_bn_feature 266.7436218261719
------------iteration 600----------
total loss 79.4630544173411
main criterion 53.85857939597879
weighted_aux_loss 25.604475021362305
loss_r_bn_feature 256.04473876953125
------------iteration 700----------
total loss 64.30883293731358
main criterion 39.60110931975988
weighted_aux_loss 24.70772361755371
loss_r_bn_feature 247.0772247314453
------------iteration 800----------
total loss 60.66097159652586
main criterion 36.33903212814207
weighted_aux_loss 24.32193946838379
loss_r_bn_feature 243.21939086914062
------------iteration 900----------
total loss 61.25606773544979
main criterion 37.642620546118735
weighted_aux_loss 23.613447189331055
loss_r_bn_feature 236.13446044921875
------------iteration 1000----------
total loss 55.63463270289669
main criterion 33.464552517960165
weighted_aux_loss 22.170080184936523
loss_r_bn_feature 221.7008056640625
------------iteration 1100----------
total loss 59.81561566608916
main criterion 40.113200199780565
weighted_aux_loss 19.702415466308594
loss_r_bn_feature 197.02415466308594
------------iteration 1200----------
total loss 48.76652058510467
main criterion 28.980284775778504
weighted_aux_loss 19.786235809326172
loss_r_bn_feature 197.8623504638672
------------iteration 1300----------
total loss 63.24505418698745
main criterion 46.30683130185562
weighted_aux_loss 16.938222885131836
loss_r_bn_feature 169.38223266601562
------------iteration 1400----------
total loss 48.459404500655495
main criterion 30.03499081962522
weighted_aux_loss 18.424413681030273
loss_r_bn_feature 184.24412536621094
------------iteration 1500----------
total loss 43.845599526185
main criterion 25.604405754822693
weighted_aux_loss 18.241193771362305
loss_r_bn_feature 182.41192626953125
------------iteration 1600----------
total loss 41.45591833265996
main criterion 24.140711804949998
weighted_aux_loss 17.31520652770996
loss_r_bn_feature 173.15206909179688
------------iteration 1700----------
total loss 44.29060872177905
main criterion 27.39419491867847
weighted_aux_loss 16.896413803100586
loss_r_bn_feature 168.96414184570312
------------iteration 1800----------
total loss 45.26574431729566
main criterion 29.442499268772714
weighted_aux_loss 15.82324504852295
loss_r_bn_feature 158.23245239257812
------------iteration 1900----------
total loss 42.45777419815595
main criterion 25.242531812291695
weighted_aux_loss 17.215242385864258
loss_r_bn_feature 172.1524200439453
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 176.5003901674555
main criterion 130.67467987082463
weighted_aux_loss 45.82571029663086
loss_r_bn_feature 458.257080078125
------------iteration 100----------
total loss 79.20658855598288
main criterion 46.46871929328757
weighted_aux_loss 32.73786926269531
loss_r_bn_feature 327.3786926269531
------------iteration 200----------
total loss 73.8255905751511
main criterion 42.047708952836636
weighted_aux_loss 31.777881622314453
loss_r_bn_feature 317.77880859375
------------iteration 300----------
total loss 66.72247083280622
main criterion 36.531478477459544
weighted_aux_loss 30.19099235534668
loss_r_bn_feature 301.909912109375
------------iteration 400----------
total loss 87.3666424934676
main criterion 57.478601950865055
weighted_aux_loss 29.88804054260254
loss_r_bn_feature 298.8804016113281
------------iteration 500----------
total loss 66.19045284189201
main criterion 37.63647869028069
weighted_aux_loss 28.553974151611328
loss_r_bn_feature 285.53973388671875
------------iteration 600----------
total loss 66.17559901553201
main criterion 37.83828250247049
weighted_aux_loss 28.337316513061523
loss_r_bn_feature 283.3731689453125
------------iteration 700----------
total loss 61.299240467811615
main criterion 34.77480160977939
weighted_aux_loss 26.524438858032227
loss_r_bn_feature 265.244384765625
------------iteration 800----------
total loss 65.63253110296361
main criterion 40.43522160894505
weighted_aux_loss 25.197309494018555
loss_r_bn_feature 251.9730987548828
------------iteration 900----------
total loss 80.29651999489201
main criterion 55.767223119892
weighted_aux_loss 24.529296875
loss_r_bn_feature 245.29296875
------------iteration 1000----------
total loss 53.84529556534925
main criterion 30.423456804973267
weighted_aux_loss 23.421838760375977
loss_r_bn_feature 234.2183837890625
------------iteration 1100----------
total loss 55.78230751657425
main criterion 33.21096695566116
weighted_aux_loss 22.571340560913086
loss_r_bn_feature 225.71340942382812
------------iteration 1200----------
total loss 73.04080309624146
main criterion 50.12001337761353
weighted_aux_loss 22.92078971862793
loss_r_bn_feature 229.2078857421875
------------iteration 1300----------
total loss 48.619062776446455
main criterion 27.571857805132975
weighted_aux_loss 21.047204971313477
loss_r_bn_feature 210.4720458984375
------------iteration 1400----------
total loss 45.24318707470417
main criterion 25.42987072948932
weighted_aux_loss 19.813316345214844
loss_r_bn_feature 198.13316345214844
------------iteration 1500----------
total loss 64.5056609315855
main criterion 44.6459139986021
weighted_aux_loss 19.8597469329834
loss_r_bn_feature 198.5974578857422
------------iteration 1600----------
total loss 44.13287232169135
main criterion 24.686762549962832
weighted_aux_loss 19.446109771728516
loss_r_bn_feature 194.46109008789062
------------iteration 1700----------
total loss 48.06941968424002
main criterion 28.750795187780053
weighted_aux_loss 19.31862449645996
loss_r_bn_feature 193.18624877929688
------------iteration 1800----------
total loss 43.3292262729729
main criterion 24.370928543480716
weighted_aux_loss 18.958297729492188
loss_r_bn_feature 189.58297729492188
------------iteration 1900----------
total loss 59.93639510790088
main criterion 41.325444637930175
weighted_aux_loss 18.610950469970703
loss_r_bn_feature 186.1094970703125
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 179.2943803643095
main criterion 135.01314562309855
weighted_aux_loss 44.28123474121094
loss_r_bn_feature 442.8123474121094
------------iteration 100----------
total loss 78.59340901622559
main criterion 46.31327672252443
weighted_aux_loss 32.28013229370117
loss_r_bn_feature 322.80133056640625
------------iteration 200----------
total loss 72.79916400147798
main criterion 42.281917755140086
weighted_aux_loss 30.51724624633789
loss_r_bn_feature 305.1724548339844
------------iteration 300----------
total loss 70.18269906264757
main criterion 41.070995189966915
weighted_aux_loss 29.111703872680664
loss_r_bn_feature 291.1170349121094
------------iteration 400----------
total loss 69.58729708427842
main criterion 42.344539287037215
weighted_aux_loss 27.24275779724121
loss_r_bn_feature 272.4275817871094
------------iteration 500----------
total loss 99.58115323886963
main criterion 72.84140905246826
weighted_aux_loss 26.739744186401367
loss_r_bn_feature 267.3974304199219
------------iteration 600----------
total loss 70.53674318469926
main criterion 44.21132471240922
weighted_aux_loss 26.32541847229004
loss_r_bn_feature 263.2541809082031
------------iteration 700----------
total loss 59.93262310080716
main criterion 34.700678062965366
weighted_aux_loss 25.231945037841797
loss_r_bn_feature 252.31944274902344
------------iteration 800----------
total loss 56.113004353488805
main criterion 32.23554578110111
weighted_aux_loss 23.877458572387695
loss_r_bn_feature 238.7745819091797
------------iteration 900----------
total loss 67.87207422707044
main criterion 44.13869295570813
weighted_aux_loss 23.733381271362305
loss_r_bn_feature 237.3338165283203
------------iteration 1000----------
total loss 56.91280406463554
main criterion 35.403593478087686
weighted_aux_loss 21.50921058654785
loss_r_bn_feature 215.09210205078125
------------iteration 1100----------
total loss 51.729524648828416
main criterion 29.8901214963626
weighted_aux_loss 21.83940315246582
loss_r_bn_feature 218.39402770996094
------------iteration 1200----------
total loss 87.39348274353438
main criterion 63.735655366215035
weighted_aux_loss 23.657827377319336
loss_r_bn_feature 236.57826232910156
------------iteration 1300----------
total loss 74.32235799451779
main criterion 55.008736473521694
weighted_aux_loss 19.313621520996094
loss_r_bn_feature 193.13621520996094
------------iteration 1400----------
total loss 47.09711607948702
main criterion 27.916452016987026
weighted_aux_loss 19.1806640625
loss_r_bn_feature 191.806640625
------------iteration 1500----------
total loss 82.6709961938351
main criterion 61.24915743345912
weighted_aux_loss 21.421838760375977
loss_r_bn_feature 214.2183837890625
------------iteration 1600----------
total loss 51.56575393059357
main criterion 33.79100608208283
weighted_aux_loss 17.774747848510742
loss_r_bn_feature 177.7474822998047
------------iteration 1700----------
total loss 41.7403324873043
main criterion 24.453873518188093
weighted_aux_loss 17.28645896911621
loss_r_bn_feature 172.8645782470703
------------iteration 1800----------
total loss 56.07081161995524
main criterion 39.128484120565595
weighted_aux_loss 16.94232749938965
loss_r_bn_feature 169.42327880859375
------------iteration 1900----------
total loss 55.24162370716187
main criterion 38.05289537464234
weighted_aux_loss 17.18872833251953
loss_r_bn_feature 171.8872833251953
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 172.768326574998
main criterion 127.76349335356248
weighted_aux_loss 45.00483322143555
loss_r_bn_feature 450.0483093261719
------------iteration 100----------
total loss 111.46581567617035
main criterion 77.00149835439301
weighted_aux_loss 34.464317321777344
loss_r_bn_feature 344.6431579589844
------------iteration 200----------
total loss 72.01192223923528
main criterion 41.49521577256048
weighted_aux_loss 30.516706466674805
loss_r_bn_feature 305.16705322265625
------------iteration 300----------
total loss 155.08661726899015
main criterion 115.81811980194912
weighted_aux_loss 39.268497467041016
loss_r_bn_feature 392.6849670410156
------------iteration 400----------
total loss 63.56704969086757
main criterion 35.68178243317714
weighted_aux_loss 27.88526725769043
loss_r_bn_feature 278.8526611328125
------------iteration 500----------
total loss 62.5954844808433
main criterion 35.427033171639195
weighted_aux_loss 27.1684513092041
loss_r_bn_feature 271.68450927734375
------------iteration 600----------
total loss 103.77739686453324
main criterion 75.24699754202348
weighted_aux_loss 28.530399322509766
loss_r_bn_feature 285.3039855957031
------------iteration 700----------
total loss 57.04117572968501
main criterion 31.666704065256297
weighted_aux_loss 25.37447166442871
loss_r_bn_feature 253.74472045898438
------------iteration 800----------
total loss 118.65103419868049
main criterion 90.38300784675178
weighted_aux_loss 28.26802635192871
loss_r_bn_feature 282.6802673339844
------------iteration 900----------
total loss 53.37412544923875
main criterion 30.115040747700657
weighted_aux_loss 23.259084701538086
loss_r_bn_feature 232.59085083007812
------------iteration 1000----------
total loss 58.035477603416716
main criterion 35.4471444735339
weighted_aux_loss 22.588333129882812
loss_r_bn_feature 225.88333129882812
------------iteration 1100----------
total loss 76.79560723534794
main criterion 54.08866181603642
weighted_aux_loss 22.706945419311523
loss_r_bn_feature 227.06944274902344
------------iteration 1200----------
total loss 65.66791433015635
main criterion 45.967030464799905
weighted_aux_loss 19.700883865356445
loss_r_bn_feature 197.0088348388672
------------iteration 1300----------
total loss 46.96339664737689
main criterion 27.587378123206967
weighted_aux_loss 19.376018524169922
loss_r_bn_feature 193.7601776123047
------------iteration 1400----------
total loss 45.86270502329579
main criterion 26.64601305247059
weighted_aux_loss 19.216691970825195
loss_r_bn_feature 192.1669158935547
------------iteration 1500----------
total loss 62.980896284299874
main criterion 44.84671049328425
weighted_aux_loss 18.134185791015625
loss_r_bn_feature 181.34185791015625
------------iteration 1600----------
total loss 45.54710529650325
main criterion 28.69085262621516
weighted_aux_loss 16.856252670288086
loss_r_bn_feature 168.56253051757812
------------iteration 1700----------
total loss 41.10220676355517
main criterion 23.88086658411181
weighted_aux_loss 17.22134017944336
loss_r_bn_feature 172.21339416503906
------------iteration 1800----------
total loss 50.615166353326856
main criterion 34.28143374834639
weighted_aux_loss 16.33373260498047
loss_r_bn_feature 163.3373260498047
------------iteration 1900----------
total loss 54.20010012312527
main criterion 38.72054403944607
weighted_aux_loss 15.4795560836792
loss_r_bn_feature 154.79556274414062
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 178.48130459379234
main criterion 130.65210003446617
weighted_aux_loss 47.82920455932617
loss_r_bn_feature 478.29205322265625
------------iteration 100----------
total loss 80.56877197980006
main criterion 46.97856201886257
weighted_aux_loss 33.5902099609375
loss_r_bn_feature 335.902099609375
------------iteration 200----------
total loss 126.85556231034994
main criterion 91.15004549516439
weighted_aux_loss 35.70551681518555
loss_r_bn_feature 357.0551452636719
------------iteration 300----------
total loss 79.37767868937269
main criterion 48.58725434244885
weighted_aux_loss 30.790424346923828
loss_r_bn_feature 307.90423583984375
------------iteration 400----------
total loss 68.92172186388623
main criterion 39.87803785814893
weighted_aux_loss 29.043684005737305
loss_r_bn_feature 290.43682861328125
------------iteration 500----------
total loss 65.4210734439763
main criterion 37.569520480719454
weighted_aux_loss 27.851552963256836
loss_r_bn_feature 278.5155334472656
------------iteration 600----------
total loss 64.37857545762108
main criterion 37.29402078537987
weighted_aux_loss 27.08455467224121
loss_r_bn_feature 270.8455505371094
------------iteration 700----------
total loss 62.928265322207345
main criterion 37.17083047723664
weighted_aux_loss 25.757434844970703
loss_r_bn_feature 257.5743408203125
------------iteration 800----------
total loss 59.3366469116105
main criterion 34.18427645872964
weighted_aux_loss 25.15237045288086
loss_r_bn_feature 251.52369689941406
------------iteration 900----------
total loss 62.519653049298775
main criterion 37.3347413213935
weighted_aux_loss 25.184911727905273
loss_r_bn_feature 251.84910583496094
------------iteration 1000----------
total loss 55.93262938057233
main criterion 32.199356828082095
weighted_aux_loss 23.733272552490234
loss_r_bn_feature 237.3327178955078
------------iteration 1100----------
total loss 59.073773350914365
main criterion 38.00109955330694
weighted_aux_loss 21.072673797607422
loss_r_bn_feature 210.7267303466797
------------iteration 1200----------
total loss 51.694523776340525
main criterion 31.19884010629658
weighted_aux_loss 20.495683670043945
loss_r_bn_feature 204.9568328857422
------------iteration 1300----------
total loss 65.27419890883384
main criterion 43.94137420180259
weighted_aux_loss 21.33282470703125
loss_r_bn_feature 213.3282470703125
------------iteration 1400----------
total loss 46.453919610607095
main criterion 27.111735543834634
weighted_aux_loss 19.34218406677246
loss_r_bn_feature 193.4218292236328
------------iteration 1500----------
total loss 46.54233746531912
main criterion 27.452682542833767
weighted_aux_loss 19.08965492248535
loss_r_bn_feature 190.89654541015625
------------iteration 1600----------
total loss 48.7434644910704
main criterion 29.979617139996183
weighted_aux_loss 18.76384735107422
loss_r_bn_feature 187.6384735107422
------------iteration 1700----------
total loss 58.1589140655746
main criterion 42.08968684694667
weighted_aux_loss 16.06922721862793
loss_r_bn_feature 160.69227600097656
------------iteration 1800----------
total loss 44.558251215542555
main criterion 26.920710398281813
weighted_aux_loss 17.637540817260742
loss_r_bn_feature 176.37539672851562
------------iteration 1900----------
total loss 45.886683996921064
main criterion 27.528026160007
weighted_aux_loss 18.358657836914062
loss_r_bn_feature 183.58657836914062
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 179.53787399331836
main criterion 134.33412719766406
weighted_aux_loss 45.2037467956543
loss_r_bn_feature 452.0374450683594
------------iteration 100----------
total loss 129.04509336948092
main criterion 93.50883467197117
weighted_aux_loss 35.536258697509766
loss_r_bn_feature 355.3625793457031
------------iteration 200----------
total loss 69.73961299888424
main criterion 40.04398197166256
weighted_aux_loss 29.69563102722168
loss_r_bn_feature 296.956298828125
------------iteration 300----------
total loss 69.14765794549004
main criterion 40.58011682305351
weighted_aux_loss 28.567541122436523
loss_r_bn_feature 285.6754150390625
------------iteration 400----------
total loss 64.44768734044555
main criterion 36.76240558690552
weighted_aux_loss 27.68528175354004
loss_r_bn_feature 276.8528137207031
------------iteration 500----------
total loss 62.29727762536384
main criterion 35.300529654782785
weighted_aux_loss 26.996747970581055
loss_r_bn_feature 269.96746826171875
------------iteration 600----------
total loss 58.96935116259924
main criterion 33.56733929126135
weighted_aux_loss 25.40201187133789
loss_r_bn_feature 254.02011108398438
------------iteration 700----------
total loss 71.00910233198387
main criterion 46.664671408521954
weighted_aux_loss 24.344430923461914
loss_r_bn_feature 243.44430541992188
------------iteration 800----------
total loss 82.15437208992464
main criterion 56.9437817273758
weighted_aux_loss 25.210590362548828
loss_r_bn_feature 252.10589599609375
------------iteration 900----------
total loss 57.78727817572467
main criterion 35.564902305973696
weighted_aux_loss 22.222375869750977
loss_r_bn_feature 222.2237548828125
------------iteration 1000----------
total loss 53.16110315101082
main criterion 31.4176415421241
weighted_aux_loss 21.74346160888672
loss_r_bn_feature 217.4346160888672
------------iteration 1100----------
total loss 58.46911606633239
main criterion 37.419273231859734
weighted_aux_loss 21.049842834472656
loss_r_bn_feature 210.49842834472656
------------iteration 1200----------
total loss 84.57221384226227
main criterion 61.22228021799469
weighted_aux_loss 23.349933624267578
loss_r_bn_feature 233.49932861328125
------------iteration 1300----------
total loss 51.557720385194834
main criterion 32.85066624701124
weighted_aux_loss 18.707054138183594
loss_r_bn_feature 187.07054138183594
------------iteration 1400----------
total loss 45.09708131540581
main criterion 26.2918235181646
weighted_aux_loss 18.80525779724121
loss_r_bn_feature 188.0525665283203
------------iteration 1500----------
total loss 45.1866169773874
main criterion 26.689045032196972
weighted_aux_loss 18.49757194519043
loss_r_bn_feature 184.97572326660156
------------iteration 1600----------
total loss 49.33302738711673
main criterion 31.38602879092532
weighted_aux_loss 17.946998596191406
loss_r_bn_feature 179.46998596191406
------------iteration 1700----------
total loss 48.172823140423446
main criterion 29.69623584489122
weighted_aux_loss 18.476587295532227
loss_r_bn_feature 184.765869140625
------------iteration 1800----------
total loss 48.924001751765694
main criterion 32.748756466731514
weighted_aux_loss 16.17524528503418
loss_r_bn_feature 161.75244140625
------------iteration 1900----------
total loss 44.26166825248339
main criterion 27.299843835369128
weighted_aux_loss 16.961824417114258
loss_r_bn_feature 169.6182403564453
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 176.9396977534808
main criterion 129.88189746051205
weighted_aux_loss 47.05780029296875
loss_r_bn_feature 470.5780029296875
------------iteration 100----------
total loss 81.60826195604287
main criterion 47.159104241199124
weighted_aux_loss 34.44915771484375
loss_r_bn_feature 344.4915771484375
------------iteration 200----------
total loss 71.80803788552025
main criterion 39.96001733193138
weighted_aux_loss 31.848020553588867
loss_r_bn_feature 318.4801940917969
------------iteration 300----------
total loss 84.16257280342967
main criterion 54.03820413582712
weighted_aux_loss 30.12436866760254
loss_r_bn_feature 301.2436828613281
------------iteration 400----------
total loss 71.11747223168412
main criterion 41.49115043908158
weighted_aux_loss 29.62632179260254
loss_r_bn_feature 296.2632141113281
------------iteration 500----------
total loss 61.86644625285458
main criterion 33.63443636515927
weighted_aux_loss 28.232009887695312
loss_r_bn_feature 282.3200988769531
------------iteration 600----------
total loss 112.05778485563333
main criterion 81.66581136014993
weighted_aux_loss 30.3919734954834
loss_r_bn_feature 303.91973876953125
------------iteration 700----------
total loss 68.26272495570238
main criterion 43.09263714137133
weighted_aux_loss 25.170087814331055
loss_r_bn_feature 251.7008819580078
------------iteration 800----------
total loss 68.14203516975246
main criterion 42.73713938728176
weighted_aux_loss 25.404895782470703
loss_r_bn_feature 254.0489501953125
------------iteration 900----------
total loss 61.78972116168358
main criterion 37.633968979676744
weighted_aux_loss 24.155752182006836
loss_r_bn_feature 241.55752563476562
------------iteration 1000----------
total loss 53.948873149018354
main criterion 30.694554911713666
weighted_aux_loss 23.254318237304688
loss_r_bn_feature 232.54318237304688
------------iteration 1100----------
total loss 64.35655963904739
main criterion 40.96714771277785
weighted_aux_loss 23.38941192626953
loss_r_bn_feature 233.8941192626953
------------iteration 1200----------
total loss 58.347531382068
main criterion 37.87980181358655
weighted_aux_loss 20.467729568481445
loss_r_bn_feature 204.6772918701172
------------iteration 1300----------
total loss 50.16965690564298
main criterion 29.73034110974454
weighted_aux_loss 20.439315795898438
loss_r_bn_feature 204.39315795898438
------------iteration 1400----------
total loss 47.8739626752477
main criterion 27.899643217239888
weighted_aux_loss 19.974319458007812
loss_r_bn_feature 199.74319458007812
------------iteration 1500----------
total loss 55.87202420463779
main criterion 37.56554570427158
weighted_aux_loss 18.30647850036621
loss_r_bn_feature 183.0647735595703
------------iteration 1600----------
total loss 59.405042395764895
main criterion 41.11926625745923
weighted_aux_loss 18.285776138305664
loss_r_bn_feature 182.85775756835938
------------iteration 1700----------
total loss 49.55725806105008
main criterion 30.724656513808863
weighted_aux_loss 18.83260154724121
loss_r_bn_feature 188.3260040283203
------------iteration 1800----------
total loss 43.90556954462575
main criterion 26.22944115717458
weighted_aux_loss 17.676128387451172
loss_r_bn_feature 176.7612762451172
------------iteration 1900----------
total loss 52.77136479012516
main criterion 35.990557295007974
weighted_aux_loss 16.780807495117188
loss_r_bn_feature 167.80807495117188
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 182.94071632120793
main criterion 139.83027702067082
weighted_aux_loss 43.11043930053711
loss_r_bn_feature 431.1043701171875
------------iteration 100----------
total loss 108.97406023999032
main criterion 75.0376169111329
weighted_aux_loss 33.93644332885742
loss_r_bn_feature 339.3644104003906
------------iteration 200----------
total loss 108.48879171176657
main criterion 77.03888059421286
weighted_aux_loss 31.44991111755371
loss_r_bn_feature 314.4991149902344
------------iteration 300----------
total loss 83.39455410157203
main criterion 55.15557666932105
weighted_aux_loss 28.238977432250977
loss_r_bn_feature 282.3897705078125
------------iteration 400----------
total loss 65.20645083482695
main criterion 37.73235644395781
weighted_aux_loss 27.47409439086914
loss_r_bn_feature 274.7409362792969
------------iteration 500----------
total loss 72.96603744899426
main criterion 46.27217071925793
weighted_aux_loss 26.693866729736328
loss_r_bn_feature 266.93865966796875
------------iteration 600----------
total loss 70.01348569662142
main criterion 42.78564145834017
weighted_aux_loss 27.22784423828125
loss_r_bn_feature 272.2784423828125
------------iteration 700----------
total loss 58.88607718060596
main criterion 33.88409544537647
weighted_aux_loss 25.001981735229492
loss_r_bn_feature 250.0198211669922
------------iteration 800----------
total loss 62.12228145221442
main criterion 38.121747394597236
weighted_aux_loss 24.000534057617188
loss_r_bn_feature 240.00534057617188
------------iteration 900----------
total loss 63.52311078421022
main criterion 40.70853177420045
weighted_aux_loss 22.814579010009766
loss_r_bn_feature 228.14578247070312
------------iteration 1000----------
total loss 64.93077837790511
main criterion 44.68881403769516
weighted_aux_loss 20.24196434020996
loss_r_bn_feature 202.4196319580078
------------iteration 1100----------
total loss 51.47598529444511
main criterion 30.292631870372848
weighted_aux_loss 21.183353424072266
loss_r_bn_feature 211.83352661132812
------------iteration 1200----------
total loss 51.661831340128366
main criterion 30.636019191080514
weighted_aux_loss 21.02581214904785
loss_r_bn_feature 210.25811767578125
------------iteration 1300----------
total loss 49.59169235498632
main criterion 30.405874636480462
weighted_aux_loss 19.18581771850586
loss_r_bn_feature 191.85816955566406
------------iteration 1400----------
total loss 50.718406018905064
main criterion 33.35111170738163
weighted_aux_loss 17.367294311523438
loss_r_bn_feature 173.67294311523438
------------iteration 1500----------
total loss 43.43042910543397
main criterion 25.574142102870493
weighted_aux_loss 17.856287002563477
loss_r_bn_feature 178.5628662109375
------------iteration 1600----------
total loss 67.95156042903804
main criterion 48.74503844112301
weighted_aux_loss 19.20652198791504
loss_r_bn_feature 192.06521606445312
------------iteration 1700----------
total loss 93.94080906462227
main criterion 69.98921375822579
weighted_aux_loss 23.951595306396484
loss_r_bn_feature 239.5159454345703
------------iteration 1800----------
total loss 44.176042081321455
main criterion 27.943950177634935
weighted_aux_loss 16.232091903686523
loss_r_bn_feature 162.32090759277344
------------iteration 1900----------
total loss 78.54695640669155
main criterion 58.980521546462064
weighted_aux_loss 19.566434860229492
loss_r_bn_feature 195.6643524169922
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 183.23760182620697
main criterion 136.8326907849472
weighted_aux_loss 46.404911041259766
loss_r_bn_feature 464.0491027832031
------------iteration 100----------
total loss 84.31674282913917
main criterion 51.30983822708839
weighted_aux_loss 33.00690460205078
loss_r_bn_feature 330.06903076171875
------------iteration 200----------
total loss 70.90153588366056
main criterion 41.8778389842465
weighted_aux_loss 29.023696899414062
loss_r_bn_feature 290.2369689941406
------------iteration 300----------
total loss 82.45896653341745
main criterion 53.00008324789499
weighted_aux_loss 29.45888328552246
loss_r_bn_feature 294.5888366699219
------------iteration 400----------
total loss 105.74930428294519
main criterion 75.63883637217859
weighted_aux_loss 30.1104679107666
loss_r_bn_feature 301.10467529296875
------------iteration 500----------
total loss 65.35082991173587
main criterion 39.133987260368684
weighted_aux_loss 26.216842651367188
loss_r_bn_feature 262.1684265136719
------------iteration 600----------
total loss 94.36836743777835
main criterion 66.37878346866214
weighted_aux_loss 27.98958396911621
loss_r_bn_feature 279.8958435058594
------------iteration 700----------
total loss 57.490109112416654
main criterion 32.752106335317045
weighted_aux_loss 24.73800277709961
loss_r_bn_feature 247.38002014160156
------------iteration 800----------
total loss 59.87558035553712
main criterion 36.45879225434083
weighted_aux_loss 23.41678810119629
loss_r_bn_feature 234.16787719726562
------------iteration 900----------
total loss 86.93798046509585
main criterion 62.397500805061675
weighted_aux_loss 24.54047966003418
loss_r_bn_feature 245.40478515625
------------iteration 1000----------
total loss 60.52835720669262
main criterion 37.643303572049064
weighted_aux_loss 22.885053634643555
loss_r_bn_feature 228.8505401611328
------------iteration 1100----------
total loss 52.63004871139623
main criterion 31.672098119355212
weighted_aux_loss 20.957950592041016
loss_r_bn_feature 209.57949829101562
------------iteration 1200----------
total loss 71.16582189516141
main criterion 50.745392665425086
weighted_aux_loss 20.420429229736328
loss_r_bn_feature 204.20428466796875
------------iteration 1300----------
total loss 48.586768940608906
main criterion 30.3507192885093
weighted_aux_loss 18.23604965209961
loss_r_bn_feature 182.36048889160156
------------iteration 1400----------
total loss 51.173404101077324
main criterion 33.06492364758611
weighted_aux_loss 18.10848045349121
loss_r_bn_feature 181.0847930908203
------------iteration 1500----------
total loss 42.476254976061355
main criterion 24.794438874987137
weighted_aux_loss 17.68181610107422
loss_r_bn_feature 176.8181610107422
------------iteration 1600----------
total loss 51.17542050804854
main criterion 33.43394635643721
weighted_aux_loss 17.741474151611328
loss_r_bn_feature 177.41473388671875
------------iteration 1700----------
total loss 65.04109789386283
main criterion 47.404718651919474
weighted_aux_loss 17.63637924194336
loss_r_bn_feature 176.36378479003906
------------iteration 1800----------
total loss 48.35292416523209
main criterion 32.088191804392245
weighted_aux_loss 16.264732360839844
loss_r_bn_feature 162.64732360839844
------------iteration 1900----------
total loss 48.18534794182825
main criterion 32.567344096613404
weighted_aux_loss 15.618003845214844
loss_r_bn_feature 156.18003845214844
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 205.91535536121808
main criterion 158.73457304310284
weighted_aux_loss 47.180782318115234
loss_r_bn_feature 471.80780029296875
------------iteration 100----------
total loss 86.57497715135236
main criterion 52.3569443144383
weighted_aux_loss 34.21803283691406
loss_r_bn_feature 342.1803283691406
------------iteration 200----------
total loss 73.7234997921814
main criterion 42.63625194367065
weighted_aux_loss 31.087247848510742
loss_r_bn_feature 310.8724670410156
------------iteration 300----------
total loss 75.49538154748907
main criterion 44.792012406864075
weighted_aux_loss 30.703369140625
loss_r_bn_feature 307.03369140625
------------iteration 400----------
total loss 65.30760950110215
main criterion 37.77414697668809
weighted_aux_loss 27.533462524414062
loss_r_bn_feature 275.3346252441406
------------iteration 500----------
total loss 67.78733742143885
main criterion 39.66101944353358
weighted_aux_loss 28.126317977905273
loss_r_bn_feature 281.26318359375
------------iteration 600----------
total loss 60.13770349518174
main criterion 33.60933168426865
weighted_aux_loss 26.528371810913086
loss_r_bn_feature 265.2837219238281
------------iteration 700----------
total loss 77.44651880077576
main criterion 52.212206743280646
weighted_aux_loss 25.234312057495117
loss_r_bn_feature 252.34310913085938
------------iteration 800----------
total loss 56.27917637498761
main criterion 32.28227200181866
weighted_aux_loss 23.996904373168945
loss_r_bn_feature 239.9690399169922
------------iteration 900----------
total loss 89.34338469186294
main criterion 62.757859179167625
weighted_aux_loss 26.585525512695312
loss_r_bn_feature 265.8552551269531
------------iteration 1000----------
total loss 53.12698727263684
main criterion 31.495096022026484
weighted_aux_loss 21.63189125061035
loss_r_bn_feature 216.31890869140625
------------iteration 1100----------
total loss 53.15351795830692
main criterion 31.005517241143835
weighted_aux_loss 22.148000717163086
loss_r_bn_feature 221.47999572753906
------------iteration 1200----------
total loss 67.38954249633346
main criterion 47.6956509710649
weighted_aux_loss 19.693891525268555
loss_r_bn_feature 196.93890380859375
------------iteration 1300----------
total loss 50.06402670109096
main criterion 30.006920683024553
weighted_aux_loss 20.057106018066406
loss_r_bn_feature 200.57106018066406
------------iteration 1400----------
total loss 77.55485379261373
main criterion 59.95512425464985
weighted_aux_loss 17.599729537963867
loss_r_bn_feature 175.99728393554688
------------iteration 1500----------
total loss 46.215133204687206
main criterion 28.361833110082717
weighted_aux_loss 17.853300094604492
loss_r_bn_feature 178.5330047607422
------------iteration 1600----------
total loss 42.4279282515936
main criterion 24.953931135504735
weighted_aux_loss 17.473997116088867
loss_r_bn_feature 174.73997497558594
------------iteration 1700----------
total loss 44.41808122842751
main criterion 26.277679388217553
weighted_aux_loss 18.14040184020996
loss_r_bn_feature 181.4040069580078
------------iteration 1800----------
total loss 45.93089793263529
main criterion 28.74977610646341
weighted_aux_loss 17.181121826171875
loss_r_bn_feature 171.81121826171875
------------iteration 1900----------
total loss 44.33813209100968
main criterion 28.21376723810441
weighted_aux_loss 16.124364852905273
loss_r_bn_feature 161.24365234375
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 172.14207028726364
main criterion 127.4423899588945
weighted_aux_loss 44.69968032836914
loss_r_bn_feature 446.9967956542969
------------iteration 100----------
total loss 117.01161537526241
main criterion 83.43831596730342
weighted_aux_loss 33.573299407958984
loss_r_bn_feature 335.7330017089844
------------iteration 200----------
total loss 67.32170633735313
main criterion 40.17359499396935
weighted_aux_loss 27.14811134338379
loss_r_bn_feature 271.4811096191406
------------iteration 300----------
total loss 65.96259169460998
main criterion 40.67057089688049
weighted_aux_loss 25.292020797729492
loss_r_bn_feature 252.92019653320312
------------iteration 400----------
total loss 59.48655690268114
main criterion 35.09066952780321
weighted_aux_loss 24.39588737487793
loss_r_bn_feature 243.9588623046875
------------iteration 500----------
total loss 85.71822186394061
main criterion 60.78638096733417
weighted_aux_loss 24.931840896606445
loss_r_bn_feature 249.3184051513672
------------iteration 600----------
total loss 59.068262849816016
main criterion 36.97432402230137
weighted_aux_loss 22.09393882751465
loss_r_bn_feature 220.9393768310547
------------iteration 700----------
total loss 50.90571549377765
main criterion 29.533866433719055
weighted_aux_loss 21.371849060058594
loss_r_bn_feature 213.71849060058594
------------iteration 800----------
total loss 60.26840942052354
main criterion 39.360358883414165
weighted_aux_loss 20.908050537109375
loss_r_bn_feature 209.08050537109375
------------iteration 900----------
total loss 54.284576839059795
main criterion 34.5594734607639
weighted_aux_loss 19.7251033782959
loss_r_bn_feature 197.25103759765625
------------iteration 1000----------
total loss 50.982881653360636
main criterion 31.58760653739384
weighted_aux_loss 19.395275115966797
loss_r_bn_feature 193.95274353027344
------------iteration 1100----------
total loss 62.85391583692581
main criterion 45.5785423780635
weighted_aux_loss 17.275373458862305
loss_r_bn_feature 172.75372314453125
------------iteration 1200----------
total loss 134.53764856797525
main criterion 101.7695325706608
weighted_aux_loss 32.76811599731445
loss_r_bn_feature 327.68115234375
------------iteration 1300----------
total loss 47.7968915811621
main criterion 30.181346308334952
weighted_aux_loss 17.61554527282715
loss_r_bn_feature 176.15545654296875
------------iteration 1400----------
total loss 43.25747546543091
main criterion 27.279868692054443
weighted_aux_loss 15.977606773376465
loss_r_bn_feature 159.77606201171875
------------iteration 1500----------
total loss 63.115754047882184
main criterion 46.47671977664195
weighted_aux_loss 16.639034271240234
loss_r_bn_feature 166.3903350830078
------------iteration 1600----------
total loss 45.40306335065404
main criterion 31.069232517218975
weighted_aux_loss 14.333830833435059
loss_r_bn_feature 143.3383026123047
------------iteration 1700----------
total loss 38.58737310006643
main criterion 23.473344169402363
weighted_aux_loss 15.114028930664062
loss_r_bn_feature 151.14028930664062
------------iteration 1800----------
total loss 40.40131098409924
main criterion 27.020103517973755
weighted_aux_loss 13.381207466125488
loss_r_bn_feature 133.81207275390625
------------iteration 1900----------
total loss 41.164718281299244
main criterion 27.625147472888603
weighted_aux_loss 13.539570808410645
loss_r_bn_feature 135.3957061767578
=> using pytorch pre-trained model 'resnet18'
process data from ../recover/syn_data/tiny-imagenet/238
  0%|          | 0/300 [00:00<?, ?it/s]  0%|          | 1/300 [00:08<44:50,  9.00s/it]  1%|          | 2/300 [00:11<24:53,  5.01s/it]  1%|          | 3/300 [00:13<18:16,  3.69s/it]  1%|▏         | 4/300 [00:15<15:16,  3.10s/it]  2%|▏         | 5/300 [00:17<13:31,  2.75s/it]  2%|▏         | 6/300 [00:19<12:20,  2.52s/it]  2%|▏         | 7/300 [00:21<11:45,  2.41s/it]  3%|▎         | 8/300 [00:24<11:23,  2.34s/it]  3%|▎         | 9/300 [00:26<11:01,  2.27s/it]  3%|▎         | 10/300 [00:28<10:43,  2.22s/it]  4%|▎         | 11/300 [00:30<10:36,  2.20s/it]  4%|▍         | 12/300 [00:32<10:37,  2.21s/it]  4%|▍         | 13/300 [00:35<11:35,  2.42s/it]  5%|▍         | 14/300 [00:40<15:30,  3.26s/it]  5%|▌         | 15/300 [00:43<14:06,  2.97s/it]  5%|▌         | 16/300 [00:45<13:16,  2.80s/it]  6%|▌         | 17/300 [00:48<12:52,  2.73s/it]  6%|▌         | 18/300 [00:50<12:18,  2.62s/it]  6%|▋         | 19/300 [00:52<12:02,  2.57s/it]  7%|▋         | 20/300 [00:55<11:26,  2.45s/it]  7%|▋         | 21/300 [00:57<11:02,  2.37s/it]  7%|▋         | 22/300 [00:59<10:44,  2.32s/it]  8%|▊         | 23/300 [01:01<10:31,  2.28s/it]  8%|▊         | 24/300 [01:03<10:21,  2.25s/it]  8%|▊         | 25/300 [01:06<10:14,  2.24s/it]  9%|▊         | 26/300 [01:08<10:16,  2.25s/it]  9%|▉         | 27/300 [01:10<10:06,  2.22s/it]  9%|▉         | 28/300 [01:12<10:04,  2.22s/it] 10%|▉         | 29/300 [01:14<09:56,  2.20s/it] 10%|█         | 30/300 [01:17<09:53,  2.20s/it] 10%|█         | 31/300 [01:19<09:59,  2.23s/it] 11%|█         | 32/300 [01:21<09:53,  2.22s/it] 11%|█         | 33/300 [01:23<09:54,  2.23s/it] 11%|█▏        | 34/300 [01:26<09:56,  2.24s/it] 12%|█▏        | 35/300 [01:28<09:54,  2.24s/it] 12%|█▏        | 36/300 [01:30<09:58,  2.27s/it] 12%|█▏        | 37/300 [01:32<09:57,  2.27s/it] 13%|█▎        | 38/300 [01:35<09:49,  2.25s/it] 13%|█▎        | 39/300 [01:37<09:49,  2.26s/it] 13%|█▎        | 40/300 [01:39<09:39,  2.23s/it] 14%|█▎        | 41/300 [01:41<09:32,  2.21s/it] 14%|█▍        | 42/300 [01:44<09:40,  2.25s/it] 14%|█▍        | 43/300 [01:46<09:33,  2.23s/it] 15%|█▍        | 44/300 [01:48<09:31,  2.23s/it] 15%|█▌        | 45/300 [01:50<09:28,  2.23s/it] 15%|█▌        | 46/300 [01:52<09:29,  2.24s/it] 16%|█▌        | 47/300 [01:55<09:26,  2.24s/it] 16%|█▌        | 48/300 [01:57<09:17,  2.21s/it] 16%|█▋        | 49/300 [01:59<09:16,  2.22s/it] 17%|█▋        | 50/300 [02:01<09:15,  2.22s/it] 17%|█▋        | 51/300 [02:04<09:17,  2.24s/it] 17%|█▋        | 52/300 [02:06<09:26,  2.28s/it] 18%|█▊        | 53/300 [02:08<09:25,  2.29s/it] 18%|█▊        | 54/300 [02:11<09:19,  2.28s/it] 18%|█▊        | 55/300 [02:13<09:14,  2.26s/it] 19%|█▊        | 56/300 [02:15<09:12,  2.26s/it] 19%|█▉        | 57/300 [02:17<09:10,  2.26s/it] 19%|█▉        | 58/300 [02:19<09:02,  2.24s/it] 20%|█▉        | 59/300 [02:22<08:53,  2.21s/it] 20%|██        | 60/300 [02:24<08:50,  2.21s/it] 20%|██        | 61/300 [02:26<08:53,  2.23s/it] 21%|██        | 62/300 [02:28<08:44,  2.20s/it] 21%|██        | 63/300 [02:30<08:43,  2.21s/it] 21%|██▏       | 64/300 [02:33<08:39,  2.20s/it] 22%|██▏       | 65/300 [02:35<08:36,  2.20s/it] 22%|██▏       | 66/300 [02:37<08:39,  2.22s/it] 22%|██▏       | 67/300 [02:39<08:37,  2.22s/it] 23%|██▎       | 68/300 [02:42<08:39,  2.24s/it] 23%|██▎       | 69/300 [02:44<08:32,  2.22s/it] 23%|██▎       | 70/300 [02:46<08:40,  2.26s/it] 24%|██▎       | 71/300 [02:48<08:41,  2.28s/it] 24%|██▍       | 72/300 [02:51<08:35,  2.26s/it] 24%|██▍       | 73/300 [02:53<08:27,  2.23s/it] 25%|██▍       | 74/300 [02:55<08:21,  2.22s/it] 25%|██▌       | 75/300 [02:57<08:14,  2.20s/it] 25%|██▌       | 76/300 [03:00<08:20,  2.23s/it] 26%|██▌       | 77/300 [03:02<08:12,  2.21s/it] 26%|██▌       | 78/300 [03:04<08:11,  2.21s/it] 26%|██▋       | 79/300 [03:06<08:07,  2.20s/it] 27%|██▋       | 80/300 [03:08<08:08,  2.22s/it] 27%|██▋       | 81/300 [03:11<08:08,  2.23s/it] 27%|██▋       | 82/300 [03:13<08:05,  2.23s/it] 28%|██▊       | 83/300 [03:15<08:00,  2.22s/it] 28%|██▊       | 84/300 [03:17<07:56,  2.20s/it] 28%|██▊       | 85/300 [03:19<07:57,  2.22s/it] 29%|██▊       | 86/300 [03:22<08:00,  2.24s/it] 29%|██▉       | 87/300 [03:24<08:01,  2.26s/it] 29%|██▉       | 88/300 [03:26<07:57,  2.25s/it] 30%|██▉       | 89/300 [03:29<07:58,  2.27s/it] 30%|███       | 90/300 [03:31<07:53,  2.25s/it] 30%|███       | 91/300 [03:33<07:46,  2.23s/it] 31%|███       | 92/300 [03:35<07:44,  2.23s/it] 31%|███       | 93/300 [03:37<07:38,  2.21s/it] 31%|███▏      | 94/300 [03:40<07:37,  2.22s/it] 32%|███▏      | 95/300 [03:42<07:35,  2.22s/it] 32%|███▏      | 96/300 [03:44<07:31,  2.21s/it] 32%|███▏      | 97/300 [03:46<07:32,  2.23s/it] 33%|███▎      | 98/300 [03:48<07:24,  2.20s/it] 33%|███▎      | 99/300 [03:51<07:23,  2.20s/it] 33%|███▎      | 100/300 [03:53<07:20,  2.20s/it] 34%|███▎      | 101/300 [03:55<07:16,  2.19s/it] 34%|███▍      | 102/300 [03:57<07:16,  2.21s/it] 34%|███▍      | 103/300 [04:00<07:23,  2.25s/it] 35%|███▍      | 104/300 [04:02<07:20,  2.25s/it] 35%|███▌      | 105/300 [04:04<07:17,  2.24s/it] 35%|███▌      | 106/300 [04:06<07:12,  2.23s/it] 36%|███▌      | 107/300 [04:09<07:10,  2.23s/it] 36%|███▌      | 108/300 [04:11<07:10,  2.24s/it] 36%|███▋      | 109/300 [04:13<07:05,  2.23s/it] 37%|███▋      | 110/300 [04:15<07:00,  2.21s/it] 37%|███▋      | 111/300 [04:17<06:54,  2.20s/it] 37%|███▋      | 112/300 [04:20<06:55,  2.21s/it] 38%|███▊      | 113/300 [04:22<06:49,  2.19s/it] 38%|███▊      | 114/300 [04:24<06:46,  2.19s/it] 38%|███▊      | 115/300 [04:26<06:45,  2.19s/it] 39%|███▊      | 116/300 [04:28<06:41,  2.18s/it] 39%|███▉      | 117/300 [04:30<06:38,  2.18s/it] 39%|███▉      | 118/300 [04:33<06:39,  2.20s/it] 40%|███▉      | 119/300 [04:35<06:36,  2.19s/it] 40%|████      | 120/300 [04:37<06:32,  2.18s/it] 40%|████      | 121/300 [04:39<06:31,  2.19s/it] 41%|████      | 122/300 [04:42<06:38,  2.24s/it] 41%|████      | 123/300 [04:44<06:32,  2.22s/it] 41%|████▏     | 124/300 [04:46<06:32,  2.23s/it] 42%|████▏     | 125/300 [04:48<06:33,  2.25s/it] 42%|████▏     | 126/300 [04:51<06:33,  2.26s/it] 42%|████▏     | 127/300 [04:53<06:28,  2.24s/it] 43%|████▎     | 128/300 [04:55<06:25,  2.24s/it] 43%|████▎     | 129/300 [04:57<06:23,  2.24s/it] 43%|████▎     | 130/300 [04:59<06:18,  2.23s/it] 44%|████▎     | 131/300 [05:02<06:18,  2.24s/it] 44%|████▍     | 132/300 [05:04<06:16,  2.24s/it] 44%|████▍     | 133/300 [05:06<06:15,  2.25s/it] 45%|████▍     | 134/300 [05:08<06:09,  2.23s/it] 45%|████▌     | 135/300 [05:11<06:05,  2.22s/it] 45%|████▌     | 136/300 [05:13<06:06,  2.24s/it] 46%|████▌     | 137/300 [05:15<06:05,  2.24s/it] 46%|████▌     | 138/300 [05:17<06:01,  2.23s/it] 46%|████▋     | 139/300 [05:20<05:58,  2.22s/it] 47%|████▋     | 140/300 [05:22<05:53,  2.21s/it] 47%|████▋     | 141/300 [05:24<05:55,  2.24s/it] 47%|████▋     | 142/300 [05:26<05:53,  2.24s/it] 48%|████▊     | 143/300 [05:29<05:54,  2.26s/it] 48%|████▊     | 144/300 [05:31<05:50,  2.25s/it] 48%|████▊     | 145/300 [05:33<05:50,  2.26s/it] 49%|████▊     | 146/300 [05:35<05:47,  2.26s/it] 49%|████▉     | 147/300 [05:37<05:40,  2.23s/it] 49%|████▉     | 148/300 [05:40<05:35,  2.21s/it] 50%|████▉     | 149/300 [05:42<05:34,  2.22s/it] 50%|█████     | 150/300 [05:44<05:32,  2.22s/it] 50%|█████     | 151/300 [05:46<05:29,  2.21s/it] 51%|█████     | 152/300 [05:49<05:29,  2.23s/it] 51%|█████     | 153/300 [05:51<05:27,  2.23s/it] 51%|█████▏    | 154/300 [05:53<05:23,  2.22s/it] 52%|█████▏    | 155/300 [05:55<05:21,  2.22s/it] 52%|█████▏    | 156/300 [05:57<05:19,  2.22s/it] 52%|█████▏    | 157/300 [06:00<05:20,  2.24s/it] 53%|█████▎    | 158/300 [06:02<05:19,  2.25s/it] 53%|█████▎    | 159/300 [06:04<05:13,  2.23s/it] 53%|█████▎    | 160/300 [06:06<05:12,  2.23s/it] 54%|█████▎    | 161/300 [06:09<05:06,  2.21s/it] 54%|█████▍    | 162/300 [06:11<05:05,  2.21s/it] 54%|█████▍    | 163/300 [06:13<05:03,  2.22s/it] 55%|█████▍    | 164/300 [06:15<05:04,  2.24s/it] 55%|█████▌    | 165/300 [06:18<05:01,  2.24s/it] 55%|█████▌    | 166/300 [06:20<05:02,  2.26s/it] 56%|█████▌    | 167/300 [06:22<05:01,  2.27s/it] 56%|█████▌    | 168/300 [06:24<04:55,  2.24s/it] 56%|█████▋    | 169/300 [06:26<04:51,  2.23s/it] 57%|█████▋    | 170/300 [06:29<04:48,  2.22s/it] 57%|█████▋    | 171/300 [06:31<04:46,  2.22s/it] 57%|█████▋    | 172/300 [06:33<04:45,  2.23s/it] 58%|█████▊    | 173/300 [06:35<04:42,  2.22s/it] 58%|█████▊    | 174/300 [06:38<04:43,  2.25s/it] 58%|█████▊    | 175/300 [06:40<04:42,  2.26s/it] 59%|█████▊    | 176/300 [06:42<04:40,  2.26s/it] 59%|█████▉    | 177/300 [06:45<04:39,  2.27s/it] 59%|█████▉    | 178/300 [06:47<04:33,  2.24s/it] 60%|█████▉    | 179/300 [06:49<04:27,  2.21s/it] 60%|██████    | 180/300 [06:51<04:25,  2.21s/it] 60%|██████    | 181/300 [06:53<04:24,  2.23s/it] 61%|██████    | 182/300 [06:56<04:25,  2.25s/it] 61%|██████    | 183/300 [06:58<04:25,  2.27s/it] 61%|██████▏   | 184/300 [07:00<04:20,  2.25s/it] 62%|██████▏   | 185/300 [07:02<04:17,  2.24s/it] 62%|██████▏   | 186/300 [07:05<04:14,  2.23s/it] 62%|██████▏   | 187/300 [07:07<04:10,  2.22s/it] 63%|██████▎   | 188/300 [07:09<04:09,  2.23s/it] 63%|██████▎   | 189/300 [07:11<04:08,  2.24s/it] 63%|██████▎   | 190/300 [07:14<04:09,  2.27s/it] 64%|██████▎   | 191/300 [07:16<04:08,  2.28s/it] 64%|██████▍   | 192/300 [07:18<04:06,  2.28s/it] 64%|██████▍   | 193/300 [07:20<04:00,  2.25s/it] 65%|██████▍   | 194/300 [07:23<03:59,  2.26s/it] 65%|██████▌   | 195/300 [07:25<04:01,  2.30s/it] 65%|██████▌   | 196/300 [07:27<03:58,  2.29s/it] 66%|██████▌   | 197/300 [07:30<03:58,  2.32s/it] 66%|██████▌   | 198/300 [07:32<03:53,  2.29s/it] 66%|██████▋   | 199/300 [07:34<03:51,  2.30s/it] 67%|██████▋   | 200/300 [07:36<03:46,  2.27s/it] 67%|██████▋   | 201/300 [07:39<03:43,  2.26s/it] 67%|██████▋   | 202/300 [07:41<03:38,  2.23s/it] 68%|██████▊   | 203/300 [07:43<03:35,  2.22s/it] 68%|██████▊   | 204/300 [07:45<03:33,  2.22s/it] 68%|██████▊   | 205/300 [07:48<03:31,  2.23s/it] 69%|██████▊   | 206/300 [07:50<03:28,  2.22s/it] 69%|██████▉   | 207/300 [07:52<03:25,  2.21s/it] 69%|██████▉   | 208/300 [07:54<03:25,  2.23s/it] 70%|██████▉   | 209/300 [07:56<03:21,  2.22s/it] 70%|███████   | 210/300 [07:59<03:21,  2.24s/it] 70%|███████   | 211/300 [08:01<03:19,  2.24s/it] 71%|███████   | 212/300 [08:03<03:19,  2.27s/it] 71%|███████   | 213/300 [08:05<03:13,  2.23s/it] 71%|███████▏  | 214/300 [08:08<03:10,  2.22s/it] 72%|███████▏  | 215/300 [08:10<03:10,  2.25s/it] 72%|███████▏  | 216/300 [08:12<03:09,  2.25s/it] 72%|███████▏  | 217/300 [08:14<03:09,  2.29s/it] 73%|███████▎  | 218/300 [08:17<03:07,  2.28s/it] 73%|███████▎  | 219/300 [08:19<03:05,  2.29s/it] 73%|███████▎  | 220/300 [08:21<03:02,  2.28s/it] 74%|███████▎  | 221/300 [08:24<02:57,  2.25s/it] 74%|███████▍  | 222/300 [08:26<02:53,  2.22s/it] 74%|███████▍  | 223/300 [08:28<02:52,  2.24s/it] 75%|███████▍  | 224/300 [08:30<02:51,  2.25s/it] 75%|███████▌  | 225/300 [08:33<02:49,  2.26s/it] 75%|███████▌  | 226/300 [08:35<02:46,  2.25s/it] 76%|███████▌  | 227/300 [08:37<02:44,  2.25s/it] 76%|███████▌  | 228/300 [08:39<02:41,  2.24s/it] 76%|███████▋  | 229/300 [08:41<02:39,  2.25s/it] 77%|███████▋  | 230/300 [08:44<02:36,  2.24s/it] 77%|███████▋  | 231/300 [08:46<02:34,  2.24s/it] 77%|███████▋  | 232/300 [08:48<02:32,  2.24s/it] 78%|███████▊  | 233/300 [08:51<02:34,  2.30s/it] 78%|███████▊  | 234/300 [08:53<02:31,  2.30s/it] 78%|███████▊  | 235/300 [08:55<02:29,  2.30s/it] 79%|███████▊  | 236/300 [08:57<02:25,  2.27s/it] 79%|███████▉  | 237/300 [09:00<02:22,  2.26s/it] 79%|███████▉  | 238/300 [09:02<02:18,  2.23s/it] 80%|███████▉  | 239/300 [09:04<02:16,  2.25s/it] 80%|████████  | 240/300 [09:06<02:13,  2.23s/it] 80%|████████  | 241/300 [09:09<02:12,  2.24s/it] 81%|████████  | 242/300 [09:11<02:10,  2.25s/it] 81%|████████  | 243/300 [09:13<02:07,  2.24s/it] 81%|████████▏ | 244/300 [09:15<02:04,  2.23s/it] 82%|████████▏ | 245/300 [09:17<02:01,  2.21s/it] 82%|████████▏ | 246/300 [09:20<02:01,  2.24s/it] 82%|████████▏ | 247/300 [09:22<01:57,  2.22s/it] 83%|████████▎ | 248/300 [09:24<01:55,  2.22s/it] 83%|████████▎ | 249/300 [09:26<01:52,  2.22s/it] 83%|████████▎ | 250/300 [09:29<01:50,  2.22s/it] 84%|████████▎ | 251/300 [09:31<01:52,  2.30s/it] 84%|████████▍ | 252/300 [09:33<01:48,  2.26s/it] 84%|████████▍ | 253/300 [09:35<01:45,  2.25s/it] 85%|████████▍ | 254/300 [09:38<01:42,  2.24s/it] 85%|████████▌ | 255/300 [09:40<01:42,  2.27s/it] 85%|████████▌ | 256/300 [09:42<01:39,  2.26s/it] 86%|████████▌ | 257/300 [09:44<01:35,  2.23s/it] 86%|████████▌ | 258/300 [09:47<01:33,  2.22s/it] 86%|████████▋ | 259/300 [09:49<01:30,  2.20s/it] 87%|████████▋ | 260/300 [09:51<01:29,  2.23s/it] 87%|████████▋ | 261/300 [09:53<01:27,  2.24s/it] 87%|████████▋ | 262/300 [09:56<01:24,  2.23s/it] 88%|████████▊ | 263/300 [09:58<01:21,  2.21s/it] 88%|████████▊ | 264/300 [10:00<01:20,  2.23s/it] 88%|████████▊ | 265/300 [10:02<01:18,  2.24s/it] 89%|████████▊ | 266/300 [10:05<01:17,  2.29s/it] 89%|████████▉ | 267/300 [10:07<01:14,  2.26s/it] 89%|████████▉ | 268/300 [10:09<01:12,  2.26s/it] 90%|████████▉ | 269/300 [10:11<01:11,  2.29s/it] 90%|█████████ | 270/300 [10:14<01:08,  2.29s/it] 90%|█████████ | 271/300 [10:16<01:05,  2.25s/it] 91%|█████████ | 272/300 [10:18<01:02,  2.25s/it] 91%|█████████ | 273/300 [10:20<01:00,  2.22s/it] 91%|█████████▏| 274/300 [10:22<00:57,  2.21s/it] 92%|█████████▏| 275/300 [10:25<00:54,  2.19s/it] 92%|█████████▏| 276/300 [10:27<00:52,  2.18s/it] 92%|█████████▏| 277/300 [10:29<00:50,  2.19s/it] 93%|█████████▎| 278/300 [10:31<00:48,  2.22s/it] 93%|█████████▎| 279/300 [10:34<00:46,  2.23s/it] 93%|█████████▎| 280/300 [10:36<00:46,  2.34s/it] 94%|█████████▎| 281/300 [10:38<00:43,  2.31s/it] 94%|█████████▍| 282/300 [10:41<00:40,  2.27s/it] 94%|█████████▍| 283/300 [10:43<00:38,  2.28s/it] 95%|█████████▍| 284/300 [10:45<00:37,  2.37s/it] 95%|█████████▌| 285/300 [10:48<00:35,  2.36s/it] 95%|█████████▌| 286/300 [10:50<00:32,  2.32s/it] 96%|█████████▌| 287/300 [10:52<00:30,  2.31s/it] 96%|█████████▌| 288/300 [10:54<00:27,  2.26s/it] 96%|█████████▋| 289/300 [10:57<00:24,  2.24s/it] 97%|█████████▋| 290/300 [10:59<00:22,  2.21s/it] 97%|█████████▋| 291/300 [11:01<00:20,  2.23s/it] 97%|█████████▋| 292/300 [11:03<00:17,  2.22s/it] 98%|█████████▊| 293/300 [11:05<00:15,  2.24s/it] 98%|█████████▊| 294/300 [11:08<00:13,  2.26s/it] 98%|█████████▊| 295/300 [11:10<00:11,  2.23s/it] 99%|█████████▊| 296/300 [11:12<00:08,  2.21s/it] 99%|█████████▉| 297/300 [11:14<00:06,  2.22s/it] 99%|█████████▉| 298/300 [11:17<00:04,  2.25s/it]100%|█████████▉| 299/300 [11:19<00:02,  2.26s/it]100%|██████████| 300/300 [11:21<00:00,  2.26s/it]100%|██████████| 300/300 [11:21<00:00,  2.27s/it]
W&B disabled.
W&B online. Running your script from this directory will now sync to the cloud.
wandb: Currently logged in as: hl57. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.12 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.10
wandb: Run data is saved locally in /media/techt/One Touch/DD/SRe2L/train/wandb/run-20231007_145721-xr8zkkd0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cerulean-water-405
wandb: ⭐️ View project at https://wandb.ai/hl57/final_rn18_fkd
wandb: 🚀 View run at https://wandb.ai/hl57/final_rn18_fkd/runs/xr8zkkd0
======= FKD: dataset info ======
path: ../relabel/FKD_cutmix_fp16/tiny-imagenet/238/
num img: 2000
batch size: 100
max epoch: 300
================================
load data successfully
=> loading student model 'resnet18'

Epoch: 0
TRAIN Iter 0: lr = 0.001000,	loss = 0.007332,	Top-1 err = 99.100000,	Top-5 err = 95.450000,	train_time = 15.960356
TEST Iter 0: loss = 7.476370,	Top-1 err = 99.060000,	Top-5 err = 95.000000,	val_time = 17.892302

Epoch: 1

Epoch: 2

Epoch: 3

Epoch: 4

Epoch: 5

Epoch: 6

Epoch: 7

Epoch: 8

Epoch: 9

Epoch: 10
TRAIN Iter 10: lr = 0.000997,	loss = 0.005538,	Top-1 err = 95.300000,	Top-5 err = 82.050000,	train_time = 15.036781
TEST Iter 10: loss = 6.721215,	Top-1 err = 97.340000,	Top-5 err = 89.580000,	val_time = 18.040439

Epoch: 11

Epoch: 12

Epoch: 13

Epoch: 14

Epoch: 15

Epoch: 16

Epoch: 17

Epoch: 18

Epoch: 19

Epoch: 20
TRAIN Iter 20: lr = 0.000989,	loss = 0.004390,	Top-1 err = 92.000000,	Top-5 err = 72.400000,	train_time = 14.971047
TEST Iter 20: loss = 5.506616,	Top-1 err = 96.050000,	Top-5 err = 85.220000,	val_time = 17.976490

Epoch: 21

Epoch: 22

Epoch: 23

Epoch: 24

Epoch: 25

Epoch: 26

Epoch: 27

Epoch: 28

Epoch: 29

Epoch: 30
TRAIN Iter 30: lr = 0.000976,	loss = 0.003955,	Top-1 err = 89.550000,	Top-5 err = 69.050000,	train_time = 14.966051
TEST Iter 30: loss = 5.677187,	Top-1 err = 95.880000,	Top-5 err = 84.410000,	val_time = 17.736061

Epoch: 31

Epoch: 32

Epoch: 33

Epoch: 34

Epoch: 35

Epoch: 36

Epoch: 37

Epoch: 38

Epoch: 39

Epoch: 40
TRAIN Iter 40: lr = 0.000957,	loss = 0.003808,	Top-1 err = 82.150000,	Top-5 err = 56.650000,	train_time = 14.960629
TEST Iter 40: loss = 5.340979,	Top-1 err = 94.030000,	Top-5 err = 79.080000,	val_time = 17.780871

Epoch: 41

Epoch: 42

Epoch: 43

Epoch: 44

Epoch: 45

Epoch: 46

Epoch: 47

Epoch: 48

Epoch: 49

Epoch: 50
TRAIN Iter 50: lr = 0.000933,	loss = 0.003485,	Top-1 err = 74.400000,	Top-5 err = 43.300000,	train_time = 15.018930
TEST Iter 50: loss = 6.095872,	Top-1 err = 93.270000,	Top-5 err = 77.960000,	val_time = 17.844378

Epoch: 51

Epoch: 52

Epoch: 53

Epoch: 54

Epoch: 55

Epoch: 56

Epoch: 57

Epoch: 58

Epoch: 59

Epoch: 60
TRAIN Iter 60: lr = 0.000905,	loss = 0.003204,	Top-1 err = 70.100000,	Top-5 err = 40.350000,	train_time = 14.974768
TEST Iter 60: loss = 5.082737,	Top-1 err = 92.370000,	Top-5 err = 75.560000,	val_time = 17.851657

Epoch: 61

Epoch: 62

Epoch: 63

Epoch: 64

Epoch: 65

Epoch: 66

Epoch: 67

Epoch: 68

Epoch: 69

Epoch: 70
TRAIN Iter 70: lr = 0.000872,	loss = 0.003042,	Top-1 err = 67.700000,	Top-5 err = 42.950000,	train_time = 14.941287
TEST Iter 70: loss = 5.113256,	Top-1 err = 90.840000,	Top-5 err = 72.990000,	val_time = 17.776896

Epoch: 71

Epoch: 72

Epoch: 73

Epoch: 74

Epoch: 75

Epoch: 76

Epoch: 77

Epoch: 78

Epoch: 79

Epoch: 80
TRAIN Iter 80: lr = 0.000835,	loss = 0.002895,	Top-1 err = 61.150000,	Top-5 err = 32.200000,	train_time = 15.053045
TEST Iter 80: loss = 4.672278,	Top-1 err = 88.170000,	Top-5 err = 68.370000,	val_time = 17.819730

Epoch: 81

Epoch: 82

Epoch: 83

Epoch: 84

Epoch: 85

Epoch: 86

Epoch: 87

Epoch: 88

Epoch: 89

Epoch: 90
TRAIN Iter 90: lr = 0.000794,	loss = 0.002708,	Top-1 err = 56.500000,	Top-5 err = 29.000000,	train_time = 14.936167
TEST Iter 90: loss = 4.798963,	Top-1 err = 88.800000,	Top-5 err = 70.100000,	val_time = 17.850947

Epoch: 91

Epoch: 92

Epoch: 93

Epoch: 94

Epoch: 95

Epoch: 96

Epoch: 97

Epoch: 98

Epoch: 99

Epoch: 100
TRAIN Iter 100: lr = 0.000750,	loss = 0.002536,	Top-1 err = 53.050000,	Top-5 err = 26.550000,	train_time = 15.028377
TEST Iter 100: loss = 5.178642,	Top-1 err = 89.920000,	Top-5 err = 72.330000,	val_time = 17.951178

Epoch: 101

Epoch: 102

Epoch: 103

Epoch: 104

Epoch: 105

Epoch: 106

Epoch: 107

Epoch: 108

Epoch: 109

Epoch: 110
TRAIN Iter 110: lr = 0.000703,	loss = 0.002421,	Top-1 err = 53.550000,	Top-5 err = 26.900000,	train_time = 15.001969
TEST Iter 110: loss = 4.873221,	Top-1 err = 86.550000,	Top-5 err = 66.510000,	val_time = 17.765407

Epoch: 111

Epoch: 112

Epoch: 113

Epoch: 114

Epoch: 115

Epoch: 116

Epoch: 117

Epoch: 118

Epoch: 119

Epoch: 120
TRAIN Iter 120: lr = 0.000655,	loss = 0.002318,	Top-1 err = 61.000000,	Top-5 err = 38.300000,	train_time = 14.931024
TEST Iter 120: loss = 4.497174,	Top-1 err = 85.580000,	Top-5 err = 64.820000,	val_time = 17.760406

Epoch: 121

Epoch: 122

Epoch: 123

Epoch: 124

Epoch: 125

Epoch: 126

Epoch: 127

Epoch: 128

Epoch: 129

Epoch: 130
TRAIN Iter 130: lr = 0.000604,	loss = 0.002285,	Top-1 err = 53.850000,	Top-5 err = 32.400000,	train_time = 14.917884
TEST Iter 130: loss = 4.571485,	Top-1 err = 82.540000,	Top-5 err = 60.660000,	val_time = 17.805385

Epoch: 131

Epoch: 132

Epoch: 133

Epoch: 134

Epoch: 135

Epoch: 136

Epoch: 137

Epoch: 138

Epoch: 139

Epoch: 140
TRAIN Iter 140: lr = 0.000552,	loss = 0.002086,	Top-1 err = 52.650000,	Top-5 err = 33.450000,	train_time = 14.927704
TEST Iter 140: loss = 4.422925,	Top-1 err = 82.560000,	Top-5 err = 60.080000,	val_time = 17.802848

Epoch: 141

Epoch: 142

Epoch: 143

Epoch: 144

Epoch: 145

Epoch: 146

Epoch: 147

Epoch: 148

Epoch: 149

Epoch: 150
TRAIN Iter 150: lr = 0.000500,	loss = 0.002115,	Top-1 err = 41.700000,	Top-5 err = 20.150000,	train_time = 14.957724
TEST Iter 150: loss = 4.735413,	Top-1 err = 83.280000,	Top-5 err = 62.410000,	val_time = 17.904307

Epoch: 151

Epoch: 152

Epoch: 153

Epoch: 154

Epoch: 155

Epoch: 156

Epoch: 157

Epoch: 158

Epoch: 159

Epoch: 160
TRAIN Iter 160: lr = 0.000448,	loss = 0.001968,	Top-1 err = 50.600000,	Top-5 err = 35.350000,	train_time = 14.948608
TEST Iter 160: loss = 4.204233,	Top-1 err = 80.580000,	Top-5 err = 57.080000,	val_time = 18.086241

Epoch: 161

Epoch: 162

Epoch: 163

Epoch: 164

Epoch: 165

Epoch: 166

Epoch: 167

Epoch: 168

Epoch: 169

Epoch: 170
TRAIN Iter 170: lr = 0.000396,	loss = 0.001944,	Top-1 err = 47.100000,	Top-5 err = 30.200000,	train_time = 14.975431
TEST Iter 170: loss = 4.034347,	Top-1 err = 78.640000,	Top-5 err = 55.380000,	val_time = 17.799025

Epoch: 171

Epoch: 172

Epoch: 173

Epoch: 174

Epoch: 175

Epoch: 176

Epoch: 177

Epoch: 178

Epoch: 179

Epoch: 180
TRAIN Iter 180: lr = 0.000345,	loss = 0.001861,	Top-1 err = 44.300000,	Top-5 err = 27.250000,	train_time = 15.061748
TEST Iter 180: loss = 3.992799,	Top-1 err = 79.260000,	Top-5 err = 56.160000,	val_time = 17.783320

Epoch: 181

Epoch: 182

Epoch: 183

Epoch: 184

Epoch: 185

Epoch: 186

Epoch: 187

Epoch: 188

Epoch: 189

Epoch: 190
TRAIN Iter 190: lr = 0.000297,	loss = 0.001826,	Top-1 err = 47.800000,	Top-5 err = 28.750000,	train_time = 15.129629
TEST Iter 190: loss = 3.893066,	Top-1 err = 78.350000,	Top-5 err = 54.770000,	val_time = 18.091186

Epoch: 191

Epoch: 192

Epoch: 193

Epoch: 194

Epoch: 195

Epoch: 196

Epoch: 197

Epoch: 198

Epoch: 199

Epoch: 200
TRAIN Iter 200: lr = 0.000250,	loss = 0.001786,	Top-1 err = 37.900000,	Top-5 err = 18.450000,	train_time = 15.027933
TEST Iter 200: loss = 3.863212,	Top-1 err = 78.090000,	Top-5 err = 54.460000,	val_time = 18.155946

Epoch: 201

Epoch: 202

Epoch: 203

Epoch: 204

Epoch: 205

Epoch: 206

Epoch: 207

Epoch: 208

Epoch: 209

Epoch: 210
TRAIN Iter 210: lr = 0.000206,	loss = 0.001748,	Top-1 err = 53.150000,	Top-5 err = 33.500000,	train_time = 15.068416
TEST Iter 210: loss = 3.723594,	Top-1 err = 76.270000,	Top-5 err = 51.060000,	val_time = 17.937752

Epoch: 211

Epoch: 212

Epoch: 213

Epoch: 214

Epoch: 215

Epoch: 216

Epoch: 217

Epoch: 218

Epoch: 219

Epoch: 220
TRAIN Iter 220: lr = 0.000165,	loss = 0.001662,	Top-1 err = 45.500000,	Top-5 err = 28.200000,	train_time = 15.306318
TEST Iter 220: loss = 3.533062,	Top-1 err = 73.550000,	Top-5 err = 48.450000,	val_time = 18.289717

Epoch: 221

Epoch: 222

Epoch: 223

Epoch: 224

Epoch: 225

Epoch: 226

Epoch: 227

Epoch: 228

Epoch: 229

Epoch: 230
TRAIN Iter 230: lr = 0.000128,	loss = 0.001680,	Top-1 err = 58.500000,	Top-5 err = 39.650000,	train_time = 14.958700
TEST Iter 230: loss = 3.528588,	Top-1 err = 74.020000,	Top-5 err = 48.840000,	val_time = 17.767579

Epoch: 231

Epoch: 232

Epoch: 233

Epoch: 234

Epoch: 235

Epoch: 236

Epoch: 237

Epoch: 238

Epoch: 239

Epoch: 240
TRAIN Iter 240: lr = 0.000095,	loss = 0.001618,	Top-1 err = 47.650000,	Top-5 err = 31.200000,	train_time = 14.903306
TEST Iter 240: loss = 3.532740,	Top-1 err = 74.040000,	Top-5 err = 49.040000,	val_time = 17.873309

Epoch: 241

Epoch: 242

Epoch: 243

Epoch: 244

Epoch: 245

Epoch: 246

Epoch: 247

Epoch: 248

Epoch: 249

Epoch: 250
TRAIN Iter 250: lr = 0.000067,	loss = 0.001587,	Top-1 err = 34.550000,	Top-5 err = 20.350000,	train_time = 15.110433
TEST Iter 250: loss = 3.573839,	Top-1 err = 74.270000,	Top-5 err = 49.840000,	val_time = 17.974214

Epoch: 251

Epoch: 252

Epoch: 253

Epoch: 254

Epoch: 255

Epoch: 256

Epoch: 257

Epoch: 258

Epoch: 259

Epoch: 260
TRAIN Iter 260: lr = 0.000043,	loss = 0.001596,	Top-1 err = 58.300000,	Top-5 err = 41.250000,	train_time = 14.930107
TEST Iter 260: loss = 3.493905,	Top-1 err = 73.240000,	Top-5 err = 48.380000,	val_time = 17.842071

Epoch: 261

Epoch: 262

Epoch: 263

Epoch: 264

Epoch: 265

Epoch: 266

Epoch: 267

Epoch: 268

Epoch: 269

Epoch: 270
TRAIN Iter 270: lr = 0.000024,	loss = 0.001614,	Top-1 err = 55.750000,	Top-5 err = 37.650000,	train_time = 15.004214
TEST Iter 270: loss = 3.425536,	Top-1 err = 72.630000,	Top-5 err = 46.900000,	val_time = 17.740196

Epoch: 271

Epoch: 272

Epoch: 273

Epoch: 274

Epoch: 275

Epoch: 276

Epoch: 277

Epoch: 278

Epoch: 279

Epoch: 280
TRAIN Iter 280: lr = 0.000011,	loss = 0.001596,	Top-1 err = 41.350000,	Top-5 err = 23.650000,	train_time = 14.948832
TEST Iter 280: loss = 3.421004,	Top-1 err = 72.430000,	Top-5 err = 47.130000,	val_time = 17.828119

Epoch: 281

Epoch: 282

Epoch: 283

Epoch: 284

Epoch: 285

Epoch: 286

Epoch: 287

Epoch: 288

Epoch: 289

Epoch: 290
TRAIN Iter 290: lr = 0.000003,	loss = 0.001611,	Top-1 err = 43.350000,	Top-5 err = 26.150000,	train_time = 14.987080
TEST Iter 290: loss = 3.436723,	Top-1 err = 72.590000,	Top-5 err = 47.360000,	val_time = 17.877697

Epoch: 291

Epoch: 292

Epoch: 293

Epoch: 294

Epoch: 295

Epoch: 296

Epoch: 297

Epoch: 298

Epoch: 299
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.004 MB of 0.029 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.029 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:  train/Top1 ▁▁▁▂▂▃▃▄▄▄▄▄▄▄▆▆▅▆▇▆▆▇▆▇▆▇▆▇█▇▇▇▇▆█▇▇▇█▆
wandb:  train/Top5 ▁▁▂▃▄▄▄▅▅▅▅▅▅▅▇▆▆▇▇▇▇▇▇▇▆▇▆▇█▇▇▇▇▇█▇▇▇▇▇
wandb: train/epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  train/loss █▇▆▆▅▅▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    train/lr ███████▇▇▇▇▇▇▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁
wandb:   val/epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███
wandb:    val/loss █▇▅▅▄▆▄▄▃▃▄▄▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁
wandb:    val/top1 ▁▁▂▂▂▃▃▃▄▄▃▄▅▅▅▅▆▆▆▆▇▇█████████
wandb:    val/top5 ▁▂▂▃▃▃▄▄▅▅▄▅▅▆▆▆▇▇▇▇▇▇█████████
wandb: 
wandb: Run summary:
wandb:  train/Top1 52.8
wandb:  train/Top5 69.9
wandb: train/epoch 299
wandb:  train/loss 0.0016
wandb:    train/lr 0.0
wandb:   val/epoch 299
wandb:    val/loss 3.43556
wandb:    val/top1 27.44
wandb:    val/top5 52.73
wandb: 
wandb: 🚀 View run cerulean-water-405 at: https://wandb.ai/hl57/final_rn18_fkd/runs/xr8zkkd0
wandb: ️⚡ View job at https://wandb.ai/hl57/final_rn18_fkd/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjk3MzkyNjU3/version_details/v37
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231007_145721-xr8zkkd0/logs
TEST Iter 299: loss = 3.435557,	Top-1 err = 72.560000,	Top-5 err = 47.270000,	val_time = 17.790078
