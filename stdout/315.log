r_bn:  30.0
lr:  0.1
bc loaded
bc shape (200, 1, 512)
getting batchnorm for class 0
getting batchnorm for class 1
getting batchnorm for class 2
getting batchnorm for class 3
getting batchnorm for class 4
getting batchnorm for class 5
getting batchnorm for class 6
getting batchnorm for class 7
getting batchnorm for class 8
getting batchnorm for class 9
getting batchnorm for class 10
getting batchnorm for class 11
getting batchnorm for class 12
getting batchnorm for class 13
getting batchnorm for class 14
getting batchnorm for class 15
getting batchnorm for class 16
getting batchnorm for class 17
getting batchnorm for class 18
getting batchnorm for class 19
getting batchnorm for class 20
getting batchnorm for class 21
getting batchnorm for class 22
getting batchnorm for class 23
getting batchnorm for class 24
getting batchnorm for class 25
getting batchnorm for class 26
getting batchnorm for class 27
getting batchnorm for class 28
getting batchnorm for class 29
getting batchnorm for class 30
getting batchnorm for class 31
getting batchnorm for class 32
getting batchnorm for class 33
getting batchnorm for class 34
getting batchnorm for class 35
getting batchnorm for class 36
getting batchnorm for class 37
getting batchnorm for class 38
getting batchnorm for class 39
getting batchnorm for class 40
getting batchnorm for class 41
getting batchnorm for class 42
getting batchnorm for class 43
getting batchnorm for class 44
getting batchnorm for class 45
getting batchnorm for class 46
getting batchnorm for class 47
getting batchnorm for class 48
getting batchnorm for class 49
getting batchnorm for class 50
getting batchnorm for class 51
getting batchnorm for class 52
getting batchnorm for class 53
getting batchnorm for class 54
getting batchnorm for class 55
getting batchnorm for class 56
getting batchnorm for class 57
getting batchnorm for class 58
getting batchnorm for class 59
getting batchnorm for class 60
getting batchnorm for class 61
getting batchnorm for class 62
getting batchnorm for class 63
getting batchnorm for class 64
getting batchnorm for class 65
getting batchnorm for class 66
getting batchnorm for class 67
getting batchnorm for class 68
getting batchnorm for class 69
getting batchnorm for class 70
getting batchnorm for class 71
getting batchnorm for class 72
getting batchnorm for class 73
getting batchnorm for class 74
getting batchnorm for class 75
getting batchnorm for class 76
getting batchnorm for class 77
getting batchnorm for class 78
getting batchnorm for class 79
getting batchnorm for class 80
getting batchnorm for class 81
getting batchnorm for class 82
getting batchnorm for class 83
getting batchnorm for class 84
getting batchnorm for class 85
getting batchnorm for class 86
getting batchnorm for class 87
getting batchnorm for class 88
getting batchnorm for class 89
getting batchnorm for class 90
getting batchnorm for class 91
getting batchnorm for class 92
getting batchnorm for class 93
getting batchnorm for class 94
getting batchnorm for class 95
getting batchnorm for class 96
getting batchnorm for class 97
getting batchnorm for class 98
getting batchnorm for class 99
getting batchnorm for class 100
getting batchnorm for class 101
getting batchnorm for class 102
getting batchnorm for class 103
getting batchnorm for class 104
getting batchnorm for class 105
getting batchnorm for class 106
getting batchnorm for class 107
getting batchnorm for class 108
getting batchnorm for class 109
getting batchnorm for class 110
getting batchnorm for class 111
getting batchnorm for class 112
getting batchnorm for class 113
getting batchnorm for class 114
getting batchnorm for class 115
getting batchnorm for class 116
getting batchnorm for class 117
getting batchnorm for class 118
getting batchnorm for class 119
getting batchnorm for class 120
getting batchnorm for class 121
getting batchnorm for class 122
getting batchnorm for class 123
getting batchnorm for class 124
getting batchnorm for class 125
getting batchnorm for class 126
getting batchnorm for class 127
getting batchnorm for class 128
getting batchnorm for class 129
getting batchnorm for class 130
getting batchnorm for class 131
getting batchnorm for class 132
getting batchnorm for class 133
getting batchnorm for class 134
getting batchnorm for class 135
getting batchnorm for class 136
getting batchnorm for class 137
getting batchnorm for class 138
getting batchnorm for class 139
getting batchnorm for class 140
getting batchnorm for class 141
getting batchnorm for class 142
getting batchnorm for class 143
getting batchnorm for class 144
getting batchnorm for class 145
getting batchnorm for class 146
getting batchnorm for class 147
getting batchnorm for class 148
getting batchnorm for class 149
getting batchnorm for class 150
getting batchnorm for class 151
getting batchnorm for class 152
getting batchnorm for class 153
getting batchnorm for class 154
getting batchnorm for class 155
getting batchnorm for class 156
getting batchnorm for class 157
getting batchnorm for class 158
getting batchnorm for class 159
getting batchnorm for class 160
getting batchnorm for class 161
getting batchnorm for class 162
getting batchnorm for class 163
getting batchnorm for class 164
getting batchnorm for class 165
getting batchnorm for class 166
getting batchnorm for class 167
getting batchnorm for class 168
getting batchnorm for class 169
getting batchnorm for class 170
getting batchnorm for class 171
getting batchnorm for class 172
getting batchnorm for class 173
getting batchnorm for class 174
getting batchnorm for class 175
getting batchnorm for class 176
getting batchnorm for class 177
getting batchnorm for class 178
getting batchnorm for class 179
getting batchnorm for class 180
getting batchnorm for class 181
getting batchnorm for class 182
getting batchnorm for class 183
getting batchnorm for class 184
getting batchnorm for class 185
getting batchnorm for class 186
getting batchnorm for class 187
getting batchnorm for class 188
getting batchnorm for class 189
getting batchnorm for class 190
getting batchnorm for class 191
getting batchnorm for class 192
getting batchnorm for class 193
getting batchnorm for class 194
getting batchnorm for class 195
getting batchnorm for class 196
getting batchnorm for class 197
getting batchnorm for class 198
getting batchnorm for class 199
/home/techt/anaconda3/envs/hl/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/techt/anaconda3/envs/hl/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
/home/techt/anaconda3/envs/hl/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
Execution time for computing per-class batchnorm statistics: 410.069108 seconds
get_images call
class_per_batch  100 batch_size  100 args.ipc  1
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 0 end_cls 100
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 12774.896309635402
main criterion 126.71564557290142
weighted_aux_loss 12648.1806640625
loss_r_bn_feature 421.60601806640625
------------iteration 100----------
total loss 4209.543683945637
main criterion 54.915754258136765
weighted_aux_loss 4154.6279296875
loss_r_bn_feature 138.4875946044922
------------iteration 200----------
total loss 2854.3112181555025
main criterion 50.23773182737771
weighted_aux_loss 2804.073486328125
loss_r_bn_feature 93.4691162109375
------------iteration 300----------
total loss 2276.170364024108
main criterion 48.67573511785819
weighted_aux_loss 2227.49462890625
loss_r_bn_feature 74.24981689453125
------------iteration 400----------
total loss 2612.993406673513
main criterion 53.94750823601332
weighted_aux_loss 2559.0458984375
loss_r_bn_feature 85.30152893066406
------------iteration 500----------
total loss 2161.066232860145
main criterion 45.47126215701998
weighted_aux_loss 2115.594970703125
loss_r_bn_feature 70.51982879638672
------------iteration 600----------
total loss 2255.275892041106
main criterion 44.13795258798126
weighted_aux_loss 2211.137939453125
loss_r_bn_feature 73.70459747314453
------------iteration 700----------
total loss 1698.954010609054
main criterion 41.33071959342902
weighted_aux_loss 1657.623291015625
loss_r_bn_feature 55.25410842895508
------------iteration 800----------
total loss 1766.3683243633645
main criterion 40.60721596492708
weighted_aux_loss 1725.7611083984375
loss_r_bn_feature 57.52537155151367
------------iteration 900----------
total loss 3662.621332305057
main criterion 62.59740652380694
weighted_aux_loss 3600.02392578125
loss_r_bn_feature 120.00079345703125
------------iteration 1000----------
total loss 4346.44981314656
main criterion 70.32432486530952
weighted_aux_loss 4276.12548828125
loss_r_bn_feature 142.5375213623047
------------iteration 1100----------
total loss 2292.898083817038
main criterion 49.55653108266263
weighted_aux_loss 2243.341552734375
loss_r_bn_feature 74.7780532836914
------------iteration 1200----------
total loss 1424.6265996064799
main criterion 37.45313769241747
weighted_aux_loss 1387.1734619140625
loss_r_bn_feature 46.23911666870117
------------iteration 1300----------
total loss 1744.7855882418103
main criterion 43.98383042931023
weighted_aux_loss 1700.8017578125
loss_r_bn_feature 56.69339370727539
------------iteration 1400----------
total loss 1904.3743423436458
main criterion 48.58759917958334
weighted_aux_loss 1855.7867431640625
loss_r_bn_feature 61.85955810546875
------------iteration 1500----------
total loss 1320.5954920208922
main criterion 37.49295295839207
weighted_aux_loss 1283.1025390625
loss_r_bn_feature 42.770084381103516
------------iteration 1600----------
total loss 1606.6947268156757
main criterion 41.61904322192579
weighted_aux_loss 1565.07568359375
loss_r_bn_feature 52.169189453125
------------iteration 1700----------
total loss 1323.6941347280972
main criterion 37.00138570465962
weighted_aux_loss 1286.6927490234375
loss_r_bn_feature 42.8897590637207
------------iteration 1800----------
total loss 2531.6599297470616
main criterion 54.19215630956178
weighted_aux_loss 2477.4677734375
loss_r_bn_feature 82.58226013183594
------------iteration 1900----------
total loss 5153.606323239382
main criterion 68.19421386438138
weighted_aux_loss 5085.412109375
loss_r_bn_feature 169.51373291015625
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 100 end_cls 200
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 13903.18004243322
main criterion 126.4515268082196
weighted_aux_loss 13776.728515625
loss_r_bn_feature 459.2242736816406
------------iteration 100----------
total loss 5075.076803859227
main criterion 59.59877651547693
weighted_aux_loss 5015.47802734375
loss_r_bn_feature 167.18260192871094
------------iteration 200----------
total loss 6348.498326219642
main criterion 92.92117778214222
weighted_aux_loss 6255.5771484375
loss_r_bn_feature 208.5192413330078
------------iteration 300----------
total loss 2748.9611828569514
main criterion 53.278077388201304
weighted_aux_loss 2695.68310546875
loss_r_bn_feature 89.8561019897461
------------iteration 400----------
total loss 2555.4733522783754
main criterion 44.973596419000486
weighted_aux_loss 2510.499755859375
loss_r_bn_feature 83.6833267211914
------------iteration 500----------
total loss 2543.173472106361
main criterion 51.58606976261081
weighted_aux_loss 2491.58740234375
loss_r_bn_feature 83.05290985107422
------------iteration 600----------
total loss 3007.435248125272
main criterion 58.28241609402213
weighted_aux_loss 2949.15283203125
loss_r_bn_feature 98.30509185791016
------------iteration 700----------
total loss 1854.8499745549761
main criterion 43.7379140081012
weighted_aux_loss 1811.112060546875
loss_r_bn_feature 60.37040328979492
------------iteration 800----------
total loss 2140.6861086155236
main criterion 46.819165256148594
weighted_aux_loss 2093.866943359375
loss_r_bn_feature 69.79556274414062
------------iteration 900----------
total loss 1817.627320682047
main criterion 41.42895642423456
weighted_aux_loss 1776.1983642578125
loss_r_bn_feature 59.20661163330078
------------iteration 1000----------
total loss 2287.5802483290463
main criterion 49.134691688421206
weighted_aux_loss 2238.445556640625
loss_r_bn_feature 74.61485290527344
------------iteration 1100----------
total loss 1613.0947150707277
main criterion 38.500965070727794
weighted_aux_loss 1574.59375
loss_r_bn_feature 52.48645782470703
------------iteration 1200----------
total loss 2405.8933220490535
main criterion 49.83424001780328
weighted_aux_loss 2356.05908203125
loss_r_bn_feature 78.5353012084961
------------iteration 1300----------
total loss 1780.3117947868504
main criterion 44.80276158372549
weighted_aux_loss 1735.509033203125
loss_r_bn_feature 57.85029983520508
------------iteration 1400----------
total loss 1839.9950479453094
main criterion 42.01653232030944
weighted_aux_loss 1797.978515625
loss_r_bn_feature 59.9326171875
------------iteration 1500----------
total loss 1396.5019688911268
main criterion 38.88160756300187
weighted_aux_loss 1357.620361328125
loss_r_bn_feature 45.25401306152344
------------iteration 1600----------
total loss 1971.7345091039433
main criterion 46.505505197693296
weighted_aux_loss 1925.22900390625
loss_r_bn_feature 64.17430114746094
------------iteration 1700----------
total loss 1442.980446834385
main criterion 37.25498296719743
weighted_aux_loss 1405.7254638671875
loss_r_bn_feature 46.85751724243164
------------iteration 1800----------
total loss 1344.1450438855177
main criterion 36.49392083864271
weighted_aux_loss 1307.651123046875
loss_r_bn_feature 43.58837127685547
------------iteration 1900----------
total loss 1276.7487054048934
main criterion 36.21330501426827
weighted_aux_loss 1240.535400390625
loss_r_bn_feature 41.35118103027344
=> using pytorch pre-trained model 'resnet18'
process data from ../recover/syn_data/tiny-imagenet/315
  0%|          | 0/300 [00:00<?, ?it/s]  0%|          | 1/300 [00:02<12:11,  2.45s/it]  1%|          | 2/300 [00:03<07:15,  1.46s/it]  1%|          | 3/300 [00:03<05:39,  1.14s/it]  1%|▏         | 4/300 [00:04<04:57,  1.00s/it]  2%|▏         | 5/300 [00:05<04:33,  1.08it/s]  2%|▏         | 6/300 [00:06<04:15,  1.15it/s]  2%|▏         | 7/300 [00:07<04:05,  1.19it/s]  3%|▎         | 8/300 [00:07<03:58,  1.22it/s]  3%|▎         | 9/300 [00:08<03:55,  1.24it/s]  3%|▎         | 10/300 [00:09<03:54,  1.24it/s]  4%|▎         | 11/300 [00:10<03:53,  1.24it/s]  4%|▍         | 12/300 [00:11<03:51,  1.24it/s]  4%|▍         | 13/300 [00:11<03:51,  1.24it/s]  5%|▍         | 14/300 [00:12<03:48,  1.25it/s]  5%|▌         | 15/300 [00:13<03:47,  1.25it/s]  5%|▌         | 16/300 [00:14<03:47,  1.25it/s]  6%|▌         | 17/300 [00:15<03:45,  1.26it/s]  6%|▌         | 18/300 [00:15<03:45,  1.25it/s]  6%|▋         | 19/300 [00:16<03:42,  1.26it/s]  7%|▋         | 20/300 [00:17<03:41,  1.26it/s]  7%|▋         | 21/300 [00:18<03:41,  1.26it/s]  7%|▋         | 22/300 [00:19<03:41,  1.26it/s]  8%|▊         | 23/300 [00:19<03:39,  1.26it/s]  8%|▊         | 24/300 [00:20<03:40,  1.25it/s]  8%|▊         | 25/300 [00:21<03:40,  1.25it/s]  9%|▊         | 26/300 [00:22<03:39,  1.25it/s]  9%|▉         | 27/300 [00:23<03:36,  1.26it/s]  9%|▉         | 28/300 [00:23<03:37,  1.25it/s] 10%|▉         | 29/300 [00:24<03:34,  1.26it/s] 10%|█         | 30/300 [00:25<03:34,  1.26it/s] 10%|█         | 31/300 [00:26<03:37,  1.24it/s] 11%|█         | 32/300 [00:27<03:36,  1.24it/s] 11%|█         | 33/300 [00:27<03:33,  1.25it/s] 11%|█▏        | 34/300 [00:28<03:35,  1.24it/s] 12%|█▏        | 35/300 [00:29<03:34,  1.24it/s] 12%|█▏        | 36/300 [00:30<03:31,  1.25it/s] 12%|█▏        | 37/300 [00:31<03:29,  1.26it/s] 13%|█▎        | 38/300 [00:31<03:29,  1.25it/s] 13%|█▎        | 39/300 [00:32<03:30,  1.24it/s] 13%|█▎        | 40/300 [00:33<03:26,  1.26it/s] 14%|█▎        | 41/300 [00:34<03:30,  1.23it/s] 14%|█▍        | 42/300 [00:35<03:29,  1.23it/s] 14%|█▍        | 43/300 [00:35<03:25,  1.25it/s] 15%|█▍        | 44/300 [00:36<03:24,  1.25it/s] 15%|█▌        | 45/300 [00:37<03:23,  1.25it/s] 15%|█▌        | 46/300 [00:38<03:22,  1.25it/s] 16%|█▌        | 47/300 [00:39<03:22,  1.25it/s] 16%|█▌        | 48/300 [00:39<03:22,  1.25it/s] 16%|█▋        | 49/300 [00:40<03:21,  1.25it/s] 17%|█▋        | 50/300 [00:41<03:21,  1.24it/s] 17%|█▋        | 51/300 [00:42<03:23,  1.23it/s] 17%|█▋        | 52/300 [00:43<03:19,  1.24it/s] 18%|█▊        | 53/300 [00:43<03:18,  1.24it/s] 18%|█▊        | 54/300 [00:44<03:18,  1.24it/s] 18%|█▊        | 55/300 [00:45<03:17,  1.24it/s] 19%|█▊        | 56/300 [00:46<03:14,  1.25it/s] 19%|█▉        | 57/300 [00:47<03:14,  1.25it/s] 19%|█▉        | 58/300 [00:47<03:10,  1.27it/s] 20%|█▉        | 59/300 [00:48<03:09,  1.27it/s] 20%|██        | 60/300 [00:49<03:08,  1.27it/s] 20%|██        | 61/300 [00:50<03:06,  1.28it/s] 21%|██        | 62/300 [00:50<03:06,  1.28it/s] 21%|██        | 63/300 [00:51<03:05,  1.28it/s] 21%|██▏       | 64/300 [00:52<03:05,  1.27it/s] 22%|██▏       | 65/300 [00:53<03:02,  1.29it/s] 22%|██▏       | 66/300 [00:54<03:02,  1.28it/s] 22%|██▏       | 67/300 [00:54<03:00,  1.29it/s] 23%|██▎       | 68/300 [00:55<03:01,  1.28it/s] 23%|██▎       | 69/300 [00:56<02:59,  1.29it/s] 23%|██▎       | 70/300 [00:57<02:58,  1.29it/s] 24%|██▎       | 71/300 [00:58<03:00,  1.27it/s] 24%|██▍       | 72/300 [00:58<02:59,  1.27it/s] 24%|██▍       | 73/300 [00:59<02:58,  1.27it/s] 25%|██▍       | 74/300 [01:00<02:57,  1.27it/s] 25%|██▌       | 75/300 [01:01<02:56,  1.27it/s] 25%|██▌       | 76/300 [01:01<02:58,  1.25it/s] 26%|██▌       | 77/300 [01:02<02:58,  1.25it/s] 26%|██▌       | 78/300 [01:03<02:57,  1.25it/s] 26%|██▋       | 79/300 [01:04<02:57,  1.24it/s] 27%|██▋       | 80/300 [01:05<02:56,  1.24it/s] 27%|██▋       | 81/300 [01:05<02:55,  1.25it/s] 27%|██▋       | 82/300 [01:06<02:53,  1.26it/s] 28%|██▊       | 83/300 [01:07<02:53,  1.25it/s] 28%|██▊       | 84/300 [01:08<02:52,  1.25it/s] 28%|██▊       | 85/300 [01:09<02:50,  1.26it/s] 29%|██▊       | 86/300 [01:09<02:51,  1.25it/s] 29%|██▉       | 87/300 [01:10<02:51,  1.24it/s] 29%|██▉       | 88/300 [01:11<02:49,  1.25it/s] 30%|██▉       | 89/300 [01:12<02:47,  1.26it/s] 30%|███       | 90/300 [01:13<02:45,  1.27it/s] 30%|███       | 91/300 [01:13<02:46,  1.25it/s] 31%|███       | 92/300 [01:14<02:46,  1.25it/s] 31%|███       | 93/300 [01:15<02:45,  1.25it/s] 31%|███▏      | 94/300 [01:16<02:44,  1.25it/s] 32%|███▏      | 95/300 [01:17<02:42,  1.26it/s] 32%|███▏      | 96/300 [01:17<02:42,  1.26it/s] 32%|███▏      | 97/300 [01:18<02:40,  1.27it/s] 33%|███▎      | 98/300 [01:19<02:38,  1.27it/s] 33%|███▎      | 99/300 [01:20<02:40,  1.25it/s] 33%|███▎      | 100/300 [01:21<02:39,  1.25it/s] 34%|███▎      | 101/300 [01:21<02:38,  1.26it/s] 34%|███▍      | 102/300 [01:22<02:38,  1.25it/s] 34%|███▍      | 103/300 [01:23<02:38,  1.24it/s] 35%|███▍      | 104/300 [01:24<02:38,  1.24it/s] 35%|███▌      | 105/300 [01:25<02:38,  1.23it/s] 35%|███▌      | 106/300 [01:25<02:37,  1.23it/s] 36%|███▌      | 107/300 [01:26<02:37,  1.23it/s] 36%|███▌      | 108/300 [01:27<02:34,  1.24it/s] 36%|███▋      | 109/300 [01:28<02:33,  1.24it/s] 37%|███▋      | 110/300 [01:29<02:33,  1.24it/s] 37%|███▋      | 111/300 [01:30<02:34,  1.22it/s] 37%|███▋      | 112/300 [01:30<02:32,  1.23it/s] 38%|███▊      | 113/300 [01:31<02:31,  1.23it/s] 38%|███▊      | 114/300 [01:32<02:29,  1.24it/s] 38%|███▊      | 115/300 [01:33<02:29,  1.24it/s] 39%|███▊      | 116/300 [01:34<02:29,  1.23it/s] 39%|███▉      | 117/300 [01:34<02:27,  1.24it/s] 39%|███▉      | 118/300 [01:35<02:25,  1.25it/s] 40%|███▉      | 119/300 [01:36<02:25,  1.24it/s] 40%|████      | 120/300 [01:37<02:24,  1.25it/s] 40%|████      | 121/300 [01:38<02:22,  1.25it/s] 41%|████      | 122/300 [01:38<02:22,  1.25it/s] 41%|████      | 123/300 [01:39<02:22,  1.24it/s] 41%|████▏     | 124/300 [01:40<02:19,  1.26it/s] 42%|████▏     | 125/300 [01:41<02:18,  1.26it/s] 42%|████▏     | 126/300 [01:42<02:18,  1.26it/s] 42%|████▏     | 127/300 [01:42<02:16,  1.26it/s] 43%|████▎     | 128/300 [01:43<02:15,  1.27it/s] 43%|████▎     | 129/300 [01:44<02:15,  1.27it/s] 43%|████▎     | 130/300 [01:45<02:15,  1.26it/s] 44%|████▎     | 131/300 [01:45<02:14,  1.26it/s] 44%|████▍     | 132/300 [01:46<02:12,  1.27it/s] 44%|████▍     | 133/300 [01:47<02:11,  1.27it/s] 45%|████▍     | 134/300 [01:48<02:12,  1.26it/s] 45%|████▌     | 135/300 [01:49<02:09,  1.27it/s] 45%|████▌     | 136/300 [01:49<02:09,  1.27it/s] 46%|████▌     | 137/300 [01:50<02:08,  1.27it/s] 46%|████▌     | 138/300 [01:51<02:08,  1.26it/s] 46%|████▋     | 139/300 [01:52<02:07,  1.27it/s] 47%|████▋     | 140/300 [01:53<02:06,  1.27it/s] 47%|████▋     | 141/300 [01:53<02:05,  1.27it/s] 47%|████▋     | 142/300 [01:54<02:05,  1.26it/s] 48%|████▊     | 143/300 [01:55<02:04,  1.26it/s] 48%|████▊     | 144/300 [01:56<02:04,  1.25it/s] 48%|████▊     | 145/300 [01:57<02:02,  1.26it/s] 49%|████▊     | 146/300 [01:57<02:02,  1.26it/s] 49%|████▉     | 147/300 [01:58<02:01,  1.26it/s] 49%|████▉     | 148/300 [01:59<02:00,  1.26it/s] 50%|████▉     | 149/300 [02:00<01:59,  1.26it/s] 50%|█████     | 150/300 [02:01<01:59,  1.25it/s] 50%|█████     | 151/300 [02:01<01:59,  1.25it/s] 51%|█████     | 152/300 [02:02<01:57,  1.26it/s] 51%|█████     | 153/300 [02:03<01:56,  1.26it/s] 51%|█████▏    | 154/300 [02:04<01:55,  1.26it/s] 52%|█████▏    | 155/300 [02:05<01:56,  1.25it/s] 52%|█████▏    | 156/300 [02:05<01:55,  1.25it/s] 52%|█████▏    | 157/300 [02:06<01:54,  1.25it/s] 53%|█████▎    | 158/300 [02:07<01:53,  1.25it/s] 53%|█████▎    | 159/300 [02:08<01:53,  1.24it/s] 53%|█████▎    | 160/300 [02:09<01:51,  1.26it/s] 54%|█████▎    | 161/300 [02:09<01:50,  1.26it/s] 54%|█████▍    | 162/300 [02:10<01:49,  1.26it/s] 54%|█████▍    | 163/300 [02:11<01:48,  1.26it/s] 55%|█████▍    | 164/300 [02:12<01:48,  1.26it/s] 55%|█████▌    | 165/300 [02:12<01:47,  1.26it/s] 55%|█████▌    | 166/300 [02:13<01:46,  1.26it/s] 56%|█████▌    | 167/300 [02:14<01:45,  1.27it/s] 56%|█████▌    | 168/300 [02:15<01:43,  1.27it/s] 56%|█████▋    | 169/300 [02:16<01:44,  1.26it/s] 57%|█████▋    | 170/300 [02:16<01:43,  1.26it/s] 57%|█████▋    | 171/300 [02:17<01:42,  1.26it/s] 57%|█████▋    | 172/300 [02:18<01:41,  1.26it/s] 58%|█████▊    | 173/300 [02:19<01:40,  1.27it/s] 58%|█████▊    | 174/300 [02:20<01:40,  1.26it/s] 58%|█████▊    | 175/300 [02:20<01:39,  1.25it/s] 59%|█████▊    | 176/300 [02:21<01:39,  1.24it/s] 59%|█████▉    | 177/300 [02:22<01:38,  1.25it/s] 59%|█████▉    | 178/300 [02:23<01:37,  1.26it/s] 60%|█████▉    | 179/300 [02:24<01:36,  1.26it/s] 60%|██████    | 180/300 [02:24<01:34,  1.26it/s] 60%|██████    | 181/300 [02:25<01:35,  1.25it/s] 61%|██████    | 182/300 [02:26<01:34,  1.25it/s] 61%|██████    | 183/300 [02:27<01:32,  1.26it/s] 61%|██████▏   | 184/300 [02:28<01:31,  1.26it/s] 62%|██████▏   | 185/300 [02:28<01:30,  1.27it/s] 62%|██████▏   | 186/300 [02:29<01:31,  1.25it/s] 62%|██████▏   | 187/300 [02:30<01:29,  1.26it/s] 63%|██████▎   | 188/300 [02:31<01:28,  1.27it/s] 63%|██████▎   | 189/300 [02:32<01:27,  1.27it/s] 63%|██████▎   | 190/300 [02:32<01:26,  1.27it/s] 64%|██████▎   | 191/300 [02:33<01:26,  1.27it/s] 64%|██████▍   | 192/300 [02:34<01:25,  1.27it/s] 64%|██████▍   | 193/300 [02:35<01:25,  1.25it/s] 65%|██████▍   | 194/300 [02:35<01:23,  1.27it/s] 65%|██████▌   | 195/300 [02:36<01:22,  1.27it/s] 65%|██████▌   | 196/300 [02:37<01:21,  1.27it/s] 66%|██████▌   | 197/300 [02:38<01:20,  1.27it/s] 66%|██████▌   | 198/300 [02:39<01:20,  1.27it/s] 66%|██████▋   | 199/300 [02:39<01:19,  1.28it/s] 67%|██████▋   | 200/300 [02:40<01:18,  1.27it/s] 67%|██████▋   | 201/300 [02:41<01:18,  1.27it/s] 67%|██████▋   | 202/300 [02:42<01:17,  1.27it/s] 68%|██████▊   | 203/300 [02:43<01:17,  1.26it/s] 68%|██████▊   | 204/300 [02:43<01:16,  1.26it/s] 68%|██████▊   | 205/300 [02:44<01:15,  1.26it/s] 69%|██████▊   | 206/300 [02:45<01:14,  1.27it/s] 69%|██████▉   | 207/300 [02:46<01:13,  1.26it/s] 69%|██████▉   | 208/300 [02:47<01:13,  1.25it/s] 70%|██████▉   | 209/300 [02:47<01:12,  1.26it/s] 70%|███████   | 210/300 [02:48<01:11,  1.26it/s] 70%|███████   | 211/300 [02:49<01:10,  1.26it/s] 71%|███████   | 212/300 [02:50<01:09,  1.26it/s] 71%|███████   | 213/300 [02:50<01:08,  1.27it/s] 71%|███████▏  | 214/300 [02:51<01:08,  1.26it/s] 72%|███████▏  | 215/300 [02:52<01:07,  1.26it/s] 72%|███████▏  | 216/300 [02:53<01:07,  1.25it/s] 72%|███████▏  | 217/300 [02:54<01:05,  1.26it/s] 73%|███████▎  | 218/300 [02:55<01:05,  1.25it/s] 73%|███████▎  | 219/300 [02:55<01:04,  1.26it/s] 73%|███████▎  | 220/300 [02:56<01:05,  1.23it/s] 74%|███████▎  | 221/300 [02:57<01:03,  1.24it/s] 74%|███████▍  | 222/300 [02:58<01:03,  1.23it/s] 74%|███████▍  | 223/300 [02:59<01:02,  1.23it/s] 75%|███████▍  | 224/300 [02:59<01:01,  1.24it/s] 75%|███████▌  | 225/300 [03:00<01:00,  1.24it/s] 75%|███████▌  | 226/300 [03:01<00:58,  1.26it/s] 76%|███████▌  | 227/300 [03:02<00:57,  1.26it/s] 76%|███████▌  | 228/300 [03:03<00:56,  1.27it/s] 76%|███████▋  | 229/300 [03:03<00:56,  1.26it/s] 77%|███████▋  | 230/300 [03:04<00:55,  1.26it/s] 77%|███████▋  | 231/300 [03:05<00:54,  1.26it/s] 77%|███████▋  | 232/300 [03:06<00:54,  1.26it/s] 78%|███████▊  | 233/300 [03:07<00:53,  1.25it/s] 78%|███████▊  | 234/300 [03:07<00:52,  1.25it/s] 78%|███████▊  | 235/300 [03:08<00:51,  1.26it/s] 79%|███████▊  | 236/300 [03:09<00:50,  1.26it/s] 79%|███████▉  | 237/300 [03:10<00:49,  1.26it/s] 79%|███████▉  | 238/300 [03:10<00:49,  1.26it/s] 80%|███████▉  | 239/300 [03:11<00:48,  1.26it/s] 80%|████████  | 240/300 [03:12<00:47,  1.25it/s] 80%|████████  | 241/300 [03:13<00:46,  1.26it/s] 81%|████████  | 242/300 [03:14<00:46,  1.26it/s] 81%|████████  | 243/300 [03:14<00:45,  1.26it/s] 81%|████████▏ | 244/300 [03:15<00:44,  1.25it/s] 82%|████████▏ | 245/300 [03:16<00:44,  1.25it/s] 82%|████████▏ | 246/300 [03:17<00:43,  1.24it/s] 82%|████████▏ | 247/300 [03:18<00:42,  1.26it/s] 83%|████████▎ | 248/300 [03:18<00:40,  1.27it/s] 83%|████████▎ | 249/300 [03:19<00:40,  1.27it/s] 83%|████████▎ | 250/300 [03:20<00:39,  1.27it/s] 84%|████████▎ | 251/300 [03:21<00:38,  1.27it/s] 84%|████████▍ | 252/300 [03:22<00:37,  1.28it/s] 84%|████████▍ | 253/300 [03:22<00:37,  1.27it/s] 85%|████████▍ | 254/300 [03:23<00:36,  1.27it/s] 85%|████████▌ | 255/300 [03:24<00:35,  1.29it/s] 85%|████████▌ | 256/300 [03:25<00:34,  1.29it/s] 86%|████████▌ | 257/300 [03:25<00:33,  1.28it/s] 86%|████████▌ | 258/300 [03:26<00:32,  1.28it/s] 86%|████████▋ | 259/300 [03:27<00:31,  1.29it/s] 87%|████████▋ | 260/300 [03:28<00:31,  1.29it/s] 87%|████████▋ | 261/300 [03:29<00:30,  1.30it/s] 87%|████████▋ | 262/300 [03:29<00:29,  1.28it/s] 88%|████████▊ | 263/300 [03:30<00:28,  1.28it/s] 88%|████████▊ | 264/300 [03:31<00:27,  1.29it/s] 88%|████████▊ | 265/300 [03:32<00:27,  1.29it/s] 89%|████████▊ | 266/300 [03:32<00:26,  1.29it/s] 89%|████████▉ | 267/300 [03:33<00:25,  1.28it/s] 89%|████████▉ | 268/300 [03:34<00:25,  1.28it/s] 90%|████████▉ | 269/300 [03:35<00:24,  1.29it/s] 90%|█████████ | 270/300 [03:36<00:23,  1.28it/s] 90%|█████████ | 271/300 [03:36<00:22,  1.29it/s] 91%|█████████ | 272/300 [03:37<00:21,  1.29it/s] 91%|█████████ | 273/300 [03:38<00:20,  1.30it/s] 91%|█████████▏| 274/300 [03:39<00:19,  1.31it/s] 92%|█████████▏| 275/300 [03:39<00:19,  1.28it/s] 92%|█████████▏| 276/300 [03:40<00:18,  1.27it/s] 92%|█████████▏| 277/300 [03:41<00:17,  1.28it/s] 93%|█████████▎| 278/300 [03:42<00:17,  1.29it/s] 93%|█████████▎| 279/300 [03:43<00:16,  1.27it/s] 93%|█████████▎| 280/300 [03:43<00:15,  1.27it/s] 94%|█████████▎| 281/300 [03:44<00:15,  1.27it/s] 94%|█████████▍| 282/300 [03:45<00:14,  1.26it/s] 94%|█████████▍| 283/300 [03:46<00:13,  1.27it/s] 95%|█████████▍| 284/300 [03:47<00:12,  1.26it/s] 95%|█████████▌| 285/300 [03:47<00:11,  1.26it/s] 95%|█████████▌| 286/300 [03:48<00:11,  1.24it/s] 96%|█████████▌| 287/300 [03:49<00:10,  1.24it/s] 96%|█████████▌| 288/300 [03:50<00:09,  1.27it/s] 96%|█████████▋| 289/300 [03:50<00:08,  1.28it/s] 97%|█████████▋| 290/300 [03:51<00:07,  1.28it/s] 97%|█████████▋| 291/300 [03:52<00:07,  1.28it/s] 97%|█████████▋| 292/300 [03:53<00:06,  1.28it/s] 98%|█████████▊| 293/300 [03:54<00:05,  1.29it/s] 98%|█████████▊| 294/300 [03:54<00:04,  1.29it/s] 98%|█████████▊| 295/300 [03:55<00:03,  1.28it/s] 99%|█████████▊| 296/300 [03:56<00:03,  1.29it/s] 99%|█████████▉| 297/300 [03:57<00:02,  1.29it/s] 99%|█████████▉| 298/300 [03:57<00:01,  1.30it/s]100%|█████████▉| 299/300 [03:58<00:00,  1.30it/s]100%|██████████| 300/300 [03:59<00:00,  1.31it/s]100%|██████████| 300/300 [03:59<00:00,  1.25it/s]
W&B disabled.
W&B online. Running your script from this directory will now sync to the cloud.
wandb: Currently logged in as: hl57. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.12 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.10
wandb: Run data is saved locally in /media/techt/One Touch/DD/SRe2L/train/wandb/run-20231023_170834-2l3cdfvx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fiery-night-495
wandb: ⭐️ View project at https://wandb.ai/hl57/final_rn18_fkd
wandb: 🚀 View run at https://wandb.ai/hl57/final_rn18_fkd/runs/2l3cdfvx
======= FKD: dataset info ======
path: ../relabel/FKD_cutmix_fp16/tiny-imagenet/315/
num img: 200
batch size: 100
max epoch: 300
================================
load data successfully
=> loading student model 'resnet18'
TRAIN Iter 0: lr = 0.001000,	loss = 0.010329,	Top-1 err = 99.500000,	Top-5 err = 94.500000,	train_time = 3.654861
TEST Iter 0: loss = 22.759350,	Top-1 err = 99.150000,	Top-5 err = 97.110000,	val_time = 19.355704
TRAIN Iter 10: lr = 0.000997,	loss = 0.005724,	Top-1 err = 97.000000,	Top-5 err = 87.500000,	train_time = 2.649634
TEST Iter 10: loss = 10.236808,	Top-1 err = 98.920000,	Top-5 err = 94.170000,	val_time = 19.275561
TRAIN Iter 20: lr = 0.000989,	loss = 0.005068,	Top-1 err = 97.000000,	Top-5 err = 88.000000,	train_time = 2.643136
TEST Iter 20: loss = 6.487978,	Top-1 err = 98.330000,	Top-5 err = 92.880000,	val_time = 19.112186
TRAIN Iter 30: lr = 0.000976,	loss = 0.004943,	Top-1 err = 96.500000,	Top-5 err = 88.500000,	train_time = 2.722963
TEST Iter 30: loss = 6.470348,	Top-1 err = 98.210000,	Top-5 err = 91.750000,	val_time = 19.443614
TRAIN Iter 40: lr = 0.000957,	loss = 0.004923,	Top-1 err = 94.000000,	Top-5 err = 79.500000,	train_time = 2.645735
TEST Iter 40: loss = 6.911826,	Top-1 err = 97.820000,	Top-5 err = 89.850000,	val_time = 19.056182
TRAIN Iter 50: lr = 0.000933,	loss = 0.004485,	Top-1 err = 97.500000,	Top-5 err = 87.000000,	train_time = 2.681881
TEST Iter 50: loss = 6.107896,	Top-1 err = 97.330000,	Top-5 err = 89.410000,	val_time = 19.030457
TRAIN Iter 60: lr = 0.000905,	loss = 0.004376,	Top-1 err = 96.000000,	Top-5 err = 89.000000,	train_time = 2.683247
TEST Iter 60: loss = 6.436010,	Top-1 err = 97.240000,	Top-5 err = 88.400000,	val_time = 19.728048
TRAIN Iter 70: lr = 0.000872,	loss = 0.004703,	Top-1 err = 94.000000,	Top-5 err = 81.500000,	train_time = 2.805815
TEST Iter 70: loss = 7.230901,	Top-1 err = 97.320000,	Top-5 err = 89.850000,	val_time = 19.337506
TRAIN Iter 80: lr = 0.000835,	loss = 0.004647,	Top-1 err = 89.500000,	Top-5 err = 64.000000,	train_time = 2.637238
TEST Iter 80: loss = 6.295752,	Top-1 err = 96.490000,	Top-5 err = 87.260000,	val_time = 19.306475
TRAIN Iter 90: lr = 0.000794,	loss = 0.004058,	Top-1 err = 90.500000,	Top-5 err = 68.500000,	train_time = 2.699360
TEST Iter 90: loss = 6.788718,	Top-1 err = 97.090000,	Top-5 err = 88.620000,	val_time = 19.138501
TRAIN Iter 100: lr = 0.000750,	loss = 0.004253,	Top-1 err = 89.000000,	Top-5 err = 70.000000,	train_time = 2.618088
TEST Iter 100: loss = 6.639788,	Top-1 err = 97.050000,	Top-5 err = 88.050000,	val_time = 19.250008
TRAIN Iter 110: lr = 0.000703,	loss = 0.003804,	Top-1 err = 96.500000,	Top-5 err = 86.000000,	train_time = 2.662289
TEST Iter 110: loss = 6.122403,	Top-1 err = 96.020000,	Top-5 err = 85.860000,	val_time = 19.298972
TRAIN Iter 120: lr = 0.000655,	loss = 0.004082,	Top-1 err = 87.500000,	Top-5 err = 65.500000,	train_time = 2.682030
TEST Iter 120: loss = 6.574512,	Top-1 err = 96.390000,	Top-5 err = 86.870000,	val_time = 19.452025
TRAIN Iter 130: lr = 0.000604,	loss = 0.003855,	Top-1 err = 91.500000,	Top-5 err = 72.000000,	train_time = 2.727262
TEST Iter 130: loss = 6.597541,	Top-1 err = 96.320000,	Top-5 err = 86.800000,	val_time = 19.509280
TRAIN Iter 140: lr = 0.000552,	loss = 0.004551,	Top-1 err = 86.500000,	Top-5 err = 71.500000,	train_time = 2.696141
TEST Iter 140: loss = 6.536275,	Top-1 err = 95.820000,	Top-5 err = 86.040000,	val_time = 19.120683
TRAIN Iter 150: lr = 0.000500,	loss = 0.003932,	Top-1 err = 95.000000,	Top-5 err = 81.500000,	train_time = 2.638697
TEST Iter 150: loss = 6.908921,	Top-1 err = 96.120000,	Top-5 err = 86.060000,	val_time = 19.278982
TRAIN Iter 160: lr = 0.000448,	loss = 0.003858,	Top-1 err = 91.500000,	Top-5 err = 73.000000,	train_time = 2.650102
TEST Iter 160: loss = 6.157012,	Top-1 err = 95.840000,	Top-5 err = 85.460000,	val_time = 19.275150
TRAIN Iter 170: lr = 0.000396,	loss = 0.003655,	Top-1 err = 95.000000,	Top-5 err = 83.500000,	train_time = 2.663919
TEST Iter 170: loss = 6.387801,	Top-1 err = 95.310000,	Top-5 err = 83.630000,	val_time = 19.243140
TRAIN Iter 180: lr = 0.000345,	loss = 0.003714,	Top-1 err = 80.500000,	Top-5 err = 57.000000,	train_time = 2.736396
TEST Iter 180: loss = 5.941138,	Top-1 err = 95.350000,	Top-5 err = 83.470000,	val_time = 19.254585
TRAIN Iter 190: lr = 0.000297,	loss = 0.003564,	Top-1 err = 95.500000,	Top-5 err = 80.000000,	train_time = 2.727137
TEST Iter 190: loss = 6.306185,	Top-1 err = 94.980000,	Top-5 err = 83.020000,	val_time = 19.384421
TRAIN Iter 200: lr = 0.000250,	loss = 0.003955,	Top-1 err = 68.000000,	Top-5 err = 37.500000,	train_time = 2.657496
TEST Iter 200: loss = 5.799890,	Top-1 err = 94.470000,	Top-5 err = 81.970000,	val_time = 19.016786
TRAIN Iter 210: lr = 0.000206,	loss = 0.003535,	Top-1 err = 90.500000,	Top-5 err = 69.500000,	train_time = 2.726692
TEST Iter 210: loss = 5.769936,	Top-1 err = 94.630000,	Top-5 err = 81.870000,	val_time = 19.813602
TRAIN Iter 220: lr = 0.000165,	loss = 0.003544,	Top-1 err = 78.500000,	Top-5 err = 44.500000,	train_time = 2.672651
TEST Iter 220: loss = 6.239513,	Top-1 err = 94.730000,	Top-5 err = 82.110000,	val_time = 19.950468
TRAIN Iter 230: lr = 0.000128,	loss = 0.003608,	Top-1 err = 91.500000,	Top-5 err = 73.000000,	train_time = 2.612541
TEST Iter 230: loss = 5.944715,	Top-1 err = 94.770000,	Top-5 err = 82.110000,	val_time = 19.248028
TRAIN Iter 240: lr = 0.000095,	loss = 0.003531,	Top-1 err = 68.500000,	Top-5 err = 37.000000,	train_time = 2.625585
TEST Iter 240: loss = 5.886292,	Top-1 err = 94.160000,	Top-5 err = 80.850000,	val_time = 19.167542
TRAIN Iter 250: lr = 0.000067,	loss = 0.003498,	Top-1 err = 76.000000,	Top-5 err = 42.500000,	train_time = 2.644123
TEST Iter 250: loss = 6.106847,	Top-1 err = 94.370000,	Top-5 err = 81.520000,	val_time = 19.053718
TRAIN Iter 260: lr = 0.000043,	loss = 0.003395,	Top-1 err = 78.500000,	Top-5 err = 49.000000,	train_time = 2.716091
TEST Iter 260: loss = 5.852073,	Top-1 err = 94.130000,	Top-5 err = 80.650000,	val_time = 19.236591
TRAIN Iter 270: lr = 0.000024,	loss = 0.003534,	Top-1 err = 76.000000,	Top-5 err = 53.500000,	train_time = 2.703123
TEST Iter 270: loss = 5.989820,	Top-1 err = 94.060000,	Top-5 err = 80.780000,	val_time = 18.749835
TRAIN Iter 280: lr = 0.000011,	loss = 0.003407,	Top-1 err = 75.000000,	Top-5 err = 48.000000,	train_time = 2.654837
TEST Iter 280: loss = 6.009462,	Top-1 err = 94.030000,	Top-5 err = 80.860000,	val_time = 19.226351
TRAIN Iter 290: lr = 0.000003,	loss = 0.003512,	Top-1 err = 67.500000,	Top-5 err = 33.500000,	train_time = 2.669415
TEST Iter 290: loss = 5.979911,	Top-1 err = 94.110000,	Top-5 err = 80.820000,	val_time = 19.741921
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.026 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.026 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:  train/Top1 ▁▁▁▁▁▂▁▁▂▂▃▃▂▃▂▁▂▃▂▄▆▅▅▃▅▃▅▅▅▂█▃▄▃▄▅▆▆▄▄
wandb:  train/Top5 ▁▁▁▂▁▃▂▂▃▄▄▄▃▅▃▂▃▄▃▆▇▅▆▄▆▄▆▇▅▂█▄▆▄▄▅▇▆▆▅
wandb: train/epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  train/loss ██▆▆▅▅▄▄▅▆▄▅▃▄▃▄▃▃▃▄▄▂▃▃▂▂▁▂▂▂▂▂▁▂▂▂▂▁▁▁
wandb:    train/lr ███████▇▇▇▇▇▇▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁
wandb:   val/epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███
wandb:    val/loss █▃▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    val/top1 ▁▁▂▂▃▃▄▄▅▄▄▅▅▅▆▅▆▆▆▇▇▇▇▇███████
wandb:    val/top5 ▁▂▃▃▄▄▅▄▅▅▅▆▅▅▆▆▆▇▇▇▇▇▇▇███████
wandb: 
wandb: Run summary:
wandb:  train/Top1 15.5
wandb:  train/Top5 36.5
wandb: train/epoch 299
wandb:  train/loss 0.00337
wandb:    train/lr 0.0
wandb:   val/epoch 299
wandb:    val/loss 5.9838
wandb:    val/top1 5.88
wandb:    val/top5 19.25
wandb: 
wandb: 🚀 View run fiery-night-495 at: https://wandb.ai/hl57/final_rn18_fkd/runs/2l3cdfvx
wandb: ️⚡ View job at https://wandb.ai/hl57/final_rn18_fkd/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjk3MzkyNjU3/version_details/v47
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231023_170834-2l3cdfvx/logs
TEST Iter 299: loss = 5.983797,	Top-1 err = 94.120000,	Top-5 err = 80.750000,	val_time = 19.584591
