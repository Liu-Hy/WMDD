r_bn:  1.0
lr:  0.25
bc shape (1000, 10, 512)
Execution time for computing per-class batchnorm statistics: 10383.878233 seconds
get_images call
class_per_batch  10 batch_size  100 args.ipc  10
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 0 end_cls 10
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 784.060164011138
main criterion 213.60624555410678
weighted_aux_loss 570.4539184570312
loss_r_bn_feature 570.4539184570312
------------iteration 100----------
total loss 423.0393952057939
main criterion 116.58868109446574
weighted_aux_loss 306.4507141113281
loss_r_bn_feature 306.4507141113281
------------iteration 200----------
total loss 362.36559030283934
main criterion 101.71233102549557
weighted_aux_loss 260.65325927734375
loss_r_bn_feature 260.65325927734375
------------iteration 300----------
total loss 323.15460949432634
main criterion 100.73468761932631
weighted_aux_loss 222.419921875
loss_r_bn_feature 222.419921875
------------iteration 400----------
total loss 293.0703325505018
main criterion 92.6524400944471
weighted_aux_loss 200.4178924560547
loss_r_bn_feature 200.4178924560547
------------iteration 500----------
total loss 398.4291078217127
main criterion 167.8382112152674
weighted_aux_loss 230.5908966064453
loss_r_bn_feature 230.5908966064453
------------iteration 600----------
total loss 272.8224358265861
main criterion 90.96263358049238
weighted_aux_loss 181.85980224609375
loss_r_bn_feature 181.85980224609375
------------iteration 700----------
total loss 246.8168592514445
main criterion 90.0461072983195
weighted_aux_loss 156.770751953125
loss_r_bn_feature 156.770751953125
------------iteration 800----------
total loss 358.2774113487154
main criterion 146.0466984580904
weighted_aux_loss 212.230712890625
loss_r_bn_feature 212.230712890625
------------iteration 900----------
total loss 234.74193998942766
main criterion 91.34521147380266
weighted_aux_loss 143.396728515625
loss_r_bn_feature 143.396728515625
------------iteration 1000----------
total loss 319.2225876205418
main criterion 147.51305392913554
weighted_aux_loss 171.70953369140625
loss_r_bn_feature 171.70953369140625
------------iteration 1100----------
total loss 202.1460318115879
main criterion 87.28162141119728
weighted_aux_loss 114.86441040039062
loss_r_bn_feature 114.86441040039062
------------iteration 1200----------
total loss 246.84060941286566
main criterion 124.37590299196724
weighted_aux_loss 122.46470642089844
loss_r_bn_feature 122.46470642089844
------------iteration 1300----------
total loss 208.3725110025413
main criterion 107.67570314121318
weighted_aux_loss 100.69680786132812
loss_r_bn_feature 100.69680786132812
------------iteration 1400----------
total loss 144.16622884098237
main criterion 73.0383067828769
weighted_aux_loss 71.12792205810547
loss_r_bn_feature 71.12792205810547
------------iteration 1500----------
total loss 120.26977906005516
main criterion 61.76890167968406
weighted_aux_loss 58.500877380371094
loss_r_bn_feature 58.500877380371094
------------iteration 1600----------
total loss 114.99786613310978
main criterion 61.42517326201603
weighted_aux_loss 53.57269287109375
loss_r_bn_feature 53.57269287109375
------------iteration 1700----------
total loss 142.39147968812532
main criterion 74.14085407777375
weighted_aux_loss 68.25062561035156
loss_r_bn_feature 68.25062561035156
------------iteration 1800----------
total loss 104.1730366181103
main criterion 58.022222561713825
weighted_aux_loss 46.150814056396484
loss_r_bn_feature 46.150814056396484
------------iteration 1900----------
total loss 140.67471463391556
main criterion 77.52280194470657
weighted_aux_loss 63.151912689208984
loss_r_bn_feature 63.151912689208984
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 10 end_cls 20
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 724.6031921404847
main criterion 216.31766967954718
weighted_aux_loss 508.2855224609375
loss_r_bn_feature 508.2855224609375
------------iteration 100----------
total loss 371.9765420096735
main criterion 89.42493678506415
weighted_aux_loss 282.5516052246094
loss_r_bn_feature 282.5516052246094
------------iteration 200----------
total loss 305.98481493259715
main criterion 88.53031664158149
weighted_aux_loss 217.45449829101562
loss_r_bn_feature 217.45449829101562
------------iteration 300----------
total loss 289.5541673080071
main criterion 88.05575422206957
weighted_aux_loss 201.4984130859375
loss_r_bn_feature 201.4984130859375
------------iteration 400----------
total loss 274.7097281051763
main criterion 100.69582734834033
weighted_aux_loss 174.01390075683594
loss_r_bn_feature 174.01390075683594
------------iteration 500----------
total loss 237.17451980393605
main criterion 85.07944228928763
weighted_aux_loss 152.09507751464844
loss_r_bn_feature 152.09507751464844
------------iteration 600----------
total loss 400.4062988592583
main criterion 167.23570559753955
weighted_aux_loss 233.17059326171875
loss_r_bn_feature 233.17059326171875
------------iteration 700----------
total loss 272.6073447994657
main criterion 110.22512739223914
weighted_aux_loss 162.38221740722656
loss_r_bn_feature 162.38221740722656
------------iteration 800----------
total loss 210.76810216440856
main criterion 75.8632559730023
weighted_aux_loss 134.90484619140625
loss_r_bn_feature 134.90484619140625
------------iteration 900----------
total loss 226.11607141099012
main criterion 79.80563134751354
weighted_aux_loss 146.31044006347656
loss_r_bn_feature 146.31044006347656
------------iteration 1000----------
total loss 211.78455064535586
main criterion 90.5141725325629
weighted_aux_loss 121.27037811279297
loss_r_bn_feature 121.27037811279297
------------iteration 1100----------
total loss 175.42792649738925
main criterion 71.85235734455723
weighted_aux_loss 103.57556915283203
loss_r_bn_feature 103.57556915283203
------------iteration 1200----------
total loss 160.36418246597302
main criterion 67.59372805923473
weighted_aux_loss 92.77045440673828
loss_r_bn_feature 92.77045440673828
------------iteration 1300----------
total loss 221.64543495462516
main criterion 107.89379463480095
weighted_aux_loss 113.75164031982422
loss_r_bn_feature 113.75164031982422
------------iteration 1400----------
total loss 141.75615836475959
main criterion 66.4696822295057
weighted_aux_loss 75.2864761352539
loss_r_bn_feature 75.2864761352539
------------iteration 1500----------
total loss 158.81082117934125
main criterion 77.10636103529829
weighted_aux_loss 81.70446014404297
loss_r_bn_feature 81.70446014404297
------------iteration 1600----------
total loss 170.1764087303034
main criterion 91.73814579085027
weighted_aux_loss 78.43826293945312
loss_r_bn_feature 78.43826293945312
------------iteration 1700----------
total loss 114.10114648279864
main criterion 55.37685754236895
weighted_aux_loss 58.72428894042969
loss_r_bn_feature 58.72428894042969
------------iteration 1800----------
total loss 101.29478109664848
main criterion 53.08119619674614
weighted_aux_loss 48.213584899902344
loss_r_bn_feature 48.213584899902344
------------iteration 1900----------
total loss 97.22092087137517
main criterion 51.6189006077033
weighted_aux_loss 45.602020263671875
loss_r_bn_feature 45.602020263671875
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 20 end_cls 30
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 746.1663658111211
main criterion 218.8028404204961
weighted_aux_loss 527.363525390625
loss_r_bn_feature 527.363525390625
------------iteration 100----------
total loss 389.5163012126371
main criterion 95.58890253099644
weighted_aux_loss 293.9273986816406
loss_r_bn_feature 293.9273986816406
------------iteration 200----------
total loss 359.50321175054745
main criterion 99.03238655523494
weighted_aux_loss 260.4708251953125
loss_r_bn_feature 260.4708251953125
------------iteration 300----------
total loss 306.9616396858045
main criterion 89.19683866041392
weighted_aux_loss 217.76480102539062
loss_r_bn_feature 217.76480102539062
------------iteration 400----------
total loss 281.39407531677966
main criterion 93.63218872009998
weighted_aux_loss 187.7618865966797
loss_r_bn_feature 187.7618865966797
------------iteration 500----------
total loss 242.01148366513718
main criterion 86.49915456357469
weighted_aux_loss 155.5123291015625
loss_r_bn_feature 155.5123291015625
------------iteration 600----------
total loss 245.80606717667402
main criterion 87.88558072647871
weighted_aux_loss 157.9204864501953
loss_r_bn_feature 157.9204864501953
------------iteration 700----------
total loss 233.0581264837657
main criterion 84.3367367132579
weighted_aux_loss 148.7213897705078
loss_r_bn_feature 148.7213897705078
------------iteration 800----------
total loss 325.74508074738765
main criterion 141.771035947583
weighted_aux_loss 183.9740447998047
loss_r_bn_feature 183.9740447998047
------------iteration 900----------
total loss 265.14626606787755
main criterion 113.64240559424472
weighted_aux_loss 151.5038604736328
loss_r_bn_feature 151.5038604736328
------------iteration 1000----------
total loss 181.65477141209414
main criterion 72.72371825047306
weighted_aux_loss 108.9310531616211
loss_r_bn_feature 108.9310531616211
------------iteration 1100----------
total loss 207.1526783130123
main criterion 95.61828500246543
weighted_aux_loss 111.53439331054688
loss_r_bn_feature 111.53439331054688
------------iteration 1200----------
total loss 191.18703535348052
main criterion 83.9500434711563
weighted_aux_loss 107.23699188232422
loss_r_bn_feature 107.23699188232422
------------iteration 1300----------
total loss 148.43554886557422
main criterion 68.71592911459767
weighted_aux_loss 79.71961975097656
loss_r_bn_feature 79.71961975097656
------------iteration 1400----------
total loss 256.0651415362951
main criterion 121.2624834552404
weighted_aux_loss 134.8026580810547
loss_r_bn_feature 134.8026580810547
------------iteration 1500----------
total loss 178.995822866021
main criterion 95.5743827415093
weighted_aux_loss 83.42144012451172
loss_r_bn_feature 83.42144012451172
------------iteration 1600----------
total loss 122.673906167393
main criterion 63.77407630289104
weighted_aux_loss 58.89982986450195
loss_r_bn_feature 58.89982986450195
------------iteration 1700----------
total loss 171.67901712542962
main criterion 91.22741800433587
weighted_aux_loss 80.45159912109375
loss_r_bn_feature 80.45159912109375
------------iteration 1800----------
total loss 297.830672370271
main criterion 133.81758032925535
weighted_aux_loss 164.01309204101562
loss_r_bn_feature 164.01309204101562
------------iteration 1900----------
total loss 214.20617238619192
main criterion 98.2849992904888
weighted_aux_loss 115.92117309570312
loss_r_bn_feature 115.92117309570312
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 30 end_cls 40
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 719.6585736780139
main criterion 217.28961615848266
weighted_aux_loss 502.36895751953125
loss_r_bn_feature 502.36895751953125
------------iteration 100----------
total loss 374.77886065122823
main criterion 92.88289507505638
weighted_aux_loss 281.8959655761719
loss_r_bn_feature 281.8959655761719
------------iteration 200----------
total loss 349.7442577421529
main criterion 98.2126110136373
weighted_aux_loss 251.53164672851562
loss_r_bn_feature 251.53164672851562
------------iteration 300----------
total loss 438.46684686322357
main criterion 169.97923699994232
weighted_aux_loss 268.48760986328125
loss_r_bn_feature 268.48760986328125
------------iteration 400----------
total loss 409.3028160647708
main criterion 163.1667687014896
weighted_aux_loss 246.13604736328125
loss_r_bn_feature 246.13604736328125
------------iteration 500----------
total loss 275.37069677248246
main criterion 102.42037938966996
weighted_aux_loss 172.9503173828125
loss_r_bn_feature 172.9503173828125
------------iteration 600----------
total loss 237.3287193088834
main criterion 85.57125776103183
weighted_aux_loss 151.75746154785156
loss_r_bn_feature 151.75746154785156
------------iteration 700----------
total loss 345.93727530450593
main criterion 147.73393668145906
weighted_aux_loss 198.20333862304688
loss_r_bn_feature 198.20333862304688
------------iteration 800----------
total loss 213.51996137247716
main criterion 82.39091779337558
weighted_aux_loss 131.12904357910156
loss_r_bn_feature 131.12904357910156
------------iteration 900----------
total loss 197.70976767553566
main criterion 81.32902274145364
weighted_aux_loss 116.38074493408203
loss_r_bn_feature 116.38074493408203
------------iteration 1000----------
total loss 259.7831181912977
main criterion 116.28575796180552
weighted_aux_loss 143.4973602294922
loss_r_bn_feature 143.4973602294922
------------iteration 1100----------
total loss 185.73735732790087
main criterion 80.03637618776415
weighted_aux_loss 105.70098114013672
loss_r_bn_feature 105.70098114013672
------------iteration 1200----------
total loss 160.41748682470168
main criterion 75.33618799657668
weighted_aux_loss 85.081298828125
loss_r_bn_feature 85.081298828125
------------iteration 1300----------
total loss 135.17214909202255
main criterion 65.9521860182921
weighted_aux_loss 69.21996307373047
loss_r_bn_feature 69.21996307373047
------------iteration 1400----------
total loss 124.40178829647219
main criterion 60.572778286706566
weighted_aux_loss 63.829010009765625
loss_r_bn_feature 63.829010009765625
------------iteration 1500----------
total loss 296.54915877943466
main criterion 138.66964217787216
weighted_aux_loss 157.8795166015625
loss_r_bn_feature 157.8795166015625
------------iteration 1600----------
total loss 111.94328285307469
main criterion 58.30934883207859
weighted_aux_loss 53.633934020996094
loss_r_bn_feature 53.633934020996094
------------iteration 1700----------
total loss 102.35349919274395
main criterion 53.067190904169735
weighted_aux_loss 49.28630828857422
loss_r_bn_feature 49.28630828857422
------------iteration 1800----------
total loss 92.7938142633492
main criterion 53.07091768742147
weighted_aux_loss 39.722896575927734
loss_r_bn_feature 39.722896575927734
------------iteration 1900----------
total loss 91.6657455294274
main criterion 49.886971267952795
weighted_aux_loss 41.77877426147461
loss_r_bn_feature 41.77877426147461
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 40 end_cls 50
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 766.3123408964746
main criterion 213.75734822069336
weighted_aux_loss 552.5549926757812
loss_r_bn_feature 552.5549926757812
------------iteration 100----------
total loss 496.70353756353495
main criterion 174.25880489751933
weighted_aux_loss 322.4447326660156
loss_r_bn_feature 322.4447326660156
------------iteration 200----------
total loss 342.1050625886183
main criterion 106.11104403393078
weighted_aux_loss 235.9940185546875
loss_r_bn_feature 235.9940185546875
------------iteration 300----------
total loss 291.9467437633407
main criterion 89.0566375621688
weighted_aux_loss 202.89010620117188
loss_r_bn_feature 202.89010620117188
------------iteration 400----------
total loss 287.96893622483697
main criterion 100.5383026799151
weighted_aux_loss 187.43063354492188
loss_r_bn_feature 187.43063354492188
------------iteration 500----------
total loss 286.18625260731943
main criterion 96.669788373921
weighted_aux_loss 189.51646423339844
loss_r_bn_feature 189.51646423339844
------------iteration 600----------
total loss 397.8940513146986
main criterion 153.5570151818861
weighted_aux_loss 244.3370361328125
loss_r_bn_feature 244.3370361328125
------------iteration 700----------
total loss 233.89637753213844
main criterion 84.34846493448218
weighted_aux_loss 149.54791259765625
loss_r_bn_feature 149.54791259765625
------------iteration 800----------
total loss 213.6047326962576
main criterion 82.02376956149197
weighted_aux_loss 131.58096313476562
loss_r_bn_feature 131.58096313476562
------------iteration 900----------
total loss 192.72085803019758
main criterion 76.17262499797103
weighted_aux_loss 116.54823303222656
loss_r_bn_feature 116.54823303222656
------------iteration 1000----------
total loss 204.57548413445267
main criterion 85.91544232537063
weighted_aux_loss 118.66004180908203
loss_r_bn_feature 118.66004180908203
------------iteration 1100----------
total loss 167.9143942009647
main criterion 74.3659628044803
weighted_aux_loss 93.54843139648438
loss_r_bn_feature 93.54843139648438
------------iteration 1200----------
total loss 159.95015897227807
main criterion 69.66199674083275
weighted_aux_loss 90.28816223144531
loss_r_bn_feature 90.28816223144531
------------iteration 1300----------
total loss 151.14432557686956
main criterion 71.16138490304144
weighted_aux_loss 79.98294067382812
loss_r_bn_feature 79.98294067382812
------------iteration 1400----------
total loss 221.27492896399434
main criterion 107.78593055090842
weighted_aux_loss 113.48899841308594
loss_r_bn_feature 113.48899841308594
------------iteration 1500----------
total loss 153.05115129884206
main criterion 76.4251823657366
weighted_aux_loss 76.62596893310547
loss_r_bn_feature 76.62596893310547
------------iteration 1600----------
total loss 115.89957682764756
main criterion 54.18614833033312
weighted_aux_loss 61.71342849731445
loss_r_bn_feature 61.71342849731445
------------iteration 1700----------
total loss 197.11856787106524
main criterion 97.11295263669025
weighted_aux_loss 100.005615234375
loss_r_bn_feature 100.005615234375
------------iteration 1800----------
total loss 90.6440196514035
main criterion 48.61192660330779
weighted_aux_loss 42.0320930480957
loss_r_bn_feature 42.0320930480957
------------iteration 1900----------
total loss 86.44643246482795
main criterion 46.299379699935365
weighted_aux_loss 40.14705276489258
loss_r_bn_feature 40.14705276489258
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 50 end_cls 60
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 775.0409649594546
main criterion 215.9546612485172
weighted_aux_loss 559.0863037109375
loss_r_bn_feature 559.0863037109375
------------iteration 100----------
total loss 434.57952476049553
main criterion 128.69024253393303
weighted_aux_loss 305.8892822265625
loss_r_bn_feature 305.8892822265625
------------iteration 200----------
total loss 317.6135393054942
main criterion 87.23419055061142
weighted_aux_loss 230.3793487548828
loss_r_bn_feature 230.3793487548828
------------iteration 300----------
total loss 276.45789955253827
main criterion 90.73824928398358
weighted_aux_loss 185.7196502685547
loss_r_bn_feature 185.7196502685547
------------iteration 400----------
total loss 338.42956051981236
main criterion 120.1453655735233
weighted_aux_loss 218.28419494628906
loss_r_bn_feature 218.28419494628906
------------iteration 500----------
total loss 272.90251637213896
main criterion 87.85139942877957
weighted_aux_loss 185.05111694335938
loss_r_bn_feature 185.05111694335938
------------iteration 600----------
total loss 342.91458114688004
main criterion 127.67073043887221
weighted_aux_loss 215.2438507080078
loss_r_bn_feature 215.2438507080078
------------iteration 700----------
total loss 242.2421252792141
main criterion 84.62626529386253
weighted_aux_loss 157.61585998535156
loss_r_bn_feature 157.61585998535156
------------iteration 800----------
total loss 202.9040361254427
main criterion 75.43157823970053
weighted_aux_loss 127.47245788574219
loss_r_bn_feature 127.47245788574219
------------iteration 900----------
total loss 180.59250213025263
main criterion 68.29800750134639
weighted_aux_loss 112.29449462890625
loss_r_bn_feature 112.29449462890625
------------iteration 1000----------
total loss 180.23469877776853
main criterion 67.53371000823726
weighted_aux_loss 112.70098876953125
loss_r_bn_feature 112.70098876953125
------------iteration 1100----------
total loss 219.9620132169678
main criterion 102.36698147868654
weighted_aux_loss 117.59503173828125
loss_r_bn_feature 117.59503173828125
------------iteration 1200----------
total loss 151.20361315461088
main criterion 64.27105700226713
weighted_aux_loss 86.93255615234375
loss_r_bn_feature 86.93255615234375
------------iteration 1300----------
total loss 144.17224789844218
main criterion 64.86341000781718
weighted_aux_loss 79.308837890625
loss_r_bn_feature 79.308837890625
------------iteration 1400----------
total loss 154.00550986077823
main criterion 72.82445669915714
weighted_aux_loss 81.1810531616211
loss_r_bn_feature 81.1810531616211
------------iteration 1500----------
total loss 123.60888927149044
main criterion 62.683252979986534
weighted_aux_loss 60.925636291503906
loss_r_bn_feature 60.925636291503906
------------iteration 1600----------
total loss 112.84315586694225
main criterion 56.8796052993153
weighted_aux_loss 55.96355056762695
loss_r_bn_feature 55.96355056762695
------------iteration 1700----------
total loss 124.48221479076447
main criterion 67.65286145824494
weighted_aux_loss 56.82935333251953
loss_r_bn_feature 56.82935333251953
------------iteration 1800----------
total loss 116.63287918269799
main criterion 58.69589035213159
weighted_aux_loss 57.936988830566406
loss_r_bn_feature 57.936988830566406
------------iteration 1900----------
total loss 87.80501181477794
main criterion 47.036407536213495
weighted_aux_loss 40.76860427856445
loss_r_bn_feature 40.76860427856445
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 60 end_cls 70
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 762.3831859657259
main criterion 209.6582103797884
weighted_aux_loss 552.7249755859375
loss_r_bn_feature 552.7249755859375
------------iteration 100----------
total loss 450.76123076866884
main criterion 129.4358828682782
weighted_aux_loss 321.3253479003906
loss_r_bn_feature 321.3253479003906
------------iteration 200----------
total loss 324.64041530217736
main criterion 98.65907680120081
weighted_aux_loss 225.98133850097656
loss_r_bn_feature 225.98133850097656
------------iteration 300----------
total loss 322.2431982098551
main criterion 96.51559811219886
weighted_aux_loss 225.72760009765625
loss_r_bn_feature 225.72760009765625
------------iteration 400----------
total loss 404.48736286879546
main criterion 165.89657307387358
weighted_aux_loss 238.59078979492188
loss_r_bn_feature 238.59078979492188
------------iteration 500----------
total loss 276.37921507942497
main criterion 93.29904540169059
weighted_aux_loss 183.08016967773438
loss_r_bn_feature 183.08016967773438
------------iteration 600----------
total loss 356.9359552014616
main criterion 148.66480651982096
weighted_aux_loss 208.27114868164062
loss_r_bn_feature 208.27114868164062
------------iteration 700----------
total loss 387.7021726460387
main criterion 169.37050760697622
weighted_aux_loss 218.3316650390625
loss_r_bn_feature 218.3316650390625
------------iteration 800----------
total loss 221.70520260743524
main criterion 78.14348080567743
weighted_aux_loss 143.5617218017578
loss_r_bn_feature 143.5617218017578
------------iteration 900----------
total loss 197.12621526150613
main criterion 76.56250218777566
weighted_aux_loss 120.56371307373047
loss_r_bn_feature 120.56371307373047
------------iteration 1000----------
total loss 189.3020013548312
main criterion 77.82538544906946
weighted_aux_loss 111.47661590576172
loss_r_bn_feature 111.47661590576172
------------iteration 1100----------
total loss 184.76089947289063
main criterion 82.16478436058594
weighted_aux_loss 102.59611511230469
loss_r_bn_feature 102.59611511230469
------------iteration 1200----------
total loss 357.9939455673549
main criterion 162.15915247653462
weighted_aux_loss 195.8347930908203
loss_r_bn_feature 195.8347930908203
------------iteration 1300----------
total loss 136.7095818935915
main criterion 60.98502592435322
weighted_aux_loss 75.72455596923828
loss_r_bn_feature 75.72455596923828
------------iteration 1400----------
total loss 142.56688728792741
main criterion 71.12866249544695
weighted_aux_loss 71.43822479248047
loss_r_bn_feature 71.43822479248047
------------iteration 1500----------
total loss 111.38115396627293
main criterion 53.77161493429052
weighted_aux_loss 57.60953903198242
loss_r_bn_feature 57.60953903198242
------------iteration 1600----------
total loss 147.87956532357933
main criterion 73.85769184945823
weighted_aux_loss 74.0218734741211
loss_r_bn_feature 74.0218734741211
------------iteration 1700----------
total loss 142.47231190234754
main criterion 68.01499073535535
weighted_aux_loss 74.45732116699219
loss_r_bn_feature 74.45732116699219
------------iteration 1800----------
total loss 124.8891557160899
main criterion 56.0774110261485
weighted_aux_loss 68.8117446899414
loss_r_bn_feature 68.8117446899414
------------iteration 1900----------
total loss 102.74756971351135
main criterion 56.49706235877502
weighted_aux_loss 46.25050735473633
loss_r_bn_feature 46.25050735473633
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 70 end_cls 80
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 726.6267791186076
main criterion 213.8811126147014
weighted_aux_loss 512.7456665039062
loss_r_bn_feature 512.7456665039062
------------iteration 100----------
total loss 420.11818934824873
main criterion 98.16527797129562
weighted_aux_loss 321.9529113769531
loss_r_bn_feature 321.9529113769531
------------iteration 200----------
total loss 465.8447554037828
main criterion 173.3889143393297
weighted_aux_loss 292.4558410644531
loss_r_bn_feature 292.4558410644531
------------iteration 300----------
total loss 312.9164901919155
main criterion 88.70224153468892
weighted_aux_loss 224.21424865722656
loss_r_bn_feature 224.21424865722656
------------iteration 400----------
total loss 471.0024604814796
main criterion 178.59181595022957
weighted_aux_loss 292.41064453125
loss_r_bn_feature 292.41064453125
------------iteration 500----------
total loss 434.7415888231144
main criterion 178.4187128465519
weighted_aux_loss 256.3228759765625
loss_r_bn_feature 256.3228759765625
------------iteration 600----------
total loss 265.048211531553
main criterion 88.24277635088897
weighted_aux_loss 176.80543518066406
loss_r_bn_feature 176.80543518066406
------------iteration 700----------
total loss 339.5754445898028
main criterion 139.29437769527155
weighted_aux_loss 200.28106689453125
loss_r_bn_feature 200.28106689453125
------------iteration 800----------
total loss 327.62697658710056
main criterion 144.5412679689365
weighted_aux_loss 183.08570861816406
loss_r_bn_feature 183.08570861816406
------------iteration 900----------
total loss 248.5668133181132
main criterion 96.48827633080849
weighted_aux_loss 152.0785369873047
loss_r_bn_feature 152.0785369873047
------------iteration 1000----------
total loss 198.6417382746356
main criterion 76.42112670236999
weighted_aux_loss 122.22061157226562
loss_r_bn_feature 122.22061157226562
------------iteration 1100----------
total loss 180.309702048531
main criterion 73.04847157978101
weighted_aux_loss 107.26123046875
loss_r_bn_feature 107.26123046875
------------iteration 1200----------
total loss 166.4318674068227
main criterion 71.77651004598286
weighted_aux_loss 94.65535736083984
loss_r_bn_feature 94.65535736083984
------------iteration 1300----------
total loss 149.17438977700775
main criterion 67.68704694253509
weighted_aux_loss 81.48734283447266
loss_r_bn_feature 81.48734283447266
------------iteration 1400----------
total loss 220.94416947533813
main criterion 110.42288346459594
weighted_aux_loss 110.52128601074219
loss_r_bn_feature 110.52128601074219
------------iteration 1500----------
total loss 119.75156059124188
main criterion 58.67338600017743
weighted_aux_loss 61.07817459106445
loss_r_bn_feature 61.07817459106445
------------iteration 1600----------
total loss 158.34640972519398
main criterion 72.83455364609242
weighted_aux_loss 85.51185607910156
loss_r_bn_feature 85.51185607910156
------------iteration 1700----------
total loss 106.03498266867565
main criterion 53.77214239767956
weighted_aux_loss 52.262840270996094
loss_r_bn_feature 52.262840270996094
------------iteration 1800----------
total loss 298.42727367315285
main criterion 140.7746400061607
weighted_aux_loss 157.6526336669922
loss_r_bn_feature 157.6526336669922
------------iteration 1900----------
total loss 218.89406709021998
main criterion 106.34113435096216
weighted_aux_loss 112.55293273925781
loss_r_bn_feature 112.55293273925781
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 80 end_cls 90
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 783.7723784915551
main criterion 213.56070856968003
weighted_aux_loss 570.211669921875
loss_r_bn_feature 570.211669921875
------------iteration 100----------
total loss 367.0855150631875
main criterion 95.85553459443746
weighted_aux_loss 271.22998046875
loss_r_bn_feature 271.22998046875
------------iteration 200----------
total loss 304.7513154524255
main criterion 91.06077895340206
weighted_aux_loss 213.69053649902344
loss_r_bn_feature 213.69053649902344
------------iteration 300----------
total loss 428.9119281215685
main criterion 189.22915834617785
weighted_aux_loss 239.68276977539062
loss_r_bn_feature 239.68276977539062
------------iteration 400----------
total loss 282.7974285811546
main criterion 109.98494383994361
weighted_aux_loss 172.81248474121094
loss_r_bn_feature 172.81248474121094
------------iteration 500----------
total loss 269.3282404482975
main criterion 100.19480233794593
weighted_aux_loss 169.13343811035156
loss_r_bn_feature 169.13343811035156
------------iteration 600----------
total loss 236.2770692296387
main criterion 87.67901625112307
weighted_aux_loss 148.59805297851562
loss_r_bn_feature 148.59805297851562
------------iteration 700----------
total loss 383.2556239815365
main criterion 168.5268947334896
weighted_aux_loss 214.72872924804688
loss_r_bn_feature 214.72872924804688
------------iteration 800----------
total loss 212.45388808348113
main criterion 85.24999251463348
weighted_aux_loss 127.20389556884766
loss_r_bn_feature 127.20389556884766
------------iteration 900----------
total loss 195.40517195926842
main criterion 83.46312484012779
weighted_aux_loss 111.94204711914062
loss_r_bn_feature 111.94204711914062
------------iteration 1000----------
total loss 177.9042057408756
main criterion 77.04061931509435
weighted_aux_loss 100.86358642578125
loss_r_bn_feature 100.86358642578125
------------iteration 1100----------
total loss 169.76683262347768
main criterion 74.95262363910267
weighted_aux_loss 94.814208984375
loss_r_bn_feature 94.814208984375
------------iteration 1200----------
total loss 159.4123788062813
main criterion 72.8259759132149
weighted_aux_loss 86.5864028930664
loss_r_bn_feature 86.5864028930664
------------iteration 1300----------
total loss 148.30729391720837
main criterion 68.43739798214976
weighted_aux_loss 79.8698959350586
loss_r_bn_feature 79.8698959350586
------------iteration 1400----------
total loss 135.96351709647666
main criterion 63.8413552312423
weighted_aux_loss 72.12216186523438
loss_r_bn_feature 72.12216186523438
------------iteration 1500----------
total loss 182.16889041721777
main criterion 88.40394443333106
weighted_aux_loss 93.76494598388672
loss_r_bn_feature 93.76494598388672
------------iteration 1600----------
total loss 308.3925695186599
main criterion 147.01999566612088
weighted_aux_loss 161.37257385253906
loss_r_bn_feature 161.37257385253906
------------iteration 1700----------
total loss 138.2042613296868
main criterion 70.03549912265555
weighted_aux_loss 68.16876220703125
loss_r_bn_feature 68.16876220703125
------------iteration 1800----------
total loss 105.82850691798572
main criterion 53.92427489283924
weighted_aux_loss 51.904232025146484
loss_r_bn_feature 51.904232025146484
------------iteration 1900----------
total loss 105.95236252945789
main criterion 58.02464341324695
weighted_aux_loss 47.92771911621094
loss_r_bn_feature 47.92771911621094
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 90 end_cls 100
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 754.3996085358228
main criterion 219.1649893951979
weighted_aux_loss 535.234619140625
loss_r_bn_feature 535.234619140625
------------iteration 100----------
total loss 382.32827338946413
main criterion 97.52141914141727
weighted_aux_loss 284.8068542480469
loss_r_bn_feature 284.8068542480469
------------iteration 200----------
total loss 335.8985405624077
main criterion 102.90896231533735
weighted_aux_loss 232.9895782470703
loss_r_bn_feature 232.9895782470703
------------iteration 300----------
total loss 342.0434150810537
main criterion 118.72336198046777
weighted_aux_loss 223.32005310058594
loss_r_bn_feature 223.32005310058594
------------iteration 400----------
total loss 380.6500881993212
main criterion 143.85678375596183
weighted_aux_loss 236.79330444335938
loss_r_bn_feature 236.79330444335938
------------iteration 500----------
total loss 297.35168348614025
main criterion 113.75648390117928
weighted_aux_loss 183.59519958496094
loss_r_bn_feature 183.59519958496094
------------iteration 600----------
total loss 247.06635903110077
main criterion 86.31518410434295
weighted_aux_loss 160.7511749267578
loss_r_bn_feature 160.7511749267578
------------iteration 700----------
total loss 336.6598040831185
main criterion 145.1681811583138
weighted_aux_loss 191.4916229248047
loss_r_bn_feature 191.4916229248047
------------iteration 800----------
total loss 213.71961877156693
main criterion 79.80038354207474
weighted_aux_loss 133.9192352294922
loss_r_bn_feature 133.9192352294922
------------iteration 900----------
total loss 299.56057201346357
main criterion 135.03602062186198
weighted_aux_loss 164.52455139160156
loss_r_bn_feature 164.52455139160156
------------iteration 1000----------
total loss 191.05744006947424
main criterion 77.18279102162266
weighted_aux_loss 113.87464904785156
loss_r_bn_feature 113.87464904785156
------------iteration 1100----------
total loss 279.59629266011916
main criterion 128.431467220666
weighted_aux_loss 151.16482543945312
loss_r_bn_feature 151.16482543945312
------------iteration 1200----------
total loss 172.13369063673707
main criterion 78.18064193068238
weighted_aux_loss 93.95304870605469
loss_r_bn_feature 93.95304870605469
------------iteration 1300----------
total loss 180.43025572666065
main criterion 87.9101522720708
weighted_aux_loss 92.52010345458984
loss_r_bn_feature 92.52010345458984
------------iteration 1400----------
total loss 176.01489507875522
main criterion 81.52823888979037
weighted_aux_loss 94.48665618896484
loss_r_bn_feature 94.48665618896484
------------iteration 1500----------
total loss 132.59157847081775
main criterion 61.390566813102915
weighted_aux_loss 71.20101165771484
loss_r_bn_feature 71.20101165771484
------------iteration 1600----------
total loss 121.72326081085639
main criterion 58.57885926056342
weighted_aux_loss 63.14440155029297
loss_r_bn_feature 63.14440155029297
------------iteration 1700----------
total loss 106.8326887476864
main criterion 56.496751247686404
weighted_aux_loss 50.3359375
loss_r_bn_feature 50.3359375
------------iteration 1800----------
total loss 101.49528558482658
main criterion 52.81286676158439
weighted_aux_loss 48.68241882324219
loss_r_bn_feature 48.68241882324219
------------iteration 1900----------
total loss 211.51212759309806
main criterion 106.31631918245353
weighted_aux_loss 105.19580841064453
loss_r_bn_feature 105.19580841064453
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 100 end_cls 110
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 755.2761818313104
main criterion 214.47466815943542
weighted_aux_loss 540.801513671875
loss_r_bn_feature 540.801513671875
------------iteration 100----------
total loss 376.1668167791496
main criterion 99.89661414243088
weighted_aux_loss 276.27020263671875
loss_r_bn_feature 276.27020263671875
------------iteration 200----------
total loss 341.1372871174585
main criterion 100.29950330398195
weighted_aux_loss 240.83778381347656
loss_r_bn_feature 240.83778381347656
------------iteration 300----------
total loss 323.7319148994763
main criterion 101.25906028521848
weighted_aux_loss 222.4728546142578
loss_r_bn_feature 222.4728546142578
------------iteration 400----------
total loss 290.06002295921337
main criterion 102.49451697776803
weighted_aux_loss 187.5655059814453
loss_r_bn_feature 187.5655059814453
------------iteration 500----------
total loss 286.7201277714592
main criterion 102.83623189743578
weighted_aux_loss 183.88389587402344
loss_r_bn_feature 183.88389587402344
------------iteration 600----------
total loss 256.6539125718769
main criterion 93.09303000351754
weighted_aux_loss 163.56088256835938
loss_r_bn_feature 163.56088256835938
------------iteration 700----------
total loss 381.96216588100515
main criterion 158.44748692592702
weighted_aux_loss 223.51467895507812
loss_r_bn_feature 223.51467895507812
------------iteration 800----------
total loss 226.3800668915502
main criterion 88.99173070990959
weighted_aux_loss 137.38833618164062
loss_r_bn_feature 137.38833618164062
------------iteration 900----------
total loss 185.3031482366527
main criterion 77.25252720393785
weighted_aux_loss 108.05062103271484
loss_r_bn_feature 108.05062103271484
------------iteration 1000----------
total loss 184.3256037660027
main criterion 77.61174420789722
weighted_aux_loss 106.71385955810547
loss_r_bn_feature 106.71385955810547
------------iteration 1100----------
total loss 169.9699245577909
main criterion 74.52259789763465
weighted_aux_loss 95.44732666015625
loss_r_bn_feature 95.44732666015625
------------iteration 1200----------
total loss 224.15630841738698
main criterion 103.53056836611746
weighted_aux_loss 120.62574005126953
loss_r_bn_feature 120.62574005126953
------------iteration 1300----------
total loss 252.67775685507723
main criterion 114.53978688437412
weighted_aux_loss 138.13796997070312
loss_r_bn_feature 138.13796997070312
------------iteration 1400----------
total loss 164.10909431534947
main criterion 81.87735908585726
weighted_aux_loss 82.23173522949219
loss_r_bn_feature 82.23173522949219
------------iteration 1500----------
total loss 213.43796373932443
main criterion 108.3535063418635
weighted_aux_loss 105.08445739746094
loss_r_bn_feature 105.08445739746094
------------iteration 1600----------
total loss 191.6067942302142
main criterion 98.49922739671811
weighted_aux_loss 93.1075668334961
loss_r_bn_feature 93.1075668334961
------------iteration 1700----------
total loss 106.38577637250407
main criterion 55.850387425970865
weighted_aux_loss 50.5353889465332
loss_r_bn_feature 50.5353889465332
------------iteration 1800----------
total loss 304.69785653724625
main criterion 139.79072152748063
weighted_aux_loss 164.90713500976562
loss_r_bn_feature 164.90713500976562
------------iteration 1900----------
total loss 128.20142603560575
main criterion 70.04178858443387
weighted_aux_loss 58.159637451171875
loss_r_bn_feature 58.159637451171875
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 110 end_cls 120
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 777.3561408705591
main criterion 206.88153149555913
weighted_aux_loss 570.474609375
loss_r_bn_feature 570.474609375
------------iteration 100----------
total loss 431.03574394207976
main criterion 110.60031913739229
weighted_aux_loss 320.4354248046875
loss_r_bn_feature 320.4354248046875
------------iteration 200----------
total loss 343.4157182187845
main criterion 95.79267134378448
weighted_aux_loss 247.623046875
loss_r_bn_feature 247.623046875
------------iteration 300----------
total loss 307.3926040907612
main criterion 94.2880813856831
weighted_aux_loss 213.10452270507812
loss_r_bn_feature 213.10452270507812
------------iteration 400----------
total loss 429.8074387955445
main criterion 162.34030622718512
weighted_aux_loss 267.4671325683594
loss_r_bn_feature 267.4671325683594
------------iteration 500----------
total loss 341.5494611161743
main criterion 133.76338933883056
weighted_aux_loss 207.78607177734375
loss_r_bn_feature 207.78607177734375
------------iteration 600----------
total loss 238.46685435848514
main criterion 89.96250560360234
weighted_aux_loss 148.5043487548828
loss_r_bn_feature 148.5043487548828
------------iteration 700----------
total loss 285.1362570043009
main criterion 120.48304350332434
weighted_aux_loss 164.65321350097656
loss_r_bn_feature 164.65321350097656
------------iteration 800----------
total loss 264.1026612622302
main criterion 117.13916028566766
weighted_aux_loss 146.9635009765625
loss_r_bn_feature 146.9635009765625
------------iteration 900----------
total loss 201.49787167938868
main criterion 80.16907529266993
weighted_aux_loss 121.32879638671875
loss_r_bn_feature 121.32879638671875
------------iteration 1000----------
total loss 207.52005079489487
main criterion 92.64209821921129
weighted_aux_loss 114.8779525756836
loss_r_bn_feature 114.8779525756836
------------iteration 1100----------
total loss 201.0644209967415
main criterion 94.80048972232743
weighted_aux_loss 106.26393127441406
loss_r_bn_feature 106.26393127441406
------------iteration 1200----------
total loss 151.91217498992182
main criterion 69.45773010466793
weighted_aux_loss 82.4544448852539
loss_r_bn_feature 82.4544448852539
------------iteration 1300----------
total loss 147.14148115899627
main criterion 71.01880049493376
weighted_aux_loss 76.1226806640625
loss_r_bn_feature 76.1226806640625
------------iteration 1400----------
total loss 145.8411780942419
main criterion 74.56580272803096
weighted_aux_loss 71.27537536621094
loss_r_bn_feature 71.27537536621094
------------iteration 1500----------
total loss 341.5048585292634
main criterion 151.15988782613837
weighted_aux_loss 190.344970703125
loss_r_bn_feature 190.344970703125
------------iteration 1600----------
total loss 111.69729164979464
main criterion 57.79720238587861
weighted_aux_loss 53.900089263916016
loss_r_bn_feature 53.900089263916016
------------iteration 1700----------
total loss 107.09560445856067
main criterion 57.63709692071887
weighted_aux_loss 49.4585075378418
loss_r_bn_feature 49.4585075378418
------------iteration 1800----------
total loss 170.34650553893243
main criterion 90.11746348570979
weighted_aux_loss 80.22904205322266
loss_r_bn_feature 80.22904205322266
------------iteration 1900----------
total loss 180.17171465149553
main criterion 89.2332533477846
weighted_aux_loss 90.93846130371094
loss_r_bn_feature 90.93846130371094
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 120 end_cls 130
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 813.2115243506009
main criterion 216.81528411622588
weighted_aux_loss 596.396240234375
loss_r_bn_feature 596.396240234375
------------iteration 100----------
total loss 384.7681454212979
main criterion 93.43437466934478
weighted_aux_loss 291.3337707519531
loss_r_bn_feature 291.3337707519531
------------iteration 200----------
total loss 379.2166033620581
main criterion 112.54411801049558
weighted_aux_loss 266.6724853515625
loss_r_bn_feature 266.6724853515625
------------iteration 300----------
total loss 410.97955725636075
main criterion 141.57782385792322
weighted_aux_loss 269.4017333984375
loss_r_bn_feature 269.4017333984375
------------iteration 400----------
total loss 273.999765291667
main criterion 92.11541165397168
weighted_aux_loss 181.8843536376953
loss_r_bn_feature 181.8843536376953
------------iteration 500----------
total loss 289.40099646945134
main criterion 109.68293311495914
weighted_aux_loss 179.7180633544922
loss_r_bn_feature 179.7180633544922
------------iteration 600----------
total loss 247.3501480742727
main criterion 96.86269079888208
weighted_aux_loss 150.48745727539062
loss_r_bn_feature 150.48745727539062
------------iteration 700----------
total loss 318.7229336663293
main criterion 142.84701813898556
weighted_aux_loss 175.87591552734375
loss_r_bn_feature 175.87591552734375
------------iteration 800----------
total loss 239.15564640696869
main criterion 96.14006718333589
weighted_aux_loss 143.0155792236328
loss_r_bn_feature 143.0155792236328
------------iteration 900----------
total loss 206.67682030169112
main criterion 86.05688621965987
weighted_aux_loss 120.61993408203125
loss_r_bn_feature 120.61993408203125
------------iteration 1000----------
total loss 176.84569585229497
main criterion 78.06978642846684
weighted_aux_loss 98.77590942382812
loss_r_bn_feature 98.77590942382812
------------iteration 1100----------
total loss 217.70806341841626
main criterion 106.41084509566235
weighted_aux_loss 111.2972183227539
loss_r_bn_feature 111.2972183227539
------------iteration 1200----------
total loss 365.5536124073036
main criterion 170.53624790535045
weighted_aux_loss 195.01736450195312
loss_r_bn_feature 195.01736450195312
------------iteration 1300----------
total loss 138.96793109158165
main criterion 66.33087664822226
weighted_aux_loss 72.63705444335938
loss_r_bn_feature 72.63705444335938
------------iteration 1400----------
total loss 147.7235985049241
main criterion 69.15428545560768
weighted_aux_loss 78.5693130493164
loss_r_bn_feature 78.5693130493164
------------iteration 1500----------
total loss 136.34310680795477
main criterion 72.98877102304267
weighted_aux_loss 63.35433578491211
loss_r_bn_feature 63.35433578491211
------------iteration 1600----------
total loss 138.59414403352966
main criterion 73.91586034212341
weighted_aux_loss 64.67828369140625
loss_r_bn_feature 64.67828369140625
------------iteration 1700----------
total loss 141.8575654498726
main criterion 75.12122206608352
weighted_aux_loss 66.73634338378906
loss_r_bn_feature 66.73634338378906
------------iteration 1800----------
total loss 113.02741780427804
main criterion 57.75225606111398
weighted_aux_loss 55.27516174316406
loss_r_bn_feature 55.27516174316406
------------iteration 1900----------
total loss 118.90181582121241
main criterion 60.31015627531398
weighted_aux_loss 58.59165954589844
loss_r_bn_feature 58.59165954589844
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 130 end_cls 140
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 690.9567852267496
main criterion 215.6056804904215
weighted_aux_loss 475.3511047363281
loss_r_bn_feature 475.3511047363281
------------iteration 100----------
total loss 388.5274323339638
main criterion 119.84560860349505
weighted_aux_loss 268.68182373046875
loss_r_bn_feature 268.68182373046875
------------iteration 200----------
total loss 355.91694971287944
main criterion 119.30696436131694
weighted_aux_loss 236.6099853515625
loss_r_bn_feature 236.6099853515625
------------iteration 300----------
total loss 296.2950828109527
main criterion 103.31096721036677
weighted_aux_loss 192.98411560058594
loss_r_bn_feature 192.98411560058594
------------iteration 400----------
total loss 250.92110698932407
main criterion 88.17405193561315
weighted_aux_loss 162.74705505371094
loss_r_bn_feature 162.74705505371094
------------iteration 500----------
total loss 420.6914332283026
main criterion 181.49773815994325
weighted_aux_loss 239.19369506835938
loss_r_bn_feature 239.19369506835938
------------iteration 600----------
total loss 344.994059781184
main criterion 137.45980379973867
weighted_aux_loss 207.5342559814453
loss_r_bn_feature 207.5342559814453
------------iteration 700----------
total loss 233.3957781279701
main criterion 91.36974663382946
weighted_aux_loss 142.02603149414062
loss_r_bn_feature 142.02603149414062
------------iteration 800----------
total loss 279.532458979762
main criterion 127.15082140651982
weighted_aux_loss 152.3816375732422
loss_r_bn_feature 152.3816375732422
------------iteration 900----------
total loss 198.5656596209754
main criterion 80.57809553406132
weighted_aux_loss 117.98756408691406
loss_r_bn_feature 117.98756408691406
------------iteration 1000----------
total loss 249.17719407784153
main criterion 121.25551744209935
weighted_aux_loss 127.92167663574219
loss_r_bn_feature 127.92167663574219
------------iteration 1100----------
total loss 228.91405930706634
main criterion 109.36769547649995
weighted_aux_loss 119.5463638305664
loss_r_bn_feature 119.5463638305664
------------iteration 1200----------
total loss 180.1914748399978
main criterion 83.8480452745681
weighted_aux_loss 96.34342956542969
loss_r_bn_feature 96.34342956542969
------------iteration 1300----------
total loss 200.27864473185525
main criterion 103.88343446574196
weighted_aux_loss 96.39521026611328
loss_r_bn_feature 96.39521026611328
------------iteration 1400----------
total loss 320.23924142799336
main criterion 154.21504098854024
weighted_aux_loss 166.02420043945312
loss_r_bn_feature 166.02420043945312
------------iteration 1500----------
total loss 128.78966262934563
main criterion 65.59230545161124
weighted_aux_loss 63.197357177734375
loss_r_bn_feature 63.197357177734375
------------iteration 1600----------
total loss 105.46027482371456
main criterion 54.85093415598995
weighted_aux_loss 50.60934066772461
loss_r_bn_feature 50.60934066772461
------------iteration 1700----------
total loss 98.3305246947692
main criterion 54.400993597357086
weighted_aux_loss 43.92953109741211
loss_r_bn_feature 43.92953109741211
------------iteration 1800----------
total loss 100.16077768301088
main criterion 55.0689998814972
weighted_aux_loss 45.09177780151367
loss_r_bn_feature 45.09177780151367
------------iteration 1900----------
total loss 105.27309556776954
main criterion 55.21139283949806
weighted_aux_loss 50.061702728271484
loss_r_bn_feature 50.061702728271484
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 140 end_cls 150
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 709.969957527213
main criterion 209.83546656041614
weighted_aux_loss 500.1344909667969
loss_r_bn_feature 500.1344909667969
------------iteration 100----------
total loss 493.6478771709528
main criterion 159.10457272759345
weighted_aux_loss 334.5433044433594
loss_r_bn_feature 334.5433044433594
------------iteration 200----------
total loss 465.70287846897776
main criterion 164.05166386936838
weighted_aux_loss 301.6512145996094
loss_r_bn_feature 301.6512145996094
------------iteration 300----------
total loss 391.4802270623723
main criterion 153.10333497252856
weighted_aux_loss 238.37689208984375
loss_r_bn_feature 238.37689208984375
------------iteration 400----------
total loss 333.63756423494027
main criterion 121.39308791658088
weighted_aux_loss 212.24447631835938
loss_r_bn_feature 212.24447631835938
------------iteration 500----------
total loss 266.295622121566
main criterion 89.37002397703473
weighted_aux_loss 176.92559814453125
loss_r_bn_feature 176.92559814453125
------------iteration 600----------
total loss 306.9267087094875
main criterion 114.26450778175315
weighted_aux_loss 192.66220092773438
loss_r_bn_feature 192.66220092773438
------------iteration 700----------
total loss 326.78029308247983
main criterion 132.2461744301361
weighted_aux_loss 194.53411865234375
loss_r_bn_feature 194.53411865234375
------------iteration 800----------
total loss 232.23131612369977
main criterion 85.0273671490904
weighted_aux_loss 147.20394897460938
loss_r_bn_feature 147.20394897460938
------------iteration 900----------
total loss 212.7156290406979
main criterion 81.99941199968228
weighted_aux_loss 130.71621704101562
loss_r_bn_feature 130.71621704101562
------------iteration 1000----------
total loss 225.5181554652175
main criterion 96.34150446424093
weighted_aux_loss 129.17665100097656
loss_r_bn_feature 129.17665100097656
------------iteration 1100----------
total loss 171.38965813143062
main criterion 73.60274712068842
weighted_aux_loss 97.78691101074219
loss_r_bn_feature 97.78691101074219
------------iteration 1200----------
total loss 193.81617157567987
main criterion 86.75157349218378
weighted_aux_loss 107.0645980834961
loss_r_bn_feature 107.0645980834961
------------iteration 1300----------
total loss 158.94817043199487
main criterion 76.1461303318972
weighted_aux_loss 82.80204010009766
loss_r_bn_feature 82.80204010009766
------------iteration 1400----------
total loss 354.3938060297203
main criterion 165.24240832464218
weighted_aux_loss 189.15139770507812
loss_r_bn_feature 189.15139770507812
------------iteration 1500----------
total loss 127.95681020375417
main criterion 64.61441823598074
weighted_aux_loss 63.34239196777344
loss_r_bn_feature 63.34239196777344
------------iteration 1600----------
total loss 178.9375128140938
main criterion 95.84437316565631
weighted_aux_loss 83.0931396484375
loss_r_bn_feature 83.0931396484375
------------iteration 1700----------
total loss 106.465374488567
main criterion 58.804527778361916
weighted_aux_loss 47.66084671020508
loss_r_bn_feature 47.66084671020508
------------iteration 1800----------
total loss 172.85294699925595
main criterion 94.64132285374815
weighted_aux_loss 78.21162414550781
loss_r_bn_feature 78.21162414550781
------------iteration 1900----------
total loss 107.60873871193661
main criterion 55.04724960671199
weighted_aux_loss 52.56148910522461
loss_r_bn_feature 52.56148910522461
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 150 end_cls 160
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 740.8669482060545
main criterion 208.492314416992
weighted_aux_loss 532.3746337890625
loss_r_bn_feature 532.3746337890625
------------iteration 100----------
total loss 644.5874297376209
main criterion 191.20785210090216
weighted_aux_loss 453.37957763671875
loss_r_bn_feature 453.37957763671875
------------iteration 200----------
total loss 348.19619303969665
main criterion 91.68578654555603
weighted_aux_loss 256.5104064941406
loss_r_bn_feature 256.5104064941406
------------iteration 300----------
total loss 297.3295360789808
main criterion 91.50217401843395
weighted_aux_loss 205.82736206054688
loss_r_bn_feature 205.82736206054688
------------iteration 400----------
total loss 270.07860794847824
main criterion 88.6256965715251
weighted_aux_loss 181.45291137695312
loss_r_bn_feature 181.45291137695312
------------iteration 500----------
total loss 247.9393071245322
main criterion 83.63759508839938
weighted_aux_loss 164.3017120361328
loss_r_bn_feature 164.3017120361328
------------iteration 600----------
total loss 249.4207864382637
main criterion 92.45688873318555
weighted_aux_loss 156.96389770507812
loss_r_bn_feature 156.96389770507812
------------iteration 700----------
total loss 234.8524868660612
main criterion 84.5012905769987
weighted_aux_loss 150.3511962890625
loss_r_bn_feature 150.3511962890625
------------iteration 800----------
total loss 242.09502165604212
main criterion 90.90417998123743
weighted_aux_loss 151.1908416748047
loss_r_bn_feature 151.1908416748047
------------iteration 900----------
total loss 207.11281603184005
main criterion 78.79080980625413
weighted_aux_loss 128.32200622558594
loss_r_bn_feature 128.32200622558594
------------iteration 1000----------
total loss 195.55172574213492
main criterion 73.91293942621697
weighted_aux_loss 121.63878631591797
loss_r_bn_feature 121.63878631591797
------------iteration 1100----------
total loss 195.1353168851952
main criterion 79.9875889188866
weighted_aux_loss 115.1477279663086
loss_r_bn_feature 115.1477279663086
------------iteration 1200----------
total loss 183.21778693569215
main criterion 84.6619168795398
weighted_aux_loss 98.55587005615234
loss_r_bn_feature 98.55587005615234
------------iteration 1300----------
total loss 165.53803927436127
main criterion 75.30518252387299
weighted_aux_loss 90.23285675048828
loss_r_bn_feature 90.23285675048828
------------iteration 1400----------
total loss 153.40896658983422
main criterion 75.96253256883814
weighted_aux_loss 77.4464340209961
loss_r_bn_feature 77.4464340209961
------------iteration 1500----------
total loss 121.4987227957559
main criterion 59.29177546909575
weighted_aux_loss 62.206947326660156
loss_r_bn_feature 62.206947326660156
------------iteration 1600----------
total loss 106.6213058732487
main criterion 56.11793749556316
weighted_aux_loss 50.50336837768555
loss_r_bn_feature 50.50336837768555
------------iteration 1700----------
total loss 116.86215965204107
main criterion 57.88744346551764
weighted_aux_loss 58.97471618652344
loss_r_bn_feature 58.97471618652344
------------iteration 1800----------
total loss 93.5214753552961
main criterion 52.36345533576485
weighted_aux_loss 41.15802001953125
loss_r_bn_feature 41.15802001953125
------------iteration 1900----------
total loss 170.6199010042922
main criterion 85.57803088710469
weighted_aux_loss 85.0418701171875
loss_r_bn_feature 85.0418701171875
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 160 end_cls 170
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 746.5721562356026
main criterion 211.91224412622762
weighted_aux_loss 534.659912109375
loss_r_bn_feature 534.659912109375
------------iteration 100----------
total loss 402.2492351361676
main criterion 96.56957815374574
weighted_aux_loss 305.6796569824219
loss_r_bn_feature 305.6796569824219
------------iteration 200----------
total loss 320.2138945136518
main criterion 93.93735947947206
weighted_aux_loss 226.2765350341797
loss_r_bn_feature 226.2765350341797
------------iteration 300----------
total loss 442.05032965873977
main criterion 178.5889038774898
weighted_aux_loss 263.46142578125
loss_r_bn_feature 263.46142578125
------------iteration 400----------
total loss 279.1859032205332
main criterion 98.41655507600196
weighted_aux_loss 180.76934814453125
loss_r_bn_feature 180.76934814453125
------------iteration 500----------
total loss 275.03184805276743
main criterion 95.00278005960335
weighted_aux_loss 180.02906799316406
loss_r_bn_feature 180.02906799316406
------------iteration 600----------
total loss 362.58630309236696
main criterion 155.50369200838261
weighted_aux_loss 207.08261108398438
loss_r_bn_feature 207.08261108398438
------------iteration 700----------
total loss 225.05135405650742
main criterion 82.14240325084334
weighted_aux_loss 142.90895080566406
loss_r_bn_feature 142.90895080566406
------------iteration 800----------
total loss 248.31733213115425
main criterion 103.66698728252143
weighted_aux_loss 144.6503448486328
loss_r_bn_feature 144.6503448486328
------------iteration 900----------
total loss 205.79522775316886
main criterion 83.08985208178214
weighted_aux_loss 122.70537567138672
loss_r_bn_feature 122.70537567138672
------------iteration 1000----------
total loss 187.69869438917675
main criterion 78.43348900587598
weighted_aux_loss 109.26520538330078
loss_r_bn_feature 109.26520538330078
------------iteration 1100----------
total loss 175.15204202875066
main criterion 76.38473856195377
weighted_aux_loss 98.76730346679688
loss_r_bn_feature 98.76730346679688
------------iteration 1200----------
total loss 164.3939556197867
main criterion 74.32734337613438
weighted_aux_loss 90.06661224365234
loss_r_bn_feature 90.06661224365234
------------iteration 1300----------
total loss 158.88252370821118
main criterion 71.96695058809398
weighted_aux_loss 86.91557312011719
loss_r_bn_feature 86.91557312011719
------------iteration 1400----------
total loss 132.9462733291722
main criterion 65.55207014313702
weighted_aux_loss 67.39420318603516
loss_r_bn_feature 67.39420318603516
------------iteration 1500----------
total loss 198.53331008928654
main criterion 104.44876876848575
weighted_aux_loss 94.08454132080078
loss_r_bn_feature 94.08454132080078
------------iteration 1600----------
total loss 145.52566248757304
main criterion 64.32866389138164
weighted_aux_loss 81.1969985961914
loss_r_bn_feature 81.1969985961914
------------iteration 1700----------
total loss 130.57921448349106
main criterion 66.0172409117137
weighted_aux_loss 64.56197357177734
loss_r_bn_feature 64.56197357177734
------------iteration 1800----------
total loss 143.1471344656781
main criterion 80.15558783481875
weighted_aux_loss 62.991546630859375
loss_r_bn_feature 62.991546630859375
------------iteration 1900----------
total loss 100.04093318482154
main criterion 57.56397395630591
weighted_aux_loss 42.476959228515625
loss_r_bn_feature 42.476959228515625
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 170 end_cls 180
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 797.6911884319111
main criterion 211.62514839284862
weighted_aux_loss 586.0660400390625
loss_r_bn_feature 586.0660400390625
------------iteration 100----------
total loss 629.7899432025799
main criterion 197.04348324164246
weighted_aux_loss 432.7464599609375
loss_r_bn_feature 432.7464599609375
------------iteration 200----------
total loss 409.890495572194
main criterion 142.12630489836593
weighted_aux_loss 267.7641906738281
loss_r_bn_feature 267.7641906738281
------------iteration 300----------
total loss 297.9833318411916
main criterion 97.31705681677755
weighted_aux_loss 200.66627502441406
loss_r_bn_feature 200.66627502441406
------------iteration 400----------
total loss 278.0397725442193
main criterion 98.02721456082087
weighted_aux_loss 180.01255798339844
loss_r_bn_feature 180.01255798339844
------------iteration 500----------
total loss 260.8742101873165
main criterion 90.1780584539181
weighted_aux_loss 170.69615173339844
loss_r_bn_feature 170.69615173339844
------------iteration 600----------
total loss 253.06213584570992
main criterion 89.03074851660837
weighted_aux_loss 164.03138732910156
loss_r_bn_feature 164.03138732910156
------------iteration 700----------
total loss 362.74755395363843
main criterion 163.23568261574778
weighted_aux_loss 199.51187133789062
loss_r_bn_feature 199.51187133789062
------------iteration 800----------
total loss 369.1179758719292
main criterion 158.46537272251516
weighted_aux_loss 210.65260314941406
loss_r_bn_feature 210.65260314941406
------------iteration 900----------
total loss 246.45130261480955
main criterion 115.03963574469235
weighted_aux_loss 131.4116668701172
loss_r_bn_feature 131.4116668701172
------------iteration 1000----------
total loss 223.8073869688875
main criterion 104.30726489857499
weighted_aux_loss 119.5001220703125
loss_r_bn_feature 119.5001220703125
------------iteration 1100----------
total loss 176.62701607474958
main criterion 75.27578163871442
weighted_aux_loss 101.35123443603516
loss_r_bn_feature 101.35123443603516
------------iteration 1200----------
total loss 165.55707237407972
main criterion 78.54475853130629
weighted_aux_loss 87.01231384277344
loss_r_bn_feature 87.01231384277344
------------iteration 1300----------
total loss 158.85427048466266
main criterion 69.46607926151815
weighted_aux_loss 89.38819122314453
loss_r_bn_feature 89.38819122314453
------------iteration 1400----------
total loss 145.6528045049778
main criterion 71.53425897275125
weighted_aux_loss 74.11854553222656
loss_r_bn_feature 74.11854553222656
------------iteration 1500----------
total loss 169.8241412364547
main criterion 93.84420654407188
weighted_aux_loss 75.97993469238281
loss_r_bn_feature 75.97993469238281
------------iteration 1600----------
total loss 265.4127722799103
main criterion 127.4636450826447
weighted_aux_loss 137.94912719726562
loss_r_bn_feature 137.94912719726562
------------iteration 1700----------
total loss 133.95892207911237
main criterion 77.93490093043073
weighted_aux_loss 56.02402114868164
loss_r_bn_feature 56.02402114868164
------------iteration 1800----------
total loss 98.52527174673867
main criterion 55.65788969717811
weighted_aux_loss 42.86738204956055
loss_r_bn_feature 42.86738204956055
------------iteration 1900----------
total loss 97.84860087592078
main criterion 57.26891566473914
weighted_aux_loss 40.57968521118164
loss_r_bn_feature 40.57968521118164
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 180 end_cls 190
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 761.7818124826513
main criterion 212.64954929905758
weighted_aux_loss 549.1322631835938
loss_r_bn_feature 549.1322631835938
------------iteration 100----------
total loss 438.71913248292947
main criterion 110.79231363527323
weighted_aux_loss 327.92681884765625
loss_r_bn_feature 327.92681884765625
------------iteration 200----------
total loss 331.6240106030076
main criterion 95.51018003660138
weighted_aux_loss 236.11383056640625
loss_r_bn_feature 236.11383056640625
------------iteration 300----------
total loss 398.70753890461106
main criterion 171.4693492073454
weighted_aux_loss 227.23818969726562
loss_r_bn_feature 227.23818969726562
------------iteration 400----------
total loss 277.5244120849935
main criterion 96.92640488284508
weighted_aux_loss 180.59800720214844
loss_r_bn_feature 180.59800720214844
------------iteration 500----------
total loss 249.75842210123872
main criterion 89.85331651041841
weighted_aux_loss 159.9051055908203
loss_r_bn_feature 159.9051055908203
------------iteration 600----------
total loss 274.1706901719105
main criterion 96.37767564554332
weighted_aux_loss 177.7930145263672
loss_r_bn_feature 177.7930145263672
------------iteration 700----------
total loss 367.2938070421959
main criterion 158.40762234981312
weighted_aux_loss 208.8861846923828
loss_r_bn_feature 208.8861846923828
------------iteration 800----------
total loss 301.71299727154485
main criterion 127.31491072369332
weighted_aux_loss 174.39808654785156
loss_r_bn_feature 174.39808654785156
------------iteration 900----------
total loss 223.20735868241798
main criterion 99.75923093583596
weighted_aux_loss 123.44812774658203
loss_r_bn_feature 123.44812774658203
------------iteration 1000----------
total loss 180.84818580500342
main criterion 74.26508643976906
weighted_aux_loss 106.58309936523438
loss_r_bn_feature 106.58309936523438
------------iteration 1100----------
total loss 173.96269811808335
main criterion 77.34938635050523
weighted_aux_loss 96.61331176757812
loss_r_bn_feature 96.61331176757812
------------iteration 1200----------
total loss 189.41329936761025
main criterion 83.20191173333292
weighted_aux_loss 106.21138763427734
loss_r_bn_feature 106.21138763427734
------------iteration 1300----------
total loss 159.09583886651552
main criterion 77.93633111505068
weighted_aux_loss 81.15950775146484
loss_r_bn_feature 81.15950775146484
------------iteration 1400----------
total loss 154.4983825387213
main criterion 81.6995544137213
weighted_aux_loss 72.798828125
loss_r_bn_feature 72.798828125
------------iteration 1500----------
total loss 130.0320684221105
main criterion 63.72487085130974
weighted_aux_loss 66.30719757080078
loss_r_bn_feature 66.30719757080078
------------iteration 1600----------
total loss 115.58836506643222
main criterion 64.0681471709244
weighted_aux_loss 51.52021789550781
loss_r_bn_feature 51.52021789550781
------------iteration 1700----------
total loss 101.59081565071153
main criterion 57.33762656379748
weighted_aux_loss 44.25318908691406
loss_r_bn_feature 44.25318908691406
------------iteration 1800----------
total loss 108.89804499721777
main criterion 62.123230391504876
weighted_aux_loss 46.77481460571289
loss_r_bn_feature 46.77481460571289
------------iteration 1900----------
total loss 89.82195136573753
main criterion 50.00589606788596
weighted_aux_loss 39.81605529785156
loss_r_bn_feature 39.81605529785156
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 190 end_cls 200
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 744.2381879046063
main criterion 216.42727481866882
weighted_aux_loss 527.8109130859375
loss_r_bn_feature 527.8109130859375
------------iteration 100----------
total loss 430.74528564095465
main criterion 99.6420751917359
weighted_aux_loss 331.10321044921875
loss_r_bn_feature 331.10321044921875
------------iteration 200----------
total loss 455.9376036108822
main criterion 170.49375595463218
weighted_aux_loss 285.44384765625
loss_r_bn_feature 285.44384765625
------------iteration 300----------
total loss 298.3638297587346
main criterion 94.27166667279711
weighted_aux_loss 204.0921630859375
loss_r_bn_feature 204.0921630859375
------------iteration 400----------
total loss 276.8076587116137
main criterion 94.00962404364499
weighted_aux_loss 182.79803466796875
loss_r_bn_feature 182.79803466796875
------------iteration 500----------
total loss 385.3979228541698
main criterion 163.0766185328807
weighted_aux_loss 222.32130432128906
loss_r_bn_feature 222.32130432128906
------------iteration 600----------
total loss 249.90148136076957
main criterion 93.1141125863555
weighted_aux_loss 156.78736877441406
loss_r_bn_feature 156.78736877441406
------------iteration 700----------
total loss 285.2643689991593
main criterion 109.49070261732331
weighted_aux_loss 175.77366638183594
loss_r_bn_feature 175.77366638183594
------------iteration 800----------
total loss 210.79314412398617
main criterion 83.27390279097837
weighted_aux_loss 127.51924133300781
loss_r_bn_feature 127.51924133300781
------------iteration 900----------
total loss 229.75762411446604
main criterion 100.98668905587229
weighted_aux_loss 128.77093505859375
loss_r_bn_feature 128.77093505859375
------------iteration 1000----------
total loss 247.44862606342866
main criterion 108.07982113178804
weighted_aux_loss 139.36880493164062
loss_r_bn_feature 139.36880493164062
------------iteration 1100----------
total loss 184.98843895118912
main criterion 81.76997123878678
weighted_aux_loss 103.21846771240234
loss_r_bn_feature 103.21846771240234
------------iteration 1200----------
total loss 177.6431037762988
main criterion 81.14534681829097
weighted_aux_loss 96.49775695800781
loss_r_bn_feature 96.49775695800781
------------iteration 1300----------
total loss 159.46347740784682
main criterion 76.67214897767103
weighted_aux_loss 82.79132843017578
loss_r_bn_feature 82.79132843017578
------------iteration 1400----------
total loss 138.01260428128518
main criterion 66.26738028226174
weighted_aux_loss 71.74522399902344
loss_r_bn_feature 71.74522399902344
------------iteration 1500----------
total loss 121.88688637777221
main criterion 62.62709404989135
weighted_aux_loss 59.25979232788086
loss_r_bn_feature 59.25979232788086
------------iteration 1600----------
total loss 106.13371843762957
main criterion 57.72767061658464
weighted_aux_loss 48.40604782104492
loss_r_bn_feature 48.40604782104492
------------iteration 1700----------
total loss 134.61370997925852
main criterion 75.11289744874094
weighted_aux_loss 59.50081253051758
loss_r_bn_feature 59.50081253051758
------------iteration 1800----------
total loss 96.79141540762396
main criterion 54.04435272451849
weighted_aux_loss 42.74706268310547
loss_r_bn_feature 42.74706268310547
------------iteration 1900----------
total loss 239.28433088001216
main criterion 121.360342537727
weighted_aux_loss 117.92398834228516
loss_r_bn_feature 117.92398834228516
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 200 end_cls 210
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 723.5580753850453
main criterion 204.1701359319203
weighted_aux_loss 519.387939453125
loss_r_bn_feature 519.387939453125
------------iteration 100----------
total loss 466.5410579885355
main criterion 108.85810510767612
weighted_aux_loss 357.6829528808594
loss_r_bn_feature 357.6829528808594
------------iteration 200----------
total loss 422.42673457315726
main criterion 141.51108515909476
weighted_aux_loss 280.9156494140625
loss_r_bn_feature 280.9156494140625
------------iteration 300----------
total loss 292.68077539516196
main criterion 91.37349390102133
weighted_aux_loss 201.30728149414062
loss_r_bn_feature 201.30728149414062
------------iteration 400----------
total loss 279.6692861651911
main criterion 96.34116116519111
weighted_aux_loss 183.328125
loss_r_bn_feature 183.328125
------------iteration 500----------
total loss 249.96095674136893
main criterion 87.67851655582204
weighted_aux_loss 162.28244018554688
loss_r_bn_feature 162.28244018554688
------------iteration 600----------
total loss 372.057490287984
main criterion 153.71500676747618
weighted_aux_loss 218.3424835205078
loss_r_bn_feature 218.3424835205078
------------iteration 700----------
total loss 287.2336774152319
main criterion 112.64769413886472
weighted_aux_loss 174.5859832763672
loss_r_bn_feature 174.5859832763672
------------iteration 800----------
total loss 374.9973297340156
main criterion 166.71391298596873
weighted_aux_loss 208.28341674804688
loss_r_bn_feature 208.28341674804688
------------iteration 900----------
total loss 279.8562107199428
main criterion 115.80428506076312
weighted_aux_loss 164.0519256591797
loss_r_bn_feature 164.0519256591797
------------iteration 1000----------
total loss 229.23023537862287
main criterion 103.23162392842754
weighted_aux_loss 125.99861145019531
loss_r_bn_feature 125.99861145019531
------------iteration 1100----------
total loss 170.72808718965274
main criterion 72.66023898408632
weighted_aux_loss 98.0678482055664
loss_r_bn_feature 98.0678482055664
------------iteration 1200----------
total loss 177.6615184809247
main criterion 84.14490928903017
weighted_aux_loss 93.51660919189453
loss_r_bn_feature 93.51660919189453
------------iteration 1300----------
total loss 173.55433122558628
main criterion 81.02562944335972
weighted_aux_loss 92.52870178222656
loss_r_bn_feature 92.52870178222656
------------iteration 1400----------
total loss 305.5708446390288
main criterion 143.23902701207564
weighted_aux_loss 162.33181762695312
loss_r_bn_feature 162.33181762695312
------------iteration 1500----------
total loss 287.06360690180185
main criterion 135.16239230219247
weighted_aux_loss 151.90121459960938
loss_r_bn_feature 151.90121459960938
------------iteration 1600----------
total loss 115.56748501327989
main criterion 64.27354733017442
weighted_aux_loss 51.29393768310547
loss_r_bn_feature 51.29393768310547
------------iteration 1700----------
total loss 102.98671128598986
main criterion 57.149484418314074
weighted_aux_loss 45.83722686767578
loss_r_bn_feature 45.83722686767578
------------iteration 1800----------
total loss 96.90961704046941
main criterion 55.23097858221746
weighted_aux_loss 41.67863845825195
loss_r_bn_feature 41.67863845825195
------------iteration 1900----------
total loss 104.73979387788329
main criterion 57.93147478608641
weighted_aux_loss 46.808319091796875
loss_r_bn_feature 46.808319091796875
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 210 end_cls 220
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 759.4320250275716
main criterion 214.3585386994466
weighted_aux_loss 545.073486328125
loss_r_bn_feature 545.073486328125
------------iteration 100----------
total loss 472.0880367898559
main criterion 131.3718502664184
weighted_aux_loss 340.7161865234375
loss_r_bn_feature 340.7161865234375
------------iteration 200----------
total loss 429.8944181853606
main criterion 152.26001877129812
weighted_aux_loss 277.6343994140625
loss_r_bn_feature 277.6343994140625
------------iteration 300----------
total loss 309.7280012214451
main criterion 114.48497448804665
weighted_aux_loss 195.24302673339844
loss_r_bn_feature 195.24302673339844
------------iteration 400----------
total loss 265.0708864445921
main criterion 90.86870748951397
weighted_aux_loss 174.20217895507812
loss_r_bn_feature 174.20217895507812
------------iteration 500----------
total loss 255.2875446681029
main criterion 89.17740672864979
weighted_aux_loss 166.11013793945312
loss_r_bn_feature 166.11013793945312
------------iteration 600----------
total loss 241.57760717305513
main criterion 90.41333105000827
weighted_aux_loss 151.16427612304688
loss_r_bn_feature 151.16427612304688
------------iteration 700----------
total loss 362.88842206149076
main criterion 156.0029087558267
weighted_aux_loss 206.88551330566406
loss_r_bn_feature 206.88551330566406
------------iteration 800----------
total loss 234.67633431264522
main criterion 91.80498116323116
weighted_aux_loss 142.87135314941406
loss_r_bn_feature 142.87135314941406
------------iteration 900----------
total loss 242.8070139650381
main criterion 94.57591960468653
weighted_aux_loss 148.23109436035156
loss_r_bn_feature 148.23109436035156
------------iteration 1000----------
total loss 185.57502886382827
main criterion 76.45398851958998
weighted_aux_loss 109.12104034423828
loss_r_bn_feature 109.12104034423828
------------iteration 1100----------
total loss 175.01147822998809
main criterion 74.73258571289823
weighted_aux_loss 100.27889251708984
loss_r_bn_feature 100.27889251708984
------------iteration 1200----------
total loss 253.3473032081808
main criterion 114.82041722185265
weighted_aux_loss 138.52688598632812
loss_r_bn_feature 138.52688598632812
------------iteration 1300----------
total loss 148.07323015878114
main criterion 71.77389086434756
weighted_aux_loss 76.2993392944336
loss_r_bn_feature 76.2993392944336
------------iteration 1400----------
total loss 152.45673961652338
main criterion 79.20950908673822
weighted_aux_loss 73.24723052978516
loss_r_bn_feature 73.24723052978516
------------iteration 1500----------
total loss 123.17055475702183
main criterion 65.00313913812535
weighted_aux_loss 58.167415618896484
loss_r_bn_feature 58.167415618896484
------------iteration 1600----------
total loss 122.73909379232963
main criterion 62.08144379843314
weighted_aux_loss 60.657649993896484
loss_r_bn_feature 60.657649993896484
------------iteration 1700----------
total loss 107.4958618801331
main criterion 61.70734488184208
weighted_aux_loss 45.788516998291016
loss_r_bn_feature 45.788516998291016
------------iteration 1800----------
total loss 94.51895532780662
main criterion 53.2047387140371
weighted_aux_loss 41.31421661376953
loss_r_bn_feature 41.31421661376953
------------iteration 1900----------
total loss 254.06810780381676
main criterion 116.01520558213706
weighted_aux_loss 138.0529022216797
loss_r_bn_feature 138.0529022216797
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 220 end_cls 230
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 740.2446890016329
main criterion 213.6536245485079
weighted_aux_loss 526.591064453125
loss_r_bn_feature 526.591064453125
------------iteration 100----------
total loss 444.39449137921866
main criterion 112.7639371799999
weighted_aux_loss 331.63055419921875
loss_r_bn_feature 331.63055419921875
------------iteration 200----------
total loss 356.6636861006477
main criterion 102.93961078326487
weighted_aux_loss 253.7240753173828
loss_r_bn_feature 253.7240753173828
------------iteration 300----------
total loss 379.80102586838365
main criterion 144.74006700607896
weighted_aux_loss 235.0609588623047
loss_r_bn_feature 235.0609588623047
------------iteration 400----------
total loss 283.9184561960971
main criterion 96.50744545390961
weighted_aux_loss 187.4110107421875
loss_r_bn_feature 187.4110107421875
------------iteration 500----------
total loss 260.4579190310948
main criterion 92.08728304476665
weighted_aux_loss 168.37063598632812
loss_r_bn_feature 168.37063598632812
------------iteration 600----------
total loss 337.81843757716354
main criterion 143.07420539942916
weighted_aux_loss 194.74423217773438
loss_r_bn_feature 194.74423217773438
------------iteration 700----------
total loss 223.18104165648697
main criterion 83.09017556761978
weighted_aux_loss 140.0908660888672
loss_r_bn_feature 140.0908660888672
------------iteration 800----------
total loss 208.12955251057448
main criterion 82.32199635823073
weighted_aux_loss 125.80755615234375
loss_r_bn_feature 125.80755615234375
------------iteration 900----------
total loss 314.9362383503852
main criterion 148.74065119218207
weighted_aux_loss 166.19558715820312
loss_r_bn_feature 166.19558715820312
------------iteration 1000----------
total loss 187.29304119391944
main criterion 78.384975398021
weighted_aux_loss 108.90806579589844
loss_r_bn_feature 108.90806579589844
------------iteration 1100----------
total loss 175.15720926483874
main criterion 77.66579233368638
weighted_aux_loss 97.49141693115234
loss_r_bn_feature 97.49141693115234
------------iteration 1200----------
total loss 322.3955830381548
main criterion 152.83514297467826
weighted_aux_loss 169.56044006347656
loss_r_bn_feature 169.56044006347656
------------iteration 1300----------
total loss 148.80229953613926
main criterion 68.5489883789127
weighted_aux_loss 80.25331115722656
loss_r_bn_feature 80.25331115722656
------------iteration 1400----------
total loss 187.5764084334921
main criterion 96.86840825038664
weighted_aux_loss 90.70800018310547
loss_r_bn_feature 90.70800018310547
------------iteration 1500----------
total loss 137.76463182141973
main criterion 68.99689347913457
weighted_aux_loss 68.76773834228516
loss_r_bn_feature 68.76773834228516
------------iteration 1600----------
total loss 114.71263731156427
main criterion 58.65491331254082
weighted_aux_loss 56.05772399902344
loss_r_bn_feature 56.05772399902344
------------iteration 1700----------
total loss 144.6664725591088
main criterion 80.72929680837636
weighted_aux_loss 63.93717575073242
loss_r_bn_feature 63.93717575073242
------------iteration 1800----------
total loss 116.74811532963767
main criterion 65.73382547368064
weighted_aux_loss 51.01428985595703
loss_r_bn_feature 51.01428985595703
------------iteration 1900----------
total loss 122.28508299152257
main criterion 70.75848692218663
weighted_aux_loss 51.52659606933594
loss_r_bn_feature 51.52659606933594
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 230 end_cls 240
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 760.7801146358684
main criterion 207.80190418664967
weighted_aux_loss 552.9782104492188
loss_r_bn_feature 552.9782104492188
------------iteration 100----------
total loss 443.4792908345056
main criterion 95.69727789505245
weighted_aux_loss 347.7820129394531
loss_r_bn_feature 347.7820129394531
------------iteration 200----------
total loss 341.29941420724333
main criterion 88.63565688302458
weighted_aux_loss 252.66375732421875
loss_r_bn_feature 252.66375732421875
------------iteration 300----------
total loss 330.1004974363881
main criterion 104.02963561998185
weighted_aux_loss 226.07086181640625
loss_r_bn_feature 226.07086181640625
------------iteration 400----------
total loss 309.75978088773826
main criterion 111.62471008695702
weighted_aux_loss 198.13507080078125
loss_r_bn_feature 198.13507080078125
------------iteration 500----------
total loss 242.73980318738046
main criterion 80.6395224256617
weighted_aux_loss 162.10028076171875
loss_r_bn_feature 162.10028076171875
------------iteration 600----------
total loss 226.34232426090676
main criterion 83.4257593195005
weighted_aux_loss 142.91656494140625
loss_r_bn_feature 142.91656494140625
------------iteration 700----------
total loss 206.335763602295
main criterion 79.52012792114266
weighted_aux_loss 126.81563568115234
loss_r_bn_feature 126.81563568115234
------------iteration 800----------
total loss 234.02371239285728
main criterion 91.79527306180259
weighted_aux_loss 142.2284393310547
loss_r_bn_feature 142.2284393310547
------------iteration 900----------
total loss 187.40501826536166
main criterion 75.557323868389
weighted_aux_loss 111.84769439697266
loss_r_bn_feature 111.84769439697266
------------iteration 1000----------
total loss 181.4204104234962
main criterion 75.87474849722668
weighted_aux_loss 105.54566192626953
loss_r_bn_feature 105.54566192626953
------------iteration 1100----------
total loss 168.4589469346289
main criterion 71.78260110943357
weighted_aux_loss 96.67634582519531
loss_r_bn_feature 96.67634582519531
------------iteration 1200----------
total loss 403.71710615833723
main criterion 171.19586592396223
weighted_aux_loss 232.521240234375
loss_r_bn_feature 232.521240234375
------------iteration 1300----------
total loss 246.3515852298923
main criterion 120.89617904092744
weighted_aux_loss 125.45540618896484
loss_r_bn_feature 125.45540618896484
------------iteration 1400----------
total loss 148.33494920666345
main criterion 75.46367998058925
weighted_aux_loss 72.87126922607422
loss_r_bn_feature 72.87126922607422
------------iteration 1500----------
total loss 131.3354906470027
main criterion 67.75860160891675
weighted_aux_loss 63.57688903808594
loss_r_bn_feature 63.57688903808594
------------iteration 1600----------
total loss 105.25759764434122
main criterion 55.82517882109902
weighted_aux_loss 49.43241882324219
loss_r_bn_feature 49.43241882324219
------------iteration 1700----------
total loss 100.57835115298215
main criterion 57.781891954984104
weighted_aux_loss 42.79645919799805
loss_r_bn_feature 42.79645919799805
------------iteration 1800----------
total loss 146.86693349799634
main criterion 77.97739950141431
weighted_aux_loss 68.88953399658203
loss_r_bn_feature 68.88953399658203
------------iteration 1900----------
total loss 173.51547413645375
main criterion 93.00082569895375
weighted_aux_loss 80.5146484375
loss_r_bn_feature 80.5146484375
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 240 end_cls 250
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 772.8929534422534
main criterion 210.68134455553468
weighted_aux_loss 562.2116088867188
loss_r_bn_feature 562.2116088867188
------------iteration 100----------
total loss 455.76798759152945
main criterion 119.27122245481071
weighted_aux_loss 336.49676513671875
loss_r_bn_feature 336.49676513671875
------------iteration 200----------
total loss 337.6118760268654
main criterion 89.14127971338883
weighted_aux_loss 248.47059631347656
loss_r_bn_feature 248.47059631347656
------------iteration 300----------
total loss 358.91730130249755
main criterion 123.8757821374585
weighted_aux_loss 235.04151916503906
loss_r_bn_feature 235.04151916503906
------------iteration 400----------
total loss 345.90446611066835
main criterion 133.22555680891054
weighted_aux_loss 212.6789093017578
loss_r_bn_feature 212.6789093017578
------------iteration 500----------
total loss 259.64749183374596
main criterion 88.89329566675376
weighted_aux_loss 170.7541961669922
loss_r_bn_feature 170.7541961669922
------------iteration 600----------
total loss 262.6720285293667
main criterion 101.68281649323387
weighted_aux_loss 160.9892120361328
loss_r_bn_feature 160.9892120361328
------------iteration 700----------
total loss 321.84160626838224
main criterion 145.3821946472885
weighted_aux_loss 176.45941162109375
loss_r_bn_feature 176.45941162109375
------------iteration 800----------
total loss 207.5181644570857
main criterion 80.36717110991773
weighted_aux_loss 127.15099334716797
loss_r_bn_feature 127.15099334716797
------------iteration 900----------
total loss 188.88271948084213
main criterion 76.94711157068588
weighted_aux_loss 111.93560791015625
loss_r_bn_feature 111.93560791015625
------------iteration 1000----------
total loss 192.806592232173
main criterion 79.10953260082532
weighted_aux_loss 113.69705963134766
loss_r_bn_feature 113.69705963134766
------------iteration 1100----------
total loss 229.0045542712351
main criterion 102.8260416979929
weighted_aux_loss 126.17851257324219
loss_r_bn_feature 126.17851257324219
------------iteration 1200----------
total loss 171.14771499260473
main criterion 73.73934402092506
weighted_aux_loss 97.40837097167969
loss_r_bn_feature 97.40837097167969
------------iteration 1300----------
total loss 166.9834719850404
main criterion 85.05473815935683
weighted_aux_loss 81.9287338256836
loss_r_bn_feature 81.9287338256836
------------iteration 1400----------
total loss 147.40854849102425
main criterion 70.90394796612192
weighted_aux_loss 76.50460052490234
loss_r_bn_feature 76.50460052490234
------------iteration 1500----------
total loss 117.81625384775589
main criterion 59.33109302011916
weighted_aux_loss 58.48516082763672
loss_r_bn_feature 58.48516082763672
------------iteration 1600----------
total loss 136.90167109916888
main criterion 68.72025935600482
weighted_aux_loss 68.18141174316406
loss_r_bn_feature 68.18141174316406
------------iteration 1700----------
total loss 171.21005634997618
main criterion 83.33411793444886
weighted_aux_loss 87.87593841552734
loss_r_bn_feature 87.87593841552734
------------iteration 1800----------
total loss 269.7645499594398
main criterion 109.70913003756483
weighted_aux_loss 160.055419921875
loss_r_bn_feature 160.055419921875
------------iteration 1900----------
total loss 100.12181373364767
main criterion 55.12878318555197
weighted_aux_loss 44.9930305480957
loss_r_bn_feature 44.9930305480957
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 250 end_cls 260
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 753.8555393366115
main criterion 210.93549539129904
weighted_aux_loss 542.9200439453125
loss_r_bn_feature 542.9200439453125
------------iteration 100----------
total loss 432.1041290472645
main criterion 98.37515565859263
weighted_aux_loss 333.7289733886719
loss_r_bn_feature 333.7289733886719
------------iteration 200----------
total loss 354.5490082760822
main criterion 91.47384348116029
weighted_aux_loss 263.0751647949219
loss_r_bn_feature 263.0751647949219
------------iteration 300----------
total loss 324.81624999936776
main criterion 109.86265197690679
weighted_aux_loss 214.95359802246094
loss_r_bn_feature 214.95359802246094
------------iteration 400----------
total loss 285.58232208896334
main criterion 92.48140045810396
weighted_aux_loss 193.10092163085938
loss_r_bn_feature 193.10092163085938
------------iteration 500----------
total loss 319.6211770002739
main criterion 110.88925866531298
weighted_aux_loss 208.73191833496094
loss_r_bn_feature 208.73191833496094
------------iteration 600----------
total loss 240.12728830304218
main criterion 85.45440622296407
weighted_aux_loss 154.67288208007812
loss_r_bn_feature 154.67288208007812
------------iteration 700----------
total loss 343.3038984412076
main criterion 153.59415112675447
weighted_aux_loss 189.70974731445312
loss_r_bn_feature 189.70974731445312
------------iteration 800----------
total loss 257.2463637388043
main criterion 101.26258383157774
weighted_aux_loss 155.98377990722656
loss_r_bn_feature 155.98377990722656
------------iteration 900----------
total loss 254.90222053160664
main criterion 111.04809455504414
weighted_aux_loss 143.8541259765625
loss_r_bn_feature 143.8541259765625
------------iteration 1000----------
total loss 188.85966108506082
main criterion 81.4153938121116
weighted_aux_loss 107.44426727294922
loss_r_bn_feature 107.44426727294922
------------iteration 1100----------
total loss 185.32759748421483
main criterion 78.47023664437107
weighted_aux_loss 106.85736083984375
loss_r_bn_feature 106.85736083984375
------------iteration 1200----------
total loss 171.9308450647781
main criterion 75.87228946175075
weighted_aux_loss 96.05855560302734
loss_r_bn_feature 96.05855560302734
------------iteration 1300----------
total loss 317.269342554293
main criterion 146.7240239507774
weighted_aux_loss 170.54531860351562
loss_r_bn_feature 170.54531860351562
------------iteration 1400----------
total loss 238.4582223695605
main criterion 110.93135927141597
weighted_aux_loss 127.52686309814453
loss_r_bn_feature 127.52686309814453
------------iteration 1500----------
total loss 162.75526455148326
main criterion 68.98724392160044
weighted_aux_loss 93.76802062988281
loss_r_bn_feature 93.76802062988281
------------iteration 1600----------
total loss 109.2383195834567
main criterion 58.67317599825162
weighted_aux_loss 50.56514358520508
loss_r_bn_feature 50.56514358520508
------------iteration 1700----------
total loss 228.70766549070282
main criterion 101.54032998044893
weighted_aux_loss 127.1673355102539
loss_r_bn_feature 127.1673355102539
------------iteration 1800----------
total loss 114.35228290306306
main criterion 63.47634830223298
weighted_aux_loss 50.87593460083008
loss_r_bn_feature 50.87593460083008
------------iteration 1900----------
total loss 122.01384083037014
main criterion 60.96933857206936
weighted_aux_loss 61.04450225830078
loss_r_bn_feature 61.04450225830078
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 260 end_cls 270
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 715.727074976409
main criterion 213.9744504646902
weighted_aux_loss 501.75262451171875
loss_r_bn_feature 501.75262451171875
------------iteration 100----------
total loss 450.12666017351114
main criterion 131.50443727312054
weighted_aux_loss 318.6222229003906
loss_r_bn_feature 318.6222229003906
------------iteration 200----------
total loss 335.9248094381803
main criterion 91.04311082978191
weighted_aux_loss 244.88169860839844
loss_r_bn_feature 244.88169860839844
------------iteration 300----------
total loss 308.1302896415836
main criterion 104.46425875779455
weighted_aux_loss 203.66603088378906
loss_r_bn_feature 203.66603088378906
------------iteration 400----------
total loss 258.250483380678
main criterion 89.53701292169362
weighted_aux_loss 168.71347045898438
loss_r_bn_feature 168.71347045898438
------------iteration 500----------
total loss 263.54595770518506
main criterion 97.41296209971631
weighted_aux_loss 166.13299560546875
loss_r_bn_feature 166.13299560546875
------------iteration 600----------
total loss 249.11077440085398
main criterion 91.31220567526806
weighted_aux_loss 157.79856872558594
loss_r_bn_feature 157.79856872558594
------------iteration 700----------
total loss 398.58963995993645
main criterion 169.89501410544423
weighted_aux_loss 228.6946258544922
loss_r_bn_feature 228.6946258544922
------------iteration 800----------
total loss 272.3036612180871
main criterion 112.7051962522668
weighted_aux_loss 159.5984649658203
loss_r_bn_feature 159.5984649658203
------------iteration 900----------
total loss 213.80294728828656
main criterion 82.96754384590375
weighted_aux_loss 130.8354034423828
loss_r_bn_feature 130.8354034423828
------------iteration 1000----------
total loss 180.14257297769063
main criterion 76.1139474894094
weighted_aux_loss 104.02862548828125
loss_r_bn_feature 104.02862548828125
------------iteration 1100----------
total loss 168.7034518965237
main criterion 73.45600774369166
weighted_aux_loss 95.24744415283203
loss_r_bn_feature 95.24744415283203
------------iteration 1200----------
total loss 195.90296171966804
main criterion 98.67492674652352
weighted_aux_loss 97.22803497314453
loss_r_bn_feature 97.22803497314453
------------iteration 1300----------
total loss 157.97303239743775
main criterion 74.4237083617932
weighted_aux_loss 83.54932403564453
loss_r_bn_feature 83.54932403564453
------------iteration 1400----------
total loss 132.2191298911028
main criterion 63.90722498387624
weighted_aux_loss 68.31190490722656
loss_r_bn_feature 68.31190490722656
------------iteration 1500----------
total loss 195.3127452456959
main criterion 102.98207202792246
weighted_aux_loss 92.33067321777344
loss_r_bn_feature 92.33067321777344
------------iteration 1600----------
total loss 112.08124956684793
main criterion 62.46087679463114
weighted_aux_loss 49.6203727722168
loss_r_bn_feature 49.6203727722168
------------iteration 1700----------
total loss 130.15863043767223
main criterion 75.85212332707651
weighted_aux_loss 54.3065071105957
loss_r_bn_feature 54.3065071105957
------------iteration 1800----------
total loss 87.7125180624666
main criterion 49.73292287814043
weighted_aux_loss 37.97959518432617
loss_r_bn_feature 37.97959518432617
------------iteration 1900----------
total loss 181.34048121261625
main criterion 96.48771326828032
weighted_aux_loss 84.85276794433594
loss_r_bn_feature 84.85276794433594
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 270 end_cls 280
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 674.8637603010095
main criterion 215.19203788890007
weighted_aux_loss 459.6717224121094
loss_r_bn_feature 459.6717224121094
------------iteration 100----------
total loss 479.8379604060525
main criterion 160.02723042558375
weighted_aux_loss 319.81072998046875
loss_r_bn_feature 319.81072998046875
------------iteration 200----------
total loss 407.81786244833256
main criterion 139.44042104208253
weighted_aux_loss 268.37744140625
loss_r_bn_feature 268.37744140625
------------iteration 300----------
total loss 348.22278670252393
main criterion 130.29369429529737
weighted_aux_loss 217.92909240722656
loss_r_bn_feature 217.92909240722656
------------iteration 400----------
total loss 260.2317944162227
main criterion 90.3798809640743
weighted_aux_loss 169.85191345214844
loss_r_bn_feature 169.85191345214844
------------iteration 500----------
total loss 368.93911692652887
main criterion 152.3824763015289
weighted_aux_loss 216.556640625
loss_r_bn_feature 216.556640625
------------iteration 600----------
total loss 319.22155781015886
main criterion 130.29954548105732
weighted_aux_loss 188.92201232910156
loss_r_bn_feature 188.92201232910156
------------iteration 700----------
total loss 220.37071610208937
main criterion 81.63435745951124
weighted_aux_loss 138.73635864257812
loss_r_bn_feature 138.73635864257812
------------iteration 800----------
total loss 205.21669027923014
main criterion 81.15213034270671
weighted_aux_loss 124.06455993652344
loss_r_bn_feature 124.06455993652344
------------iteration 900----------
total loss 227.2100370454337
main criterion 93.13398724074618
weighted_aux_loss 134.0760498046875
loss_r_bn_feature 134.0760498046875
------------iteration 1000----------
total loss 170.2783783678903
main criterion 73.4752548937692
weighted_aux_loss 96.8031234741211
loss_r_bn_feature 96.8031234741211
------------iteration 1100----------
total loss 175.31185720929165
main criterion 76.8649425364401
weighted_aux_loss 98.44691467285156
loss_r_bn_feature 98.44691467285156
------------iteration 1200----------
total loss 150.20490948897861
main criterion 70.84848706466222
weighted_aux_loss 79.3564224243164
loss_r_bn_feature 79.3564224243164
------------iteration 1300----------
total loss 132.331414083092
main criterion 65.53321156844358
weighted_aux_loss 66.79820251464844
loss_r_bn_feature 66.79820251464844
------------iteration 1400----------
total loss 128.47078725044054
main criterion 60.79737874213974
weighted_aux_loss 67.67340850830078
loss_r_bn_feature 67.67340850830078
------------iteration 1500----------
total loss 436.0227872222613
main criterion 177.4251309722613
weighted_aux_loss 258.59765625
loss_r_bn_feature 258.59765625
------------iteration 1600----------
total loss 104.4704855986625
main criterion 56.85378256521524
weighted_aux_loss 47.616703033447266
loss_r_bn_feature 47.616703033447266
------------iteration 1700----------
total loss 226.10640613980064
main criterion 115.91849415249597
weighted_aux_loss 110.18791198730469
loss_r_bn_feature 110.18791198730469
------------iteration 1800----------
total loss 127.05312097513676
main criterion 71.05404794657231
weighted_aux_loss 55.99907302856445
loss_r_bn_feature 55.99907302856445
------------iteration 1900----------
total loss 95.501389718127
main criterion 54.48625681407426
weighted_aux_loss 41.015132904052734
loss_r_bn_feature 41.015132904052734
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 280 end_cls 290
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 706.4788591735435
main criterion 208.81842826534032
weighted_aux_loss 497.6604309082031
loss_r_bn_feature 497.6604309082031
------------iteration 100----------
total loss 393.6816639953199
main criterion 92.60024309688241
weighted_aux_loss 301.0814208984375
loss_r_bn_feature 301.0814208984375
------------iteration 200----------
total loss 327.8404549398143
main criterion 83.43571555993148
weighted_aux_loss 244.4047393798828
loss_r_bn_feature 244.4047393798828
------------iteration 300----------
total loss 464.27621433315
main criterion 164.18432590541562
weighted_aux_loss 300.0918884277344
loss_r_bn_feature 300.0918884277344
------------iteration 400----------
total loss 276.20009449603435
main criterion 89.34879139544844
weighted_aux_loss 186.85130310058594
loss_r_bn_feature 186.85130310058594
------------iteration 500----------
total loss 426.9017085724737
main criterion 152.1790523224737
weighted_aux_loss 274.72265625
loss_r_bn_feature 274.72265625
------------iteration 600----------
total loss 426.2402983573985
main criterion 169.41028126755478
weighted_aux_loss 256.83001708984375
loss_r_bn_feature 256.83001708984375
------------iteration 700----------
total loss 223.0125753712398
main criterion 81.89139006850542
weighted_aux_loss 141.12118530273438
loss_r_bn_feature 141.12118530273438
------------iteration 800----------
total loss 222.11442412889238
main criterion 78.81649627244707
weighted_aux_loss 143.2979278564453
loss_r_bn_feature 143.2979278564453
------------iteration 900----------
total loss 260.8274712581958
main criterion 103.44696283534425
weighted_aux_loss 157.38050842285156
loss_r_bn_feature 157.38050842285156
------------iteration 1000----------
total loss 270.370076079844
main criterion 117.11360635328154
weighted_aux_loss 153.2564697265625
loss_r_bn_feature 153.2564697265625
------------iteration 1100----------
total loss 181.062535626383
main criterion 77.06690726944943
weighted_aux_loss 103.9956283569336
loss_r_bn_feature 103.9956283569336
------------iteration 1200----------
total loss 158.3417844229026
main criterion 70.6879453115745
weighted_aux_loss 87.65383911132812
loss_r_bn_feature 87.65383911132812
------------iteration 1300----------
total loss 169.41371562800768
main criterion 79.96805980525377
weighted_aux_loss 89.4456558227539
loss_r_bn_feature 89.4456558227539
------------iteration 1400----------
total loss 170.11118478829889
main criterion 80.03945322091606
weighted_aux_loss 90.07173156738281
loss_r_bn_feature 90.07173156738281
------------iteration 1500----------
total loss 156.98673436067486
main criterion 74.29825016878033
weighted_aux_loss 82.68848419189453
loss_r_bn_feature 82.68848419189453
------------iteration 1600----------
total loss 112.75171200277866
main criterion 61.443331875825535
weighted_aux_loss 51.308380126953125
loss_r_bn_feature 51.308380126953125
------------iteration 1700----------
total loss 116.50829821000696
main criterion 62.898709586960074
weighted_aux_loss 53.609588623046875
loss_r_bn_feature 53.609588623046875
------------iteration 1800----------
total loss 207.12969825117162
main criterion 102.53361365644506
weighted_aux_loss 104.59608459472656
loss_r_bn_feature 104.59608459472656
------------iteration 1900----------
total loss 143.2494859740589
main criterion 71.39322376702765
weighted_aux_loss 71.85626220703125
loss_r_bn_feature 71.85626220703125
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 290 end_cls 300
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 756.7844794558271
main criterion 217.3063910769208
weighted_aux_loss 539.4780883789062
loss_r_bn_feature 539.4780883789062
------------iteration 100----------
total loss 381.65666851816934
main criterion 96.84499249277874
weighted_aux_loss 284.8116760253906
loss_r_bn_feature 284.8116760253906
------------iteration 200----------
total loss 307.4907017423018
main criterion 88.70340926183303
weighted_aux_loss 218.78729248046875
loss_r_bn_feature 218.78729248046875
------------iteration 300----------
total loss 274.19934326772824
main criterion 89.51309448843135
weighted_aux_loss 184.68624877929688
loss_r_bn_feature 184.68624877929688
------------iteration 400----------
total loss 267.8827771561951
main criterion 92.08603948529667
weighted_aux_loss 175.79673767089844
loss_r_bn_feature 175.79673767089844
------------iteration 500----------
total loss 455.45112126222057
main criterion 181.7527875219862
weighted_aux_loss 273.6983337402344
loss_r_bn_feature 273.6983337402344
------------iteration 600----------
total loss 332.17024719496
main criterion 137.7608691432022
weighted_aux_loss 194.4093780517578
loss_r_bn_feature 194.4093780517578
------------iteration 700----------
total loss 229.97944865255397
main criterion 84.04086527853055
weighted_aux_loss 145.93858337402344
loss_r_bn_feature 145.93858337402344
------------iteration 800----------
total loss 220.56292732882457
main criterion 85.60313423800427
weighted_aux_loss 134.9597930908203
loss_r_bn_feature 134.9597930908203
------------iteration 900----------
total loss 206.95248923616182
main criterion 87.11997733430636
weighted_aux_loss 119.83251190185547
loss_r_bn_feature 119.83251190185547
------------iteration 1000----------
total loss 362.5213955959499
main criterion 159.55330172387957
weighted_aux_loss 202.9680938720703
loss_r_bn_feature 202.9680938720703
------------iteration 1100----------
total loss 211.07260774444998
main criterion 93.57016633819998
weighted_aux_loss 117.50244140625
loss_r_bn_feature 117.50244140625
------------iteration 1200----------
total loss 312.17910052579424
main criterion 147.49497271817705
weighted_aux_loss 164.6841278076172
loss_r_bn_feature 164.6841278076172
------------iteration 1300----------
total loss 139.4517340733252
main criterion 66.62112189070803
weighted_aux_loss 72.83061218261719
loss_r_bn_feature 72.83061218261719
------------iteration 1400----------
total loss 134.189878310413
main criterion 68.27034553453412
weighted_aux_loss 65.9195327758789
loss_r_bn_feature 65.9195327758789
------------iteration 1500----------
total loss 135.99770798096336
main criterion 65.61912216553368
weighted_aux_loss 70.37858581542969
loss_r_bn_feature 70.37858581542969
------------iteration 1600----------
total loss 170.08224528846142
main criterion 82.46936839637159
weighted_aux_loss 87.61287689208984
loss_r_bn_feature 87.61287689208984
------------iteration 1700----------
total loss 121.17078422884589
main criterion 63.033363574549014
weighted_aux_loss 58.137420654296875
loss_r_bn_feature 58.137420654296875
------------iteration 1800----------
total loss 125.01531252385574
main criterion 68.58615526677566
weighted_aux_loss 56.42915725708008
loss_r_bn_feature 56.42915725708008
------------iteration 1900----------
total loss 88.44219389995513
main criterion 50.06358138164458
weighted_aux_loss 38.37861251831055
loss_r_bn_feature 38.37861251831055
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 300 end_cls 310
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 825.9093183803748
main criterion 213.32759230615605
weighted_aux_loss 612.5817260742188
loss_r_bn_feature 612.5817260742188
------------iteration 100----------
total loss 521.7410016165036
main criterion 179.4641156301754
weighted_aux_loss 342.2768859863281
loss_r_bn_feature 342.2768859863281
------------iteration 200----------
total loss 331.20622421231155
main criterion 89.33651901211626
weighted_aux_loss 241.8697052001953
loss_r_bn_feature 241.8697052001953
------------iteration 300----------
total loss 418.5053786097311
main criterion 160.49005878551236
weighted_aux_loss 258.01531982421875
loss_r_bn_feature 258.01531982421875
------------iteration 400----------
total loss 335.9161210666847
main criterion 122.2464433323097
weighted_aux_loss 213.669677734375
loss_r_bn_feature 213.669677734375
------------iteration 500----------
total loss 436.2043001806652
main criterion 171.26808191894648
weighted_aux_loss 264.93621826171875
loss_r_bn_feature 264.93621826171875
------------iteration 600----------
total loss 250.53026032349464
main criterion 86.45912384888526
weighted_aux_loss 164.07113647460938
loss_r_bn_feature 164.07113647460938
------------iteration 700----------
total loss 221.08709537189617
main criterion 82.88072024982586
weighted_aux_loss 138.2063751220703
loss_r_bn_feature 138.2063751220703
------------iteration 800----------
total loss 209.33938624195554
main criterion 78.65164210133052
weighted_aux_loss 130.687744140625
loss_r_bn_feature 130.687744140625
------------iteration 900----------
total loss 237.758368049431
main criterion 97.99220899181383
weighted_aux_loss 139.7661590576172
loss_r_bn_feature 139.7661590576172
------------iteration 1000----------
total loss 189.49060190723327
main criterion 72.53086222217469
weighted_aux_loss 116.9597396850586
loss_r_bn_feature 116.9597396850586
------------iteration 1100----------
total loss 164.55983507380688
main criterion 68.28836595759596
weighted_aux_loss 96.27146911621094
loss_r_bn_feature 96.27146911621094
------------iteration 1200----------
total loss 247.6380059923186
main criterion 118.92178895130297
weighted_aux_loss 128.71621704101562
loss_r_bn_feature 128.71621704101562
------------iteration 1300----------
total loss 155.26465654618994
main criterion 70.04260301835791
weighted_aux_loss 85.22205352783203
loss_r_bn_feature 85.22205352783203
------------iteration 1400----------
total loss 133.162075375364
main criterion 63.46088061218039
weighted_aux_loss 69.7011947631836
loss_r_bn_feature 69.7011947631836
------------iteration 1500----------
total loss 155.50919818878745
main criterion 72.36824512482261
weighted_aux_loss 83.14095306396484
loss_r_bn_feature 83.14095306396484
------------iteration 1600----------
total loss 116.60621523558096
main criterion 60.38151812254385
weighted_aux_loss 56.22469711303711
loss_r_bn_feature 56.22469711303711
------------iteration 1700----------
total loss 113.02779697545698
main criterion 55.846465340935495
weighted_aux_loss 57.181331634521484
loss_r_bn_feature 57.181331634521484
------------iteration 1800----------
total loss 138.19787968276503
main criterion 69.67335117934707
weighted_aux_loss 68.52452850341797
loss_r_bn_feature 68.52452850341797
------------iteration 1900----------
total loss 95.70455090247721
main criterion 50.64131085120768
weighted_aux_loss 45.06324005126953
loss_r_bn_feature 45.06324005126953
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 310 end_cls 320
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 742.6478125829028
main criterion 207.82377693837154
weighted_aux_loss 534.8240356445312
loss_r_bn_feature 534.8240356445312
------------iteration 100----------
total loss 436.58257131436915
main criterion 112.8741667733535
weighted_aux_loss 323.7084045410156
loss_r_bn_feature 323.7084045410156
------------iteration 200----------
total loss 370.2012230849936
main criterion 102.41222162014986
weighted_aux_loss 267.78900146484375
loss_r_bn_feature 267.78900146484375
------------iteration 300----------
total loss 385.86362633345243
main criterion 134.7222536527884
weighted_aux_loss 251.14137268066406
loss_r_bn_feature 251.14137268066406
------------iteration 400----------
total loss 358.2194888574428
main criterion 128.46568941896624
weighted_aux_loss 229.75379943847656
loss_r_bn_feature 229.75379943847656
------------iteration 500----------
total loss 274.58437746718175
main criterion 90.17199343397863
weighted_aux_loss 184.41238403320312
loss_r_bn_feature 184.41238403320312
------------iteration 600----------
total loss 256.9191512046041
main criterion 89.64026631690875
weighted_aux_loss 167.2788848876953
loss_r_bn_feature 167.2788848876953
------------iteration 700----------
total loss 407.39526942936254
main criterion 177.57587245670626
weighted_aux_loss 229.81939697265625
loss_r_bn_feature 229.81939697265625
------------iteration 800----------
total loss 263.69803519885886
main criterion 94.60031791370263
weighted_aux_loss 169.09771728515625
loss_r_bn_feature 169.09771728515625
------------iteration 900----------
total loss 257.42809540006067
main criterion 109.08811432095912
weighted_aux_loss 148.33998107910156
loss_r_bn_feature 148.33998107910156
------------iteration 1000----------
total loss 204.9893239904451
main criterion 82.96992244015213
weighted_aux_loss 122.01940155029297
loss_r_bn_feature 122.01940155029297
------------iteration 1100----------
total loss 195.40756046114325
main criterion 82.07569705782295
weighted_aux_loss 113.33186340332031
loss_r_bn_feature 113.33186340332031
------------iteration 1200----------
total loss 198.78287813188976
main criterion 85.52611085894054
weighted_aux_loss 113.25676727294922
loss_r_bn_feature 113.25676727294922
------------iteration 1300----------
total loss 153.55493443809655
main criterion 70.17396825157313
weighted_aux_loss 83.38096618652344
loss_r_bn_feature 83.38096618652344
------------iteration 1400----------
total loss 150.7427820717287
main criterion 72.5719522987795
weighted_aux_loss 78.17082977294922
loss_r_bn_feature 78.17082977294922
------------iteration 1500----------
total loss 193.28315853591636
main criterion 98.95768856521323
weighted_aux_loss 94.32546997070312
loss_r_bn_feature 94.32546997070312
------------iteration 1600----------
total loss 162.1626817232818
main criterion 85.02024855677789
weighted_aux_loss 77.1424331665039
loss_r_bn_feature 77.1424331665039
------------iteration 1700----------
total loss 151.2934493795696
main criterion 83.44603727019462
weighted_aux_loss 67.847412109375
loss_r_bn_feature 67.847412109375
------------iteration 1800----------
total loss 120.31019580845631
main criterion 62.71210468296803
weighted_aux_loss 57.59809112548828
loss_r_bn_feature 57.59809112548828
------------iteration 1900----------
total loss 240.12910682430982
main criterion 122.50342017880199
weighted_aux_loss 117.62568664550781
loss_r_bn_feature 117.62568664550781
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 320 end_cls 330
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 809.0639485932352
main criterion 214.28044029245396
weighted_aux_loss 594.7835083007812
loss_r_bn_feature 594.7835083007812
------------iteration 100----------
total loss 403.71460116669334
main criterion 108.18329013153712
weighted_aux_loss 295.53131103515625
loss_r_bn_feature 295.53131103515625
------------iteration 200----------
total loss 338.00737356107254
main criterion 101.65787099759599
weighted_aux_loss 236.34950256347656
loss_r_bn_feature 236.34950256347656
------------iteration 300----------
total loss 266.85140947077826
main criterion 82.22027543757515
weighted_aux_loss 184.63113403320312
loss_r_bn_feature 184.63113403320312
------------iteration 400----------
total loss 411.76901266251724
main criterion 170.24151632462662
weighted_aux_loss 241.52749633789062
loss_r_bn_feature 241.52749633789062
------------iteration 500----------
total loss 248.9996007335089
main criterion 82.15041860460266
weighted_aux_loss 166.84918212890625
loss_r_bn_feature 166.84918212890625
------------iteration 600----------
total loss 212.81238133606115
main criterion 75.72082860168615
weighted_aux_loss 137.091552734375
loss_r_bn_feature 137.091552734375
------------iteration 700----------
total loss 283.0304634380914
main criterion 114.51944659238823
weighted_aux_loss 168.51101684570312
loss_r_bn_feature 168.51101684570312
------------iteration 800----------
total loss 245.43154413958206
main criterion 95.4229381825508
weighted_aux_loss 150.00860595703125
loss_r_bn_feature 150.00860595703125
------------iteration 900----------
total loss 219.58127102280915
main criterion 88.62929043198884
weighted_aux_loss 130.9519805908203
loss_r_bn_feature 130.9519805908203
------------iteration 1000----------
total loss 175.2872105511707
main criterion 70.47307023134647
weighted_aux_loss 104.81414031982422
loss_r_bn_feature 104.81414031982422
------------iteration 1100----------
total loss 174.896377490168
main criterion 70.92358391106642
weighted_aux_loss 103.97279357910156
loss_r_bn_feature 103.97279357910156
------------iteration 1200----------
total loss 210.07929382914986
main criterion 97.57342682475533
weighted_aux_loss 112.50586700439453
loss_r_bn_feature 112.50586700439453
------------iteration 1300----------
total loss 173.20315727178934
main criterion 84.2618883508909
weighted_aux_loss 88.94126892089844
loss_r_bn_feature 88.94126892089844
------------iteration 1400----------
total loss 140.82460996972327
main criterion 65.24199125634435
weighted_aux_loss 75.5826187133789
loss_r_bn_feature 75.5826187133789
------------iteration 1500----------
total loss 142.168413263358
main criterion 64.6718617496861
weighted_aux_loss 77.49655151367188
loss_r_bn_feature 77.49655151367188
------------iteration 1600----------
total loss 107.62848962429995
main criterion 53.737803589143695
weighted_aux_loss 53.89068603515625
loss_r_bn_feature 53.89068603515625
------------iteration 1700----------
total loss 113.24273516256679
main criterion 57.691923394988656
weighted_aux_loss 55.550811767578125
loss_r_bn_feature 55.550811767578125
------------iteration 1800----------
total loss 222.73323847491463
main criterion 94.15749384600839
weighted_aux_loss 128.57574462890625
loss_r_bn_feature 128.57574462890625
------------iteration 1900----------
total loss 146.72196653363386
main criterion 68.80029752728619
weighted_aux_loss 77.92166900634766
loss_r_bn_feature 77.92166900634766
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 330 end_cls 340
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 712.6534600189889
main criterion 216.37379693305138
weighted_aux_loss 496.2796630859375
loss_r_bn_feature 496.2796630859375
------------iteration 100----------
total loss 516.6050427633353
main criterion 184.6526807027884
weighted_aux_loss 331.9523620605469
loss_r_bn_feature 331.9523620605469
------------iteration 200----------
total loss 338.0633750773829
main criterion 99.57968672289071
weighted_aux_loss 238.4836883544922
loss_r_bn_feature 238.4836883544922
------------iteration 300----------
total loss 280.7087320499634
main criterion 92.9720377140259
weighted_aux_loss 187.7366943359375
loss_r_bn_feature 187.7366943359375
------------iteration 400----------
total loss 310.2041331947229
main criterion 112.82847706679317
weighted_aux_loss 197.3756561279297
loss_r_bn_feature 197.3756561279297
------------iteration 500----------
total loss 344.0088463469004
main criterion 140.26432425217382
weighted_aux_loss 203.74452209472656
loss_r_bn_feature 203.74452209472656
------------iteration 600----------
total loss 234.52932834940162
main criterion 86.18850803690161
weighted_aux_loss 148.3408203125
loss_r_bn_feature 148.3408203125
------------iteration 700----------
total loss 243.8720294976748
main criterion 89.08232612853416
weighted_aux_loss 154.78970336914062
loss_r_bn_feature 154.78970336914062
------------iteration 800----------
total loss 230.03899451819785
main criterion 92.99718543616662
weighted_aux_loss 137.04180908203125
loss_r_bn_feature 137.04180908203125
------------iteration 900----------
total loss 213.75088687107552
main criterion 85.12887759373177
weighted_aux_loss 128.62200927734375
loss_r_bn_feature 128.62200927734375
------------iteration 1000----------
total loss 209.38538765119665
main criterion 88.57851051496617
weighted_aux_loss 120.80687713623047
loss_r_bn_feature 120.80687713623047
------------iteration 1100----------
total loss 199.04565206978933
main criterion 91.57311789010183
weighted_aux_loss 107.4725341796875
loss_r_bn_feature 107.4725341796875
------------iteration 1200----------
total loss 151.2090999076276
main criterion 69.26794542764715
weighted_aux_loss 81.94115447998047
loss_r_bn_feature 81.94115447998047
------------iteration 1300----------
total loss 161.95400755712217
main criterion 81.492665699212
weighted_aux_loss 80.46134185791016
loss_r_bn_feature 80.46134185791016
------------iteration 1400----------
total loss 133.7556574257199
main criterion 64.9500391395871
weighted_aux_loss 68.80561828613281
loss_r_bn_feature 68.80561828613281
------------iteration 1500----------
total loss 127.5466621010406
main criterion 65.16558910299372
weighted_aux_loss 62.381072998046875
loss_r_bn_feature 62.381072998046875
------------iteration 1600----------
total loss 200.5016593038399
main criterion 106.86180487268754
weighted_aux_loss 93.63985443115234
loss_r_bn_feature 93.63985443115234
------------iteration 1700----------
total loss 104.66175330961585
main criterion 58.69378913725256
weighted_aux_loss 45.96796417236328
loss_r_bn_feature 45.96796417236328
------------iteration 1800----------
total loss 322.9675766920526
main criterion 152.80859536881042
weighted_aux_loss 170.1589813232422
loss_r_bn_feature 170.1589813232422
------------iteration 1900----------
total loss 120.85503833368033
main criterion 68.47103183343619
weighted_aux_loss 52.38400650024414
loss_r_bn_feature 52.38400650024414
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 340 end_cls 350
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 716.631334199389
main criterion 220.24546994157652
weighted_aux_loss 496.3858642578125
loss_r_bn_feature 496.3858642578125
------------iteration 100----------
total loss 379.290627039569
main criterion 98.93753866066278
weighted_aux_loss 280.35308837890625
loss_r_bn_feature 280.35308837890625
------------iteration 200----------
total loss 442.93626768777136
main criterion 164.6035650510526
weighted_aux_loss 278.33270263671875
loss_r_bn_feature 278.33270263671875
------------iteration 300----------
total loss 329.44936476067437
main criterion 117.34906874016653
weighted_aux_loss 212.1002960205078
loss_r_bn_feature 212.1002960205078
------------iteration 400----------
total loss 255.444576473824
main criterion 91.38976690351147
weighted_aux_loss 164.0548095703125
loss_r_bn_feature 164.0548095703125
------------iteration 500----------
total loss 255.17663786607812
main criterion 95.2373220701797
weighted_aux_loss 159.93931579589844
loss_r_bn_feature 159.93931579589844
------------iteration 600----------
total loss 228.61356188770316
main criterion 87.76624133106252
weighted_aux_loss 140.84732055664062
loss_r_bn_feature 140.84732055664062
------------iteration 700----------
total loss 227.52520015182338
main criterion 86.02373530807338
weighted_aux_loss 141.50146484375
loss_r_bn_feature 141.50146484375
------------iteration 800----------
total loss 409.59777043609
main criterion 176.4100720718322
weighted_aux_loss 233.1876983642578
loss_r_bn_feature 233.1876983642578
------------iteration 900----------
total loss 189.6774525774233
main criterion 80.78704882986472
weighted_aux_loss 108.8904037475586
loss_r_bn_feature 108.8904037475586
------------iteration 1000----------
total loss 279.1614730129246
main criterion 131.6358077297215
weighted_aux_loss 147.52566528320312
loss_r_bn_feature 147.52566528320312
------------iteration 1100----------
total loss 182.0919339341563
main criterion 83.1951367539805
weighted_aux_loss 98.89679718017578
loss_r_bn_feature 98.89679718017578
------------iteration 1200----------
total loss 171.3547002371317
main criterion 82.19925895295202
weighted_aux_loss 89.15544128417969
loss_r_bn_feature 89.15544128417969
------------iteration 1300----------
total loss 150.0237367437905
main criterion 72.39177110658348
weighted_aux_loss 77.63196563720703
loss_r_bn_feature 77.63196563720703
------------iteration 1400----------
total loss 123.26876251414052
main criterion 63.86571304514637
weighted_aux_loss 59.40304946899414
loss_r_bn_feature 59.40304946899414
------------iteration 1500----------
total loss 112.09819264585896
main criterion 58.45497365171833
weighted_aux_loss 53.643218994140625
loss_r_bn_feature 53.643218994140625
------------iteration 1600----------
total loss 151.0799669220339
main criterion 76.25480975650655
weighted_aux_loss 74.82515716552734
loss_r_bn_feature 74.82515716552734
------------iteration 1700----------
total loss 136.0691015940655
main criterion 74.17983462629205
weighted_aux_loss 61.88926696777344
loss_r_bn_feature 61.88926696777344
------------iteration 1800----------
total loss 100.65378733059666
main criterion 61.11325235745212
weighted_aux_loss 39.54053497314453
loss_r_bn_feature 39.54053497314453
------------iteration 1900----------
total loss 93.93083086716743
main criterion 55.248534114237735
weighted_aux_loss 38.68229675292969
loss_r_bn_feature 38.68229675292969
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 350 end_cls 360
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 705.0890739913751
main criterion 218.7260368820001
weighted_aux_loss 486.363037109375
loss_r_bn_feature 486.363037109375
------------iteration 100----------
total loss 400.9176819166006
main criterion 105.96800540292871
weighted_aux_loss 294.9496765136719
loss_r_bn_feature 294.9496765136719
------------iteration 200----------
total loss 344.86814175838504
main criterion 98.86556302303347
weighted_aux_loss 246.00257873535156
loss_r_bn_feature 246.00257873535156
------------iteration 300----------
total loss 288.9100824703897
main criterion 90.33400214812406
weighted_aux_loss 198.57608032226562
loss_r_bn_feature 198.57608032226562
------------iteration 400----------
total loss 277.62656858585495
main criterion 96.30027464054247
weighted_aux_loss 181.3262939453125
loss_r_bn_feature 181.3262939453125
------------iteration 500----------
total loss 253.3210742348017
main criterion 91.41290162737981
weighted_aux_loss 161.90817260742188
loss_r_bn_feature 161.90817260742188
------------iteration 600----------
total loss 344.95072953293845
main criterion 150.33207718918848
weighted_aux_loss 194.61865234375
loss_r_bn_feature 194.61865234375
------------iteration 700----------
total loss 223.72414043771045
main criterion 85.40753582345262
weighted_aux_loss 138.3166046142578
loss_r_bn_feature 138.3166046142578
------------iteration 800----------
total loss 222.65740847156147
main criterion 88.27093386218647
weighted_aux_loss 134.386474609375
loss_r_bn_feature 134.386474609375
------------iteration 900----------
total loss 349.4493288007344
main criterion 152.77438678413282
weighted_aux_loss 196.67494201660156
loss_r_bn_feature 196.67494201660156
------------iteration 1000----------
total loss 204.32614318425846
main criterion 89.3155612140436
weighted_aux_loss 115.01058197021484
loss_r_bn_feature 115.01058197021484
------------iteration 1100----------
total loss 192.22151640513954
main criterion 84.67292479137001
weighted_aux_loss 107.54859161376953
loss_r_bn_feature 107.54859161376953
------------iteration 1200----------
total loss 191.28311164908453
main criterion 96.38071449332281
weighted_aux_loss 94.90239715576172
loss_r_bn_feature 94.90239715576172
------------iteration 1300----------
total loss 143.6467507424111
main criterion 69.47344904563373
weighted_aux_loss 74.17330169677734
loss_r_bn_feature 74.17330169677734
------------iteration 1400----------
total loss 135.9699183493737
main criterion 67.05531416236197
weighted_aux_loss 68.91460418701172
loss_r_bn_feature 68.91460418701172
------------iteration 1500----------
total loss 177.16856276260424
main criterion 95.50476729141283
weighted_aux_loss 81.6637954711914
loss_r_bn_feature 81.6637954711914
------------iteration 1600----------
total loss 105.57574770658286
main criterion 56.26734850614341
weighted_aux_loss 49.30839920043945
loss_r_bn_feature 49.30839920043945
------------iteration 1700----------
total loss 228.79807204568402
main criterion 117.7019493039848
weighted_aux_loss 111.09612274169922
loss_r_bn_feature 111.09612274169922
------------iteration 1800----------
total loss 135.4070225912618
main criterion 72.86945545258993
weighted_aux_loss 62.537567138671875
loss_r_bn_feature 62.537567138671875
------------iteration 1900----------
total loss 89.57609731001683
main criterion 50.77546101851293
weighted_aux_loss 38.800636291503906
loss_r_bn_feature 38.800636291503906
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 360 end_cls 370
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 736.0335934855934
main criterion 224.13778049731212
weighted_aux_loss 511.89581298828125
loss_r_bn_feature 511.89581298828125
------------iteration 100----------
total loss 419.0676229720839
main criterion 113.43053801114641
weighted_aux_loss 305.6370849609375
loss_r_bn_feature 305.6370849609375
------------iteration 200----------
total loss 338.3352150434855
main criterion 96.5443367475871
weighted_aux_loss 241.79087829589844
loss_r_bn_feature 241.79087829589844
------------iteration 300----------
total loss 296.96869728278375
main criterion 98.54926368903374
weighted_aux_loss 198.41943359375
loss_r_bn_feature 198.41943359375
------------iteration 400----------
total loss 275.92255572043024
main criterion 95.89373186789119
weighted_aux_loss 180.02882385253906
loss_r_bn_feature 180.02882385253906
------------iteration 500----------
total loss 307.45206413389155
main criterion 123.20201835752437
weighted_aux_loss 184.2500457763672
loss_r_bn_feature 184.2500457763672
------------iteration 600----------
total loss 422.17967871832013
main criterion 179.66600684332016
weighted_aux_loss 242.513671875
loss_r_bn_feature 242.513671875
------------iteration 700----------
total loss 231.01118874261115
main criterion 90.64307045647834
weighted_aux_loss 140.3681182861328
loss_r_bn_feature 140.3681182861328
------------iteration 800----------
total loss 364.8606263569828
main criterion 163.62263502397496
weighted_aux_loss 201.2379913330078
loss_r_bn_feature 201.2379913330078
------------iteration 900----------
total loss 358.71834940095744
main criterion 159.3676719107231
weighted_aux_loss 199.35067749023438
loss_r_bn_feature 199.35067749023438
------------iteration 1000----------
total loss 188.4747301856275
main criterion 80.05458705818607
weighted_aux_loss 108.4201431274414
loss_r_bn_feature 108.4201431274414
------------iteration 1100----------
total loss 180.11706325555218
main criterion 77.41428920770063
weighted_aux_loss 102.70277404785156
loss_r_bn_feature 102.70277404785156
------------iteration 1200----------
total loss 165.08196072241495
main criterion 78.06361965796182
weighted_aux_loss 87.01834106445312
loss_r_bn_feature 87.01834106445312
------------iteration 1300----------
total loss 183.9618329328706
main criterion 94.19819920484326
weighted_aux_loss 89.76363372802734
loss_r_bn_feature 89.76363372802734
------------iteration 1400----------
total loss 131.1053136352711
main criterion 66.90069327394299
weighted_aux_loss 64.20462036132812
loss_r_bn_feature 64.20462036132812
------------iteration 1500----------
total loss 123.96345619818537
main criterion 65.36142830511896
weighted_aux_loss 58.602027893066406
loss_r_bn_feature 58.602027893066406
------------iteration 1600----------
total loss 156.85660638531044
main criterion 76.13681878765419
weighted_aux_loss 80.71978759765625
loss_r_bn_feature 80.71978759765625
------------iteration 1700----------
total loss 113.3758358170849
main criterion 59.62050287030756
weighted_aux_loss 53.755332946777344
loss_r_bn_feature 53.755332946777344
------------iteration 1800----------
total loss 103.74153418461643
main criterion 59.18039603154025
weighted_aux_loss 44.56113815307617
loss_r_bn_feature 44.56113815307617
------------iteration 1900----------
total loss 347.70599411984995
main criterion 155.76007126828748
weighted_aux_loss 191.9459228515625
loss_r_bn_feature 191.9459228515625
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 370 end_cls 380
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 741.0876293568595
main criterion 224.23966793107823
weighted_aux_loss 516.8479614257812
loss_r_bn_feature 516.8479614257812
------------iteration 100----------
total loss 495.3305877093072
main criterion 169.86864312922913
weighted_aux_loss 325.4619445800781
loss_r_bn_feature 325.4619445800781
------------iteration 200----------
total loss 331.0614267664399
main criterion 100.7281748133149
weighted_aux_loss 230.333251953125
loss_r_bn_feature 230.333251953125
------------iteration 300----------
total loss 282.0296651665629
main criterion 94.24366968316447
weighted_aux_loss 187.78599548339844
loss_r_bn_feature 187.78599548339844
------------iteration 400----------
total loss 385.3761273039186
main criterion 152.87083250411393
weighted_aux_loss 232.5052947998047
loss_r_bn_feature 232.5052947998047
------------iteration 500----------
total loss 395.9205349030249
main criterion 167.59025841376703
weighted_aux_loss 228.3302764892578
loss_r_bn_feature 228.3302764892578
------------iteration 600----------
total loss 256.99107373923164
main criterion 96.83850110739567
weighted_aux_loss 160.15257263183594
loss_r_bn_feature 160.15257263183594
------------iteration 700----------
total loss 268.2652137849781
main criterion 111.55845719318121
weighted_aux_loss 156.70675659179688
loss_r_bn_feature 156.70675659179688
------------iteration 800----------
total loss 214.54808366815996
main criterion 82.82776201288654
weighted_aux_loss 131.72032165527344
loss_r_bn_feature 131.72032165527344
------------iteration 900----------
total loss 197.94830285897453
main criterion 77.38081323495109
weighted_aux_loss 120.56748962402344
loss_r_bn_feature 120.56748962402344
------------iteration 1000----------
total loss 252.6326981857473
main criterion 115.40633405000513
weighted_aux_loss 137.2263641357422
loss_r_bn_feature 137.2263641357422
------------iteration 1100----------
total loss 168.53145040559264
main criterion 74.51153768586606
weighted_aux_loss 94.01991271972656
loss_r_bn_feature 94.01991271972656
------------iteration 1200----------
total loss 154.54704117933142
main criterion 72.32120347181187
weighted_aux_loss 82.22583770751953
loss_r_bn_feature 82.22583770751953
------------iteration 1300----------
total loss 185.60111610784622
main criterion 94.12150185003372
weighted_aux_loss 91.4796142578125
loss_r_bn_feature 91.4796142578125
------------iteration 1400----------
total loss 136.01336209606797
main criterion 66.27810971569686
weighted_aux_loss 69.7352523803711
loss_r_bn_feature 69.7352523803711
------------iteration 1500----------
total loss 145.34228518968476
main criterion 67.80557254320038
weighted_aux_loss 77.53671264648438
loss_r_bn_feature 77.53671264648438
------------iteration 1600----------
total loss 135.17585412303043
main criterion 66.46127740183903
weighted_aux_loss 68.7145767211914
loss_r_bn_feature 68.7145767211914
------------iteration 1700----------
total loss 114.81310862188151
main criterion 65.28551310186198
weighted_aux_loss 49.52759552001953
loss_r_bn_feature 49.52759552001953
------------iteration 1800----------
total loss 110.35835436130742
main criterion 58.15391329075078
weighted_aux_loss 52.20444107055664
loss_r_bn_feature 52.20444107055664
------------iteration 1900----------
total loss 312.13471234590816
main criterion 133.6194230392675
weighted_aux_loss 178.51528930664062
loss_r_bn_feature 178.51528930664062
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 380 end_cls 390
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 769.1680880186934
main criterion 223.20812708119337
weighted_aux_loss 545.9599609375
loss_r_bn_feature 545.9599609375
------------iteration 100----------
total loss 417.71239822840863
main criterion 106.06978958583049
weighted_aux_loss 311.6426086425781
loss_r_bn_feature 311.6426086425781
------------iteration 200----------
total loss 430.97770311230124
main criterion 164.69294359081684
weighted_aux_loss 266.2847595214844
loss_r_bn_feature 266.2847595214844
------------iteration 300----------
total loss 505.92715552463255
main criterion 196.21157935275758
weighted_aux_loss 309.715576171875
loss_r_bn_feature 309.715576171875
------------iteration 400----------
total loss 267.5002250542146
main criterion 94.0637779106599
weighted_aux_loss 173.4364471435547
loss_r_bn_feature 173.4364471435547
------------iteration 500----------
total loss 287.38409374017795
main criterion 116.66632030267792
weighted_aux_loss 170.7177734375
loss_r_bn_feature 170.7177734375
------------iteration 600----------
total loss 239.12388290109106
main criterion 87.2277494782395
weighted_aux_loss 151.89613342285156
loss_r_bn_feature 151.89613342285156
------------iteration 700----------
total loss 351.8425008072535
main criterion 149.94197285315195
weighted_aux_loss 201.90052795410156
loss_r_bn_feature 201.90052795410156
------------iteration 800----------
total loss 276.97750344860947
main criterion 115.58011880505481
weighted_aux_loss 161.3973846435547
loss_r_bn_feature 161.3973846435547
------------iteration 900----------
total loss 230.66482621223776
main criterion 100.12161332161274
weighted_aux_loss 130.543212890625
loss_r_bn_feature 130.543212890625
------------iteration 1000----------
total loss 188.38546489252923
main criterion 79.71984599604487
weighted_aux_loss 108.66561889648438
loss_r_bn_feature 108.66561889648438
------------iteration 1100----------
total loss 178.78447434247843
main criterion 78.55184647382609
weighted_aux_loss 100.23262786865234
loss_r_bn_feature 100.23262786865234
------------iteration 1200----------
total loss 172.99865751532323
main criterion 77.55812864569432
weighted_aux_loss 95.4405288696289
loss_r_bn_feature 95.4405288696289
------------iteration 1300----------
total loss 177.73247944921246
main criterion 83.69674336522809
weighted_aux_loss 94.03573608398438
loss_r_bn_feature 94.03573608398438
------------iteration 1400----------
total loss 127.09949010659928
main criterion 64.15833181192154
weighted_aux_loss 62.941158294677734
loss_r_bn_feature 62.941158294677734
------------iteration 1500----------
total loss 129.29300843114922
main criterion 66.40575562353203
weighted_aux_loss 62.88725280761719
loss_r_bn_feature 62.88725280761719
------------iteration 1600----------
total loss 128.14447749475102
main criterion 67.60982859948734
weighted_aux_loss 60.53464889526367
loss_r_bn_feature 60.53464889526367
------------iteration 1700----------
total loss 302.0125582330338
main criterion 131.17884851623694
weighted_aux_loss 170.83370971679688
loss_r_bn_feature 170.83370971679688
------------iteration 1800----------
total loss 209.81237680869114
main criterion 97.98869974570286
weighted_aux_loss 111.82367706298828
loss_r_bn_feature 111.82367706298828
------------iteration 1900----------
total loss 124.92569480081346
main criterion 65.00628409524705
weighted_aux_loss 59.919410705566406
loss_r_bn_feature 59.919410705566406
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 390 end_cls 400
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 829.3614705193907
main criterion 211.36452227720312
weighted_aux_loss 617.9969482421875
loss_r_bn_feature 617.9969482421875
------------iteration 100----------
total loss 407.6807842050817
main criterion 102.32940481055044
weighted_aux_loss 305.35137939453125
loss_r_bn_feature 305.35137939453125
------------iteration 200----------
total loss 348.29079410748955
main criterion 90.57332584577082
weighted_aux_loss 257.71746826171875
loss_r_bn_feature 257.71746826171875
------------iteration 300----------
total loss 307.01878337257244
main criterion 96.64640788429116
weighted_aux_loss 210.37237548828125
loss_r_bn_feature 210.37237548828125
------------iteration 400----------
total loss 293.47053559091376
main criterion 93.02648957040593
weighted_aux_loss 200.4440460205078
loss_r_bn_feature 200.4440460205078
------------iteration 500----------
total loss 269.48291118055806
main criterion 93.87582499891747
weighted_aux_loss 175.60708618164062
loss_r_bn_feature 175.60708618164062
------------iteration 600----------
total loss 327.3646414221686
main criterion 136.60669159306698
weighted_aux_loss 190.75794982910156
loss_r_bn_feature 190.75794982910156
------------iteration 700----------
total loss 343.5517717560045
main criterion 141.56248342592636
weighted_aux_loss 201.98928833007812
loss_r_bn_feature 201.98928833007812
------------iteration 800----------
total loss 241.07530601723744
main criterion 100.45230491860464
weighted_aux_loss 140.6230010986328
loss_r_bn_feature 140.6230010986328
------------iteration 900----------
total loss 240.2292219833621
main criterion 105.53169695894803
weighted_aux_loss 134.69752502441406
loss_r_bn_feature 134.69752502441406
------------iteration 1000----------
total loss 318.49227477381646
main criterion 137.47639037440237
weighted_aux_loss 181.01588439941406
loss_r_bn_feature 181.01588439941406
------------iteration 1100----------
total loss 190.96244124147938
main criterion 82.54264143679188
weighted_aux_loss 108.4197998046875
loss_r_bn_feature 108.4197998046875
------------iteration 1200----------
total loss 164.1289485328844
main criterion 75.53793748552113
weighted_aux_loss 88.59101104736328
loss_r_bn_feature 88.59101104736328
------------iteration 1300----------
total loss 162.11601012361004
main criterion 71.75607054841473
weighted_aux_loss 90.35993957519531
loss_r_bn_feature 90.35993957519531
------------iteration 1400----------
total loss 145.49396580995813
main criterion 67.57541722597377
weighted_aux_loss 77.91854858398438
loss_r_bn_feature 77.91854858398438
------------iteration 1500----------
total loss 130.07179503538637
main criterion 62.10137419798402
weighted_aux_loss 67.97042083740234
loss_r_bn_feature 67.97042083740234
------------iteration 1600----------
total loss 127.99089333747301
main criterion 69.59076592658434
weighted_aux_loss 58.40012741088867
loss_r_bn_feature 58.40012741088867
------------iteration 1700----------
total loss 109.40647822144383
main criterion 58.46695643189305
weighted_aux_loss 50.93952178955078
loss_r_bn_feature 50.93952178955078
------------iteration 1800----------
total loss 144.50596156488348
main criterion 61.86895289789129
weighted_aux_loss 82.63700866699219
loss_r_bn_feature 82.63700866699219
------------iteration 1900----------
total loss 91.19726085915613
main criterion 48.70955562844324
weighted_aux_loss 42.48770523071289
loss_r_bn_feature 42.48770523071289
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 400 end_cls 410
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 853.363957619285
main criterion 211.37524912319122
weighted_aux_loss 641.9887084960938
loss_r_bn_feature 641.9887084960938
------------iteration 100----------
total loss 454.70135988071
main criterion 115.15067018344436
weighted_aux_loss 339.5506896972656
loss_r_bn_feature 339.5506896972656
------------iteration 200----------
total loss 380.7464406007051
main criterion 112.2775685303926
weighted_aux_loss 268.4688720703125
loss_r_bn_feature 268.4688720703125
------------iteration 300----------
total loss 323.95347321262494
main criterion 102.02261078586714
weighted_aux_loss 221.9308624267578
loss_r_bn_feature 221.9308624267578
------------iteration 400----------
total loss 415.32722905851944
main criterion 167.9285169003163
weighted_aux_loss 247.39871215820312
loss_r_bn_feature 247.39871215820312
------------iteration 500----------
total loss 248.03813922308268
main criterion 87.79933917425454
weighted_aux_loss 160.23880004882812
loss_r_bn_feature 160.23880004882812
------------iteration 600----------
total loss 384.2225653776504
main criterion 157.68582221358793
weighted_aux_loss 226.5367431640625
loss_r_bn_feature 226.5367431640625
------------iteration 700----------
total loss 231.187379430336
main criterion 85.1502242789688
weighted_aux_loss 146.0371551513672
loss_r_bn_feature 146.0371551513672
------------iteration 800----------
total loss 231.4085934624184
main criterion 88.03174714894183
weighted_aux_loss 143.37684631347656
loss_r_bn_feature 143.37684631347656
------------iteration 900----------
total loss 196.03136332627793
main criterion 75.32329447862168
weighted_aux_loss 120.70806884765625
loss_r_bn_feature 120.70806884765625
------------iteration 1000----------
total loss 222.20826110251238
main criterion 91.10938414938737
weighted_aux_loss 131.098876953125
loss_r_bn_feature 131.098876953125
------------iteration 1100----------
total loss 219.6982207682686
main criterion 94.42624048262408
weighted_aux_loss 125.27198028564453
loss_r_bn_feature 125.27198028564453
------------iteration 1200----------
total loss 177.06840173840783
main criterion 78.01728479504845
weighted_aux_loss 99.05111694335938
loss_r_bn_feature 99.05111694335938
------------iteration 1300----------
total loss 432.1626184239874
main criterion 171.55757692008115
weighted_aux_loss 260.60504150390625
loss_r_bn_feature 260.60504150390625
------------iteration 1400----------
total loss 133.8770622573722
main criterion 59.70733874662999
weighted_aux_loss 74.16972351074219
loss_r_bn_feature 74.16972351074219
------------iteration 1500----------
total loss 299.9996172588196
main criterion 127.1768328349915
weighted_aux_loss 172.82278442382812
loss_r_bn_feature 172.82278442382812
------------iteration 1600----------
total loss 204.46836658386582
main criterion 94.65289874939317
weighted_aux_loss 109.81546783447266
loss_r_bn_feature 109.81546783447266
------------iteration 1700----------
total loss 127.57809016902203
main criterion 62.015597798416565
weighted_aux_loss 65.56249237060547
loss_r_bn_feature 65.56249237060547
------------iteration 1800----------
total loss 107.17350612718697
main criterion 54.65283809740181
weighted_aux_loss 52.520668029785156
loss_r_bn_feature 52.520668029785156
------------iteration 1900----------
total loss 116.64072333147988
main criterion 56.41550360491737
weighted_aux_loss 60.2252197265625
loss_r_bn_feature 60.2252197265625
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 410 end_cls 420
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 814.3991700662394
main criterion 198.48980727327069
weighted_aux_loss 615.9093627929688
loss_r_bn_feature 615.9093627929688
------------iteration 100----------
total loss 464.6281987105681
main criterion 109.68102463830245
weighted_aux_loss 354.9471740722656
loss_r_bn_feature 354.9471740722656
------------iteration 200----------
total loss 349.3403095794752
main criterion 95.59774060974867
weighted_aux_loss 253.74256896972656
loss_r_bn_feature 253.74256896972656
------------iteration 300----------
total loss 415.45184300278856
main criterion 150.4393002781792
weighted_aux_loss 265.0125427246094
loss_r_bn_feature 265.0125427246094
------------iteration 400----------
total loss 252.78393054794708
main criterion 90.32833362411897
weighted_aux_loss 162.45559692382812
loss_r_bn_feature 162.45559692382812
------------iteration 500----------
total loss 253.81499449581042
main criterion 90.45813719600574
weighted_aux_loss 163.3568572998047
loss_r_bn_feature 163.3568572998047
------------iteration 600----------
total loss 271.2034218502249
main criterion 108.32135703088896
weighted_aux_loss 162.88206481933594
loss_r_bn_feature 162.88206481933594
------------iteration 700----------
total loss 288.11721289497956
main criterion 117.81220496040925
weighted_aux_loss 170.3050079345703
loss_r_bn_feature 170.3050079345703
------------iteration 800----------
total loss 207.9823660657164
main criterion 79.50270603153673
weighted_aux_loss 128.4796600341797
loss_r_bn_feature 128.4796600341797
------------iteration 900----------
total loss 200.8285544307287
main criterion 77.4488165767248
weighted_aux_loss 123.3797378540039
loss_r_bn_feature 123.3797378540039
------------iteration 1000----------
total loss 350.3184653406715
main criterion 148.10729895883554
weighted_aux_loss 202.21116638183594
loss_r_bn_feature 202.21116638183594
------------iteration 1100----------
total loss 187.07552066985437
main criterion 81.57420841399501
weighted_aux_loss 105.50131225585938
loss_r_bn_feature 105.50131225585938
------------iteration 1200----------
total loss 193.2013261621588
main criterion 92.96448686772521
weighted_aux_loss 100.2368392944336
loss_r_bn_feature 100.2368392944336
------------iteration 1300----------
total loss 145.00314071314352
main criterion 67.60680892603415
weighted_aux_loss 77.39633178710938
loss_r_bn_feature 77.39633178710938
------------iteration 1400----------
total loss 196.9490986025458
main criterion 96.23220254541691
weighted_aux_loss 100.7168960571289
loss_r_bn_feature 100.7168960571289
------------iteration 1500----------
total loss 127.0385502606207
main criterion 62.372534635620696
weighted_aux_loss 64.666015625
loss_r_bn_feature 64.666015625
------------iteration 1600----------
total loss 157.91559644500904
main criterion 78.61798902313403
weighted_aux_loss 79.297607421875
loss_r_bn_feature 79.297607421875
------------iteration 1700----------
total loss 119.43065513470135
main criterion 58.638250959896666
weighted_aux_loss 60.79240417480469
loss_r_bn_feature 60.79240417480469
------------iteration 1800----------
total loss 192.32097956544465
main criterion 93.211451977554
weighted_aux_loss 99.10952758789062
loss_r_bn_feature 99.10952758789062
------------iteration 1900----------
total loss 123.9889297987704
main criterion 67.89478307025477
weighted_aux_loss 56.094146728515625
loss_r_bn_feature 56.094146728515625
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 420 end_cls 430
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 815.3165078290499
main criterion 211.7632241376437
weighted_aux_loss 603.5532836914062
loss_r_bn_feature 603.5532836914062
------------iteration 100----------
total loss 440.5345474983619
main criterion 100.36963050617445
weighted_aux_loss 340.1649169921875
loss_r_bn_feature 340.1649169921875
------------iteration 200----------
total loss 337.0485963567445
main criterion 94.28672501885386
weighted_aux_loss 242.76187133789062
loss_r_bn_feature 242.76187133789062
------------iteration 300----------
total loss 431.8546292658606
main criterion 174.93690465648555
weighted_aux_loss 256.917724609375
loss_r_bn_feature 256.917724609375
------------iteration 400----------
total loss 292.8159688816234
main criterion 94.76328028299059
weighted_aux_loss 198.0526885986328
loss_r_bn_feature 198.0526885986328
------------iteration 500----------
total loss 279.49997822451945
main criterion 97.12731281924601
weighted_aux_loss 182.37266540527344
loss_r_bn_feature 182.37266540527344
------------iteration 600----------
total loss 251.69788501955395
main criterion 97.86411426760081
weighted_aux_loss 153.83377075195312
loss_r_bn_feature 153.83377075195312
------------iteration 700----------
total loss 216.81726173712968
main criterion 83.72091774298907
weighted_aux_loss 133.09634399414062
loss_r_bn_feature 133.09634399414062
------------iteration 800----------
total loss 369.9402929173888
main criterion 164.35559137930284
weighted_aux_loss 205.58470153808594
loss_r_bn_feature 205.58470153808594
------------iteration 900----------
total loss 199.96321763547613
main criterion 77.67743577512456
weighted_aux_loss 122.28578186035156
loss_r_bn_feature 122.28578186035156
------------iteration 1000----------
total loss 244.4790376724069
main criterion 116.37196674955533
weighted_aux_loss 128.10707092285156
loss_r_bn_feature 128.10707092285156
------------iteration 1100----------
total loss 172.7516912698211
main criterion 73.19601957792655
weighted_aux_loss 99.55567169189453
loss_r_bn_feature 99.55567169189453
------------iteration 1200----------
total loss 169.40994430847715
main criterion 76.762567294317
weighted_aux_loss 92.64737701416016
loss_r_bn_feature 92.64737701416016
------------iteration 1300----------
total loss 172.34189240276416
main criterion 79.19750611126025
weighted_aux_loss 93.1443862915039
loss_r_bn_feature 93.1443862915039
------------iteration 1400----------
total loss 191.33151342883878
main criterion 99.27237036243251
weighted_aux_loss 92.05914306640625
loss_r_bn_feature 92.05914306640625
------------iteration 1500----------
total loss 143.29052075983628
main criterion 71.77029523493394
weighted_aux_loss 71.52022552490234
loss_r_bn_feature 71.52022552490234
------------iteration 1600----------
total loss 130.55810466749674
main criterion 66.08528820021158
weighted_aux_loss 64.47281646728516
loss_r_bn_feature 64.47281646728516
------------iteration 1700----------
total loss 113.93683967478952
main criterion 58.23056373484811
weighted_aux_loss 55.706275939941406
loss_r_bn_feature 55.706275939941406
------------iteration 1800----------
total loss 408.06694125224533
main criterion 159.86906527568286
weighted_aux_loss 248.1978759765625
loss_r_bn_feature 248.1978759765625
------------iteration 1900----------
total loss 127.62479062892098
main criterion 63.94147535182136
weighted_aux_loss 63.68331527709961
loss_r_bn_feature 63.68331527709961
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 430 end_cls 440
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 818.8128813627137
main criterion 206.83460987833874
weighted_aux_loss 611.978271484375
loss_r_bn_feature 611.978271484375
------------iteration 100----------
total loss 434.4855350173674
main criterion 105.51910435330493
weighted_aux_loss 328.9664306640625
loss_r_bn_feature 328.9664306640625
------------iteration 200----------
total loss 398.51486138562535
main criterion 141.1346733973441
weighted_aux_loss 257.38018798828125
loss_r_bn_feature 257.38018798828125
------------iteration 300----------
total loss 298.8106594914157
main criterion 89.90517243086886
weighted_aux_loss 208.90548706054688
loss_r_bn_feature 208.90548706054688
------------iteration 400----------
total loss 359.5105606793634
main criterion 130.73680274479304
weighted_aux_loss 228.7737579345703
loss_r_bn_feature 228.7737579345703
------------iteration 500----------
total loss 265.637501262132
main criterion 85.04445316642885
weighted_aux_loss 180.59304809570312
loss_r_bn_feature 180.59304809570312
------------iteration 600----------
total loss 277.9712301752008
main criterion 99.80687775820859
weighted_aux_loss 178.1643524169922
loss_r_bn_feature 178.1643524169922
------------iteration 700----------
total loss 260.9843943352063
main criterion 94.47316386645625
weighted_aux_loss 166.51123046875
loss_r_bn_feature 166.51123046875
------------iteration 800----------
total loss 253.60229081399794
main criterion 100.98331803567763
weighted_aux_loss 152.6189727783203
loss_r_bn_feature 152.6189727783203
------------iteration 900----------
total loss 341.11423486542066
main criterion 150.0138320333894
weighted_aux_loss 191.10040283203125
loss_r_bn_feature 191.10040283203125
------------iteration 1000----------
total loss 199.4313516924614
main criterion 78.2251291582817
weighted_aux_loss 121.20622253417969
loss_r_bn_feature 121.20622253417969
------------iteration 1100----------
total loss 202.45148161762097
main criterion 83.85860136859753
weighted_aux_loss 118.59288024902344
loss_r_bn_feature 118.59288024902344
------------iteration 1200----------
total loss 171.64697546886225
main criterion 73.56382269786617
weighted_aux_loss 98.0831527709961
loss_r_bn_feature 98.0831527709961
------------iteration 1300----------
total loss 172.74528897483563
main criterion 77.03274167258954
weighted_aux_loss 95.7125473022461
loss_r_bn_feature 95.7125473022461
------------iteration 1400----------
total loss 214.76596473608254
main criterion 107.20698951635596
weighted_aux_loss 107.55897521972656
loss_r_bn_feature 107.55897521972656
------------iteration 1500----------
total loss 213.95744355485556
main criterion 105.14975007341025
weighted_aux_loss 108.80769348144531
loss_r_bn_feature 108.80769348144531
------------iteration 1600----------
total loss 133.28483573830783
main criterion 66.12259666360079
weighted_aux_loss 67.16223907470703
loss_r_bn_feature 67.16223907470703
------------iteration 1700----------
total loss 130.90757402462228
main criterion 69.06095537227853
weighted_aux_loss 61.84661865234375
loss_r_bn_feature 61.84661865234375
------------iteration 1800----------
total loss 142.0957176223331
main criterion 75.49634475856358
weighted_aux_loss 66.59937286376953
loss_r_bn_feature 66.59937286376953
------------iteration 1900----------
total loss 140.7299810049506
main criterion 79.11449104523379
weighted_aux_loss 61.6154899597168
loss_r_bn_feature 61.6154899597168
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 440 end_cls 450
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 804.2793789439863
main criterion 194.93434720570508
weighted_aux_loss 609.3450317382812
loss_r_bn_feature 609.3450317382812
------------iteration 100----------
total loss 444.3733952244711
main criterion 99.93156172837737
weighted_aux_loss 344.44183349609375
loss_r_bn_feature 344.44183349609375
------------iteration 200----------
total loss 340.10448230309396
main criterion 89.04846728844551
weighted_aux_loss 251.05601501464844
loss_r_bn_feature 251.05601501464844
------------iteration 300----------
total loss 356.3099454417685
main criterion 138.96611914782318
weighted_aux_loss 217.3438262939453
loss_r_bn_feature 217.3438262939453
------------iteration 400----------
total loss 266.15189903482496
main criterion 88.78426902994215
weighted_aux_loss 177.3676300048828
loss_r_bn_feature 177.3676300048828
------------iteration 500----------
total loss 276.4268089321766
main criterion 93.92833481108283
weighted_aux_loss 182.49847412109375
loss_r_bn_feature 182.49847412109375
------------iteration 600----------
total loss 247.36085597092762
main criterion 88.85389796311512
weighted_aux_loss 158.5069580078125
loss_r_bn_feature 158.5069580078125
------------iteration 700----------
total loss 212.76509835535316
main criterion 79.77335336023597
weighted_aux_loss 132.9917449951172
loss_r_bn_feature 132.9917449951172
------------iteration 800----------
total loss 214.35831266436264
main criterion 94.61363035235091
weighted_aux_loss 119.74468231201172
loss_r_bn_feature 119.74468231201172
------------iteration 900----------
total loss 191.17380401494705
main criterion 76.26223632695876
weighted_aux_loss 114.91156768798828
loss_r_bn_feature 114.91156768798828
------------iteration 1000----------
total loss 176.7985162214967
main criterion 72.86329741046153
weighted_aux_loss 103.93521881103516
loss_r_bn_feature 103.93521881103516
------------iteration 1100----------
total loss 165.93112675271777
main criterion 70.7676364573076
weighted_aux_loss 95.16349029541016
loss_r_bn_feature 95.16349029541016
------------iteration 1200----------
total loss 181.68589455666802
main criterion 77.42146737160941
weighted_aux_loss 104.2644271850586
loss_r_bn_feature 104.2644271850586
------------iteration 1300----------
total loss 155.7907015074711
main criterion 71.8486849059086
weighted_aux_loss 83.9420166015625
loss_r_bn_feature 83.9420166015625
------------iteration 1400----------
total loss 129.25363081901938
main criterion 60.29366225212485
weighted_aux_loss 68.95996856689453
loss_r_bn_feature 68.95996856689453
------------iteration 1500----------
total loss 120.0480051206776
main criterion 57.69746877424206
weighted_aux_loss 62.35053634643555
loss_r_bn_feature 62.35053634643555
------------iteration 1600----------
total loss 147.89716488118955
main criterion 75.40834957357235
weighted_aux_loss 72.48881530761719
loss_r_bn_feature 72.48881530761719
------------iteration 1700----------
total loss 115.75365439209739
main criterion 55.54489889893333
weighted_aux_loss 60.20875549316406
loss_r_bn_feature 60.20875549316406
------------iteration 1800----------
total loss 128.55551591702186
main criterion 71.21761766262735
weighted_aux_loss 57.33789825439453
loss_r_bn_feature 57.33789825439453
------------iteration 1900----------
total loss 120.0882560338245
main criterion 59.8984595860706
weighted_aux_loss 60.189796447753906
loss_r_bn_feature 60.189796447753906
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 450 end_cls 460
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 813.559446574345
main criterion 195.50128007043878
weighted_aux_loss 618.0581665039062
loss_r_bn_feature 618.0581665039062
------------iteration 100----------
total loss 403.5548932763797
main criterion 88.59331490723908
weighted_aux_loss 314.9615783691406
loss_r_bn_feature 314.9615783691406
------------iteration 200----------
total loss 335.0214785381273
main criterion 107.98150051078355
weighted_aux_loss 227.03997802734375
loss_r_bn_feature 227.03997802734375
------------iteration 300----------
total loss 290.98912199661765
main criterion 88.08677824661768
weighted_aux_loss 202.90234375
loss_r_bn_feature 202.90234375
------------iteration 400----------
total loss 293.64995707438595
main criterion 102.14100016520628
weighted_aux_loss 191.5089569091797
loss_r_bn_feature 191.5089569091797
------------iteration 500----------
total loss 258.34029565603385
main criterion 87.02950464040887
weighted_aux_loss 171.310791015625
loss_r_bn_feature 171.310791015625
------------iteration 600----------
total loss 216.75607036121207
main criterion 80.64683269031363
weighted_aux_loss 136.10923767089844
loss_r_bn_feature 136.10923767089844
------------iteration 700----------
total loss 227.97234110082908
main criterion 85.81499246801658
weighted_aux_loss 142.1573486328125
loss_r_bn_feature 142.1573486328125
------------iteration 800----------
total loss 210.088828598601
main criterion 78.60526231442131
weighted_aux_loss 131.4835662841797
loss_r_bn_feature 131.4835662841797
------------iteration 900----------
total loss 201.59523249299139
main criterion 79.01388025910468
weighted_aux_loss 122.58135223388672
loss_r_bn_feature 122.58135223388672
------------iteration 1000----------
total loss 210.5828891227496
main criterion 94.19166902997617
weighted_aux_loss 116.39122009277344
loss_r_bn_feature 116.39122009277344
------------iteration 1100----------
total loss 206.06476690155822
main criterion 84.05230047089415
weighted_aux_loss 122.01246643066406
loss_r_bn_feature 122.01246643066406
------------iteration 1200----------
total loss 152.75178890452656
main criterion 63.020328333237515
weighted_aux_loss 89.73146057128906
loss_r_bn_feature 89.73146057128906
------------iteration 1300----------
total loss 162.16718824078507
main criterion 73.27222974469132
weighted_aux_loss 88.89495849609375
loss_r_bn_feature 88.89495849609375
------------iteration 1400----------
total loss 415.5331365812774
main criterion 147.80016538987118
weighted_aux_loss 267.73297119140625
loss_r_bn_feature 267.73297119140625
------------iteration 1500----------
total loss 123.58521267126793
main criterion 59.43998714636558
weighted_aux_loss 64.14522552490234
loss_r_bn_feature 64.14522552490234
------------iteration 1600----------
total loss 116.45611868536933
main criterion 55.10904989874824
weighted_aux_loss 61.347068786621094
loss_r_bn_feature 61.347068786621094
------------iteration 1700----------
total loss 118.08895653513241
main criterion 59.02441185739804
weighted_aux_loss 59.064544677734375
loss_r_bn_feature 59.064544677734375
------------iteration 1800----------
total loss 158.5525745684381
main criterion 67.64155619685607
weighted_aux_loss 90.91101837158203
loss_r_bn_feature 90.91101837158203
------------iteration 1900----------
total loss 118.51196869821544
main criterion 60.73100098581309
weighted_aux_loss 57.780967712402344
loss_r_bn_feature 57.780967712402344
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 460 end_cls 470
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 809.3776097236452
main criterion 199.74583482130143
weighted_aux_loss 609.6317749023438
loss_r_bn_feature 609.6317749023438
------------iteration 100----------
total loss 485.8669467666017
main criterion 111.39718968652359
weighted_aux_loss 374.4697570800781
loss_r_bn_feature 374.4697570800781
------------iteration 200----------
total loss 413.2058986304005
main criterion 124.74978290774425
weighted_aux_loss 288.45611572265625
loss_r_bn_feature 288.45611572265625
------------iteration 300----------
total loss 317.79834142392906
main criterion 103.07965245908531
weighted_aux_loss 214.71868896484375
loss_r_bn_feature 214.71868896484375
------------iteration 400----------
total loss 281.8185371789316
main criterion 91.61356586545504
weighted_aux_loss 190.20497131347656
loss_r_bn_feature 190.20497131347656
------------iteration 500----------
total loss 294.48463142981745
main criterion 111.19450081458305
weighted_aux_loss 183.29013061523438
loss_r_bn_feature 183.29013061523438
------------iteration 600----------
total loss 293.8453748847845
main criterion 116.38991529005797
weighted_aux_loss 177.45545959472656
loss_r_bn_feature 177.45545959472656
------------iteration 700----------
total loss 254.57536004443728
main criterion 99.26214288135134
weighted_aux_loss 155.31321716308594
loss_r_bn_feature 155.31321716308594
------------iteration 800----------
total loss 215.18445347090613
main criterion 85.94112156172645
weighted_aux_loss 129.2433319091797
loss_r_bn_feature 129.2433319091797
------------iteration 900----------
total loss 252.69576115852055
main criterion 109.52229924445805
weighted_aux_loss 143.1734619140625
loss_r_bn_feature 143.1734619140625
------------iteration 1000----------
total loss 331.73200214977675
main criterion 142.78090656872206
weighted_aux_loss 188.9510955810547
loss_r_bn_feature 188.9510955810547
------------iteration 1100----------
total loss 287.1134655747652
main criterion 124.71147277691365
weighted_aux_loss 162.40199279785156
loss_r_bn_feature 162.40199279785156
------------iteration 1200----------
total loss 179.63476054697185
main criterion 83.47193400888591
weighted_aux_loss 96.16282653808594
loss_r_bn_feature 96.16282653808594
------------iteration 1300----------
total loss 156.2357376327222
main criterion 71.4071014632886
weighted_aux_loss 84.8286361694336
loss_r_bn_feature 84.8286361694336
------------iteration 1400----------
total loss 148.06113816478876
main criterion 69.6911049006286
weighted_aux_loss 78.37003326416016
loss_r_bn_feature 78.37003326416016
------------iteration 1500----------
total loss 122.06836384192749
main criterion 61.0796706046228
weighted_aux_loss 60.98869323730469
loss_r_bn_feature 60.98869323730469
------------iteration 1600----------
total loss 127.79883162968426
main criterion 64.06541411869793
weighted_aux_loss 63.73341751098633
loss_r_bn_feature 63.73341751098633
------------iteration 1700----------
total loss 193.67324235451383
main criterion 98.25855424416226
weighted_aux_loss 95.41468811035156
loss_r_bn_feature 95.41468811035156
------------iteration 1800----------
total loss 119.13570144530559
main criterion 61.83998611327433
weighted_aux_loss 57.29571533203125
loss_r_bn_feature 57.29571533203125
------------iteration 1900----------
total loss 102.81662477437303
main criterion 54.335965289509744
weighted_aux_loss 48.48065948486328
loss_r_bn_feature 48.48065948486328
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 470 end_cls 480
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 831.6797074401006
main criterion 202.2687577330694
weighted_aux_loss 629.4109497070312
loss_r_bn_feature 629.4109497070312
------------iteration 100----------
total loss 438.7948803343885
main criterion 98.4553722777479
weighted_aux_loss 340.3395080566406
loss_r_bn_feature 340.3395080566406
------------iteration 200----------
total loss 398.1949686324934
main criterion 126.74605505827465
weighted_aux_loss 271.44891357421875
loss_r_bn_feature 271.44891357421875
------------iteration 300----------
total loss 333.9804751309651
main criterion 113.42810696690258
weighted_aux_loss 220.5523681640625
loss_r_bn_feature 220.5523681640625
------------iteration 400----------
total loss 270.71177922614265
main criterion 87.66165410407231
weighted_aux_loss 183.0501251220703
loss_r_bn_feature 183.0501251220703
------------iteration 500----------
total loss 262.94937355734703
main criterion 91.0610373757064
weighted_aux_loss 171.88833618164062
loss_r_bn_feature 171.88833618164062
------------iteration 600----------
total loss 261.08657443939546
main criterion 96.39754856048921
weighted_aux_loss 164.68902587890625
loss_r_bn_feature 164.68902587890625
------------iteration 700----------
total loss 264.18093138579127
main criterion 112.25020628813502
weighted_aux_loss 151.93072509765625
loss_r_bn_feature 151.93072509765625
------------iteration 800----------
total loss 338.63105677229044
main criterion 145.13319300275916
weighted_aux_loss 193.49786376953125
loss_r_bn_feature 193.49786376953125
------------iteration 900----------
total loss 221.86581202675552
main criterion 90.20867701698991
weighted_aux_loss 131.65713500976562
loss_r_bn_feature 131.65713500976562
------------iteration 1000----------
total loss 218.22466571157622
main criterion 83.32717425649811
weighted_aux_loss 134.89749145507812
loss_r_bn_feature 134.89749145507812
------------iteration 1100----------
total loss 347.46666048003055
main criterion 151.25282381010868
weighted_aux_loss 196.21383666992188
loss_r_bn_feature 196.21383666992188
------------iteration 1200----------
total loss 171.39909314378684
main criterion 76.50388287767358
weighted_aux_loss 94.89521026611328
loss_r_bn_feature 94.89521026611328
------------iteration 1300----------
total loss 169.7337883938466
main criterion 78.42261590849503
weighted_aux_loss 91.31117248535156
loss_r_bn_feature 91.31117248535156
------------iteration 1400----------
total loss 181.83415096011862
main criterion 77.1156145831655
weighted_aux_loss 104.71853637695312
loss_r_bn_feature 104.71853637695312
------------iteration 1500----------
total loss 201.7898895171203
main criterion 99.4445831206359
weighted_aux_loss 102.34530639648438
loss_r_bn_feature 102.34530639648438
------------iteration 1600----------
total loss 131.62721934231053
main criterion 69.04058375271093
weighted_aux_loss 62.58663558959961
loss_r_bn_feature 62.58663558959961
------------iteration 1700----------
total loss 108.97271865967213
main criterion 56.198289338383056
weighted_aux_loss 52.77442932128906
loss_r_bn_feature 52.77442932128906
------------iteration 1800----------
total loss 129.83041916443833
main criterion 67.6146904046727
weighted_aux_loss 62.215728759765625
loss_r_bn_feature 62.215728759765625
------------iteration 1900----------
total loss 166.66664243531602
main criterion 85.46354032349963
weighted_aux_loss 81.2031021118164
loss_r_bn_feature 81.2031021118164
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 480 end_cls 490
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 826.0074082382056
main criterion 199.20973978117436
weighted_aux_loss 626.7976684570312
loss_r_bn_feature 626.7976684570312
------------iteration 100----------
total loss 467.22134188559085
main criterion 113.36721590902835
weighted_aux_loss 353.8541259765625
loss_r_bn_feature 353.8541259765625
------------iteration 200----------
total loss 377.94000425545084
main criterion 102.23227110115397
weighted_aux_loss 275.7077331542969
loss_r_bn_feature 275.7077331542969
------------iteration 300----------
total loss 287.05508754774036
main criterion 90.81982753797472
weighted_aux_loss 196.23526000976562
loss_r_bn_feature 196.23526000976562
------------iteration 400----------
total loss 288.6938396284893
main criterion 89.7453990767315
weighted_aux_loss 198.9484405517578
loss_r_bn_feature 198.9484405517578
------------iteration 500----------
total loss 396.4070552096454
main criterion 154.07180435515323
weighted_aux_loss 242.3352508544922
loss_r_bn_feature 242.3352508544922
------------iteration 600----------
total loss 235.65238976886303
main criterion 84.02400231769116
weighted_aux_loss 151.62838745117188
loss_r_bn_feature 151.62838745117188
------------iteration 700----------
total loss 263.8374494839938
main criterion 88.87143080723598
weighted_aux_loss 174.9660186767578
loss_r_bn_feature 174.9660186767578
------------iteration 800----------
total loss 216.4846051508437
main criterion 79.85191472115619
weighted_aux_loss 136.6326904296875
loss_r_bn_feature 136.6326904296875
------------iteration 900----------
total loss 262.39189074043725
main criterion 106.94503710274192
weighted_aux_loss 155.4468536376953
loss_r_bn_feature 155.4468536376953
------------iteration 1000----------
total loss 209.8642895798937
main criterion 86.79632693340933
weighted_aux_loss 123.06796264648438
loss_r_bn_feature 123.06796264648438
------------iteration 1100----------
total loss 195.70317575795235
main criterion 80.8795749889094
weighted_aux_loss 114.82360076904297
loss_r_bn_feature 114.82360076904297
------------iteration 1200----------
total loss 176.15344814292234
main criterion 80.93319515219969
weighted_aux_loss 95.22025299072266
loss_r_bn_feature 95.22025299072266
------------iteration 1300----------
total loss 160.66105974942153
main criterion 67.25947130948012
weighted_aux_loss 93.4015884399414
loss_r_bn_feature 93.4015884399414
------------iteration 1400----------
total loss 404.1377959444262
main criterion 149.8419127657153
weighted_aux_loss 254.29588317871094
loss_r_bn_feature 254.29588317871094
------------iteration 1500----------
total loss 167.57528681819286
main criterion 82.27358241145458
weighted_aux_loss 85.30170440673828
loss_r_bn_feature 85.30170440673828
------------iteration 1600----------
total loss 145.07570961167397
main criterion 71.68896797349038
weighted_aux_loss 73.3867416381836
loss_r_bn_feature 73.3867416381836
------------iteration 1700----------
total loss 114.67535731105026
main criterion 55.88287684230026
weighted_aux_loss 58.79248046875
loss_r_bn_feature 58.79248046875
------------iteration 1800----------
total loss 115.36474825419178
main criterion 57.84423281229725
weighted_aux_loss 57.52051544189453
loss_r_bn_feature 57.52051544189453
------------iteration 1900----------
total loss 102.98242517607125
main criterion 53.498668157028284
weighted_aux_loss 49.48375701904297
loss_r_bn_feature 49.48375701904297
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 490 end_cls 500
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 800.9675655866051
main criterion 193.85452847723016
weighted_aux_loss 607.113037109375
loss_r_bn_feature 607.113037109375
------------iteration 100----------
total loss 428.9254290219989
main criterion 95.70588556496766
weighted_aux_loss 333.21954345703125
loss_r_bn_feature 333.21954345703125
------------iteration 200----------
total loss 353.2201550185351
main criterion 91.0284130751757
weighted_aux_loss 262.1917419433594
loss_r_bn_feature 262.1917419433594
------------iteration 300----------
total loss 343.18022071356376
main criterion 100.96357642645438
weighted_aux_loss 242.21664428710938
loss_r_bn_feature 242.21664428710938
------------iteration 400----------
total loss 270.24714263443815
main criterion 87.1343649244772
weighted_aux_loss 183.11277770996094
loss_r_bn_feature 183.11277770996094
------------iteration 500----------
total loss 251.63009715498606
main criterion 87.1315925163142
weighted_aux_loss 164.49850463867188
loss_r_bn_feature 164.49850463867188
------------iteration 600----------
total loss 237.22866195970954
main criterion 85.51299423510018
weighted_aux_loss 151.71566772460938
loss_r_bn_feature 151.71566772460938
------------iteration 700----------
total loss 309.7852558128151
main criterion 89.84929695051039
weighted_aux_loss 219.9359588623047
loss_r_bn_feature 219.9359588623047
------------iteration 800----------
total loss 235.82265708929594
main criterion 98.43583152777249
weighted_aux_loss 137.38682556152344
loss_r_bn_feature 137.38682556152344
------------iteration 900----------
total loss 191.05629197319496
main criterion 76.89684525688637
weighted_aux_loss 114.1594467163086
loss_r_bn_feature 114.1594467163086
------------iteration 1000----------
total loss 185.25976688430399
main criterion 77.51038486526103
weighted_aux_loss 107.74938201904297
loss_r_bn_feature 107.74938201904297
------------iteration 1100----------
total loss 227.76728800516668
main criterion 89.98068217020574
weighted_aux_loss 137.78660583496094
loss_r_bn_feature 137.78660583496094
------------iteration 1200----------
total loss 168.8166468520194
main criterion 69.7034113783866
weighted_aux_loss 99.11323547363281
loss_r_bn_feature 99.11323547363281
------------iteration 1300----------
total loss 192.1860728673655
main criterion 87.88032488152565
weighted_aux_loss 104.30574798583984
loss_r_bn_feature 104.30574798583984
------------iteration 1400----------
total loss 165.60124517991278
main criterion 77.4342529924128
weighted_aux_loss 88.1669921875
loss_r_bn_feature 88.1669921875
------------iteration 1500----------
total loss 140.36420709193615
main criterion 67.72280389369396
weighted_aux_loss 72.64140319824219
loss_r_bn_feature 72.64140319824219
------------iteration 1600----------
total loss 139.52209448851082
main criterion 67.74879431761238
weighted_aux_loss 71.77330017089844
loss_r_bn_feature 71.77330017089844
------------iteration 1700----------
total loss 130.32771381558746
main criterion 59.013245431798396
weighted_aux_loss 71.31446838378906
loss_r_bn_feature 71.31446838378906
------------iteration 1800----------
total loss 235.38072330018062
main criterion 105.56964236756343
weighted_aux_loss 129.8110809326172
loss_r_bn_feature 129.8110809326172
------------iteration 1900----------
total loss 120.83830164192071
main criterion 63.728827459791795
weighted_aux_loss 57.109474182128906
loss_r_bn_feature 57.109474182128906
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 500 end_cls 510
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 812.5963853617762
main criterion 202.72400987349502
weighted_aux_loss 609.8723754882812
loss_r_bn_feature 609.8723754882812
------------iteration 100----------
total loss 445.56622923790553
main criterion 99.8347228902493
weighted_aux_loss 345.73150634765625
loss_r_bn_feature 345.73150634765625
------------iteration 200----------
total loss 364.8113110156982
main criterion 103.60052610358883
weighted_aux_loss 261.2107849121094
loss_r_bn_feature 261.2107849121094
------------iteration 300----------
total loss 325.57172591091165
main criterion 96.51325423122414
weighted_aux_loss 229.0584716796875
loss_r_bn_feature 229.0584716796875
------------iteration 400----------
total loss 297.77501550398773
main criterion 98.96007409773773
weighted_aux_loss 198.81494140625
loss_r_bn_feature 198.81494140625
------------iteration 500----------
total loss 262.4584807427244
main criterion 88.81527700737288
weighted_aux_loss 173.64320373535156
loss_r_bn_feature 173.64320373535156
------------iteration 600----------
total loss 238.5472686601077
main criterion 84.73695066694363
weighted_aux_loss 153.81031799316406
loss_r_bn_feature 153.81031799316406
------------iteration 700----------
total loss 307.5110746970106
main criterion 119.37633958958871
weighted_aux_loss 188.13473510742188
loss_r_bn_feature 188.13473510742188
------------iteration 800----------
total loss 220.29504613964815
main criterion 79.88147192089814
weighted_aux_loss 140.41357421875
loss_r_bn_feature 140.41357421875
------------iteration 900----------
total loss 205.16038644305587
main criterion 78.80670297137618
weighted_aux_loss 126.35368347167969
loss_r_bn_feature 126.35368347167969
------------iteration 1000----------
total loss 204.35475099697396
main criterion 80.77088106289192
weighted_aux_loss 123.58386993408203
loss_r_bn_feature 123.58386993408203
------------iteration 1100----------
total loss 196.4482800110232
main criterion 81.88279691776147
weighted_aux_loss 114.56548309326172
loss_r_bn_feature 114.56548309326172
------------iteration 1200----------
total loss 189.04750479367635
main criterion 88.33200491574665
weighted_aux_loss 100.71549987792969
loss_r_bn_feature 100.71549987792969
------------iteration 1300----------
total loss 156.55131266768336
main criterion 71.1434604948318
weighted_aux_loss 85.40785217285156
loss_r_bn_feature 85.40785217285156
------------iteration 1400----------
total loss 157.78170222302109
main criterion 76.88807124157577
weighted_aux_loss 80.89363098144531
loss_r_bn_feature 80.89363098144531
------------iteration 1500----------
total loss 124.24113401875229
main criterion 59.036795945021815
weighted_aux_loss 65.20433807373047
loss_r_bn_feature 65.20433807373047
------------iteration 1600----------
total loss 217.1473903384184
main criterion 92.16610524320356
weighted_aux_loss 124.98128509521484
loss_r_bn_feature 124.98128509521484
------------iteration 1700----------
total loss 113.81192301846039
main criterion 55.963210097317805
weighted_aux_loss 57.84871292114258
loss_r_bn_feature 57.84871292114258
------------iteration 1800----------
total loss 107.18569271038409
main criterion 53.4678734965169
weighted_aux_loss 53.71781921386719
loss_r_bn_feature 53.71781921386719
------------iteration 1900----------
total loss 306.4645194532046
main criterion 124.80678935066553
weighted_aux_loss 181.65773010253906
loss_r_bn_feature 181.65773010253906
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 510 end_cls 520
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 915.8614443706581
main criterion 199.9138735698769
weighted_aux_loss 715.9475708007812
loss_r_bn_feature 715.9475708007812
------------iteration 100----------
total loss 519.3526736924736
main criterion 147.83564488387978
weighted_aux_loss 371.51702880859375
loss_r_bn_feature 371.51702880859375
------------iteration 200----------
total loss 317.70450591831695
main criterion 98.08406829624661
weighted_aux_loss 219.6204376220703
loss_r_bn_feature 219.6204376220703
------------iteration 300----------
total loss 406.7206604323178
main criterion 158.60487674091155
weighted_aux_loss 248.11578369140625
loss_r_bn_feature 248.11578369140625
------------iteration 400----------
total loss 280.09099462258473
main criterion 104.94962194192068
weighted_aux_loss 175.14137268066406
loss_r_bn_feature 175.14137268066406
------------iteration 500----------
total loss 277.482166148134
main criterion 107.4289892682512
weighted_aux_loss 170.0531768798828
loss_r_bn_feature 170.0531768798828
------------iteration 600----------
total loss 258.5801067802876
main criterion 98.39837460255323
weighted_aux_loss 160.18173217773438
loss_r_bn_feature 160.18173217773438
------------iteration 700----------
total loss 304.535962231194
main criterion 124.62255585912366
weighted_aux_loss 179.9134063720703
loss_r_bn_feature 179.9134063720703
------------iteration 800----------
total loss 378.2604169007019
main criterion 150.17658511359255
weighted_aux_loss 228.08383178710938
loss_r_bn_feature 228.08383178710938
------------iteration 900----------
total loss 209.73440372713264
main criterion 81.74885380037483
weighted_aux_loss 127.98554992675781
loss_r_bn_feature 127.98554992675781
------------iteration 1000----------
total loss 208.05490427630124
main criterion 86.49509745257077
weighted_aux_loss 121.55980682373047
loss_r_bn_feature 121.55980682373047
------------iteration 1100----------
total loss 206.6008064307431
main criterion 86.2391578711728
weighted_aux_loss 120.36164855957031
loss_r_bn_feature 120.36164855957031
------------iteration 1200----------
total loss 303.5091942663861
main criterion 133.7250603552533
weighted_aux_loss 169.7841339111328
loss_r_bn_feature 169.7841339111328
------------iteration 1300----------
total loss 183.1786857645148
main criterion 84.88966141148744
weighted_aux_loss 98.28902435302734
loss_r_bn_feature 98.28902435302734
------------iteration 1400----------
total loss 139.58219568090584
main criterion 66.0358928854957
weighted_aux_loss 73.54630279541016
loss_r_bn_feature 73.54630279541016
------------iteration 1500----------
total loss 135.51421444960408
main criterion 62.008164339740816
weighted_aux_loss 73.50605010986328
loss_r_bn_feature 73.50605010986328
------------iteration 1600----------
total loss 411.9139893472293
main criterion 159.1686585366824
weighted_aux_loss 252.74533081054688
loss_r_bn_feature 252.74533081054688
------------iteration 1700----------
total loss 122.99278301912729
main criterion 63.244362303795256
weighted_aux_loss 59.74842071533203
loss_r_bn_feature 59.74842071533203
------------iteration 1800----------
total loss 117.74801483337637
main criterion 58.49789657776113
weighted_aux_loss 59.250118255615234
loss_r_bn_feature 59.250118255615234
------------iteration 1900----------
total loss 121.62568741413142
main criterion 61.124443822822826
weighted_aux_loss 60.501243591308594
loss_r_bn_feature 60.501243591308594
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 520 end_cls 530
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 808.3633429867996
main criterion 202.54840158054952
weighted_aux_loss 605.81494140625
loss_r_bn_feature 605.81494140625
------------iteration 100----------
total loss 441.183835613384
main criterion 107.41689835752466
weighted_aux_loss 333.7669372558594
loss_r_bn_feature 333.7669372558594
------------iteration 200----------
total loss 383.86335022910237
main criterion 104.69617493613362
weighted_aux_loss 279.16717529296875
loss_r_bn_feature 279.16717529296875
------------iteration 300----------
total loss 374.3727000056957
main criterion 148.00943401448475
weighted_aux_loss 226.36326599121094
loss_r_bn_feature 226.36326599121094
------------iteration 400----------
total loss 335.9432504762599
main criterion 124.2393930543849
weighted_aux_loss 211.703857421875
loss_r_bn_feature 211.703857421875
------------iteration 500----------
total loss 334.08910941844874
main criterion 142.41659354930815
weighted_aux_loss 191.67251586914062
loss_r_bn_feature 191.67251586914062
------------iteration 600----------
total loss 268.16861040311875
main criterion 100.01742632108748
weighted_aux_loss 168.15118408203125
loss_r_bn_feature 168.15118408203125
------------iteration 700----------
total loss 228.88167224059978
main criterion 84.63171801696697
weighted_aux_loss 144.2499542236328
loss_r_bn_feature 144.2499542236328
------------iteration 800----------
total loss 356.59882319899157
main criterion 156.95648921461657
weighted_aux_loss 199.642333984375
loss_r_bn_feature 199.642333984375
------------iteration 900----------
total loss 201.25242914078012
main criterion 75.36586297867073
weighted_aux_loss 125.88656616210938
loss_r_bn_feature 125.88656616210938
------------iteration 1000----------
total loss 198.97953502586927
main criterion 77.17812816551772
weighted_aux_loss 121.80140686035156
loss_r_bn_feature 121.80140686035156
------------iteration 1100----------
total loss 436.0600515180923
main criterion 169.28771265090478
weighted_aux_loss 266.7723388671875
loss_r_bn_feature 266.7723388671875
------------iteration 1200----------
total loss 226.50117685364808
main criterion 103.58419992493712
weighted_aux_loss 122.91697692871094
loss_r_bn_feature 122.91697692871094
------------iteration 1300----------
total loss 172.49415419081942
main criterion 77.4582121131827
weighted_aux_loss 95.03594207763672
loss_r_bn_feature 95.03594207763672
------------iteration 1400----------
total loss 306.632238854982
main criterion 130.1694703002945
weighted_aux_loss 176.4627685546875
loss_r_bn_feature 176.4627685546875
------------iteration 1500----------
total loss 160.3921341341914
main criterion 78.24786228360546
weighted_aux_loss 82.14427185058594
loss_r_bn_feature 82.14427185058594
------------iteration 1600----------
total loss 157.93580880630662
main criterion 74.47417703140427
weighted_aux_loss 83.46163177490234
loss_r_bn_feature 83.46163177490234
------------iteration 1700----------
total loss 127.25599100551625
main criterion 62.97076609096547
weighted_aux_loss 64.28522491455078
loss_r_bn_feature 64.28522491455078
------------iteration 1800----------
total loss 127.34040028551115
main criterion 63.88011891344084
weighted_aux_loss 63.46028137207031
loss_r_bn_feature 63.46028137207031
------------iteration 1900----------
total loss 128.40243258336207
main criterion 69.15892901280543
weighted_aux_loss 59.24350357055664
loss_r_bn_feature 59.24350357055664
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 530 end_cls 540
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 906.8434215438065
main criterion 199.93100699302525
weighted_aux_loss 706.9124145507812
loss_r_bn_feature 706.9124145507812
------------iteration 100----------
total loss 425.06367994815224
main criterion 97.24382521182413
weighted_aux_loss 327.8198547363281
loss_r_bn_feature 327.8198547363281
------------iteration 200----------
total loss 319.09850044725664
main criterion 91.01901741503006
weighted_aux_loss 228.07948303222656
loss_r_bn_feature 228.07948303222656
------------iteration 300----------
total loss 438.65238288123965
main criterion 164.5369348831928
weighted_aux_loss 274.1154479980469
loss_r_bn_feature 274.1154479980469
------------iteration 400----------
total loss 283.98844982937374
main criterion 99.90501477077999
weighted_aux_loss 184.08343505859375
loss_r_bn_feature 184.08343505859375
------------iteration 500----------
total loss 255.4261394938613
main criterion 83.57233395186913
weighted_aux_loss 171.8538055419922
loss_r_bn_feature 171.8538055419922
------------iteration 600----------
total loss 340.62865697549415
main criterion 130.62232457803321
weighted_aux_loss 210.00633239746094
loss_r_bn_feature 210.00633239746094
------------iteration 700----------
total loss 274.21775796471945
main criterion 101.01686074792259
weighted_aux_loss 173.20089721679688
loss_r_bn_feature 173.20089721679688
------------iteration 800----------
total loss 200.79183062446464
main criterion 76.55110033882009
weighted_aux_loss 124.24073028564453
loss_r_bn_feature 124.24073028564453
------------iteration 900----------
total loss 363.3423188749349
main criterion 139.0537904325521
weighted_aux_loss 224.2885284423828
loss_r_bn_feature 224.2885284423828
------------iteration 1000----------
total loss 450.89382657685053
main criterion 174.92535123505363
weighted_aux_loss 275.9684753417969
loss_r_bn_feature 275.9684753417969
------------iteration 1100----------
total loss 169.90137678978311
main criterion 69.16842848656044
weighted_aux_loss 100.73294830322266
loss_r_bn_feature 100.73294830322266
------------iteration 1200----------
total loss 267.16882457926727
main criterion 121.02377453043913
weighted_aux_loss 146.14505004882812
loss_r_bn_feature 146.14505004882812
------------iteration 1300----------
total loss 155.9758296582919
main criterion 69.18298297860439
weighted_aux_loss 86.7928466796875
loss_r_bn_feature 86.7928466796875
------------iteration 1400----------
total loss 167.7009193462149
main criterion 72.43469162404693
weighted_aux_loss 95.26622772216797
loss_r_bn_feature 95.26622772216797
------------iteration 1500----------
total loss 172.66402596720462
main criterion 64.52797860392337
weighted_aux_loss 108.13604736328125
loss_r_bn_feature 108.13604736328125
------------iteration 1600----------
total loss 282.40855484033466
main criterion 113.9782417299831
weighted_aux_loss 168.43031311035156
loss_r_bn_feature 168.43031311035156
------------iteration 1700----------
total loss 178.960706000677
main criterion 82.1326572946223
weighted_aux_loss 96.82804870605469
loss_r_bn_feature 96.82804870605469
------------iteration 1800----------
total loss 103.6556342946456
main criterion 54.23478544821005
weighted_aux_loss 49.42084884643555
loss_r_bn_feature 49.42084884643555
------------iteration 1900----------
total loss 357.80717828085506
main criterion 136.50538995077693
weighted_aux_loss 221.30178833007812
loss_r_bn_feature 221.30178833007812
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 540 end_cls 550
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 834.1594599228711
main criterion 205.98654732521487
weighted_aux_loss 628.1729125976562
loss_r_bn_feature 628.1729125976562
------------iteration 100----------
total loss 446.20123471771444
main criterion 96.16290463958943
weighted_aux_loss 350.038330078125
loss_r_bn_feature 350.038330078125
------------iteration 200----------
total loss 335.3642736547671
main criterion 91.98455868894676
weighted_aux_loss 243.3797149658203
loss_r_bn_feature 243.3797149658203
------------iteration 300----------
total loss 306.3832892382501
main criterion 92.0802344286798
weighted_aux_loss 214.3030548095703
loss_r_bn_feature 214.3030548095703
------------iteration 400----------
total loss 283.57950158827134
main criterion 96.3009829115135
weighted_aux_loss 187.2785186767578
loss_r_bn_feature 187.2785186767578
------------iteration 500----------
total loss 318.93495682185744
main criterion 125.65118912166213
weighted_aux_loss 193.2837677001953
loss_r_bn_feature 193.2837677001953
------------iteration 600----------
total loss 252.40545881525998
main criterion 89.15950239436155
weighted_aux_loss 163.24595642089844
loss_r_bn_feature 163.24595642089844
------------iteration 700----------
total loss 217.19678571970397
main criterion 81.23020246775084
weighted_aux_loss 135.96658325195312
loss_r_bn_feature 135.96658325195312
------------iteration 800----------
total loss 237.42221696945762
main criterion 86.79889543625451
weighted_aux_loss 150.62332153320312
loss_r_bn_feature 150.62332153320312
------------iteration 900----------
total loss 239.62274601756093
main criterion 104.04900029002185
weighted_aux_loss 135.57374572753906
loss_r_bn_feature 135.57374572753906
------------iteration 1000----------
total loss 200.57817656983954
main criterion 84.4288006543122
weighted_aux_loss 116.14937591552734
loss_r_bn_feature 116.14937591552734
------------iteration 1100----------
total loss 293.24466618284134
main criterion 126.10924442991166
weighted_aux_loss 167.1354217529297
loss_r_bn_feature 167.1354217529297
------------iteration 1200----------
total loss 166.23413197753888
main criterion 70.3470317578123
weighted_aux_loss 95.88710021972656
loss_r_bn_feature 95.88710021972656
------------iteration 1300----------
total loss 161.8483791779522
main criterion 73.19054226388972
weighted_aux_loss 88.6578369140625
loss_r_bn_feature 88.6578369140625
------------iteration 1400----------
total loss 156.07918473042866
main criterion 77.94804306783101
weighted_aux_loss 78.13114166259766
loss_r_bn_feature 78.13114166259766
------------iteration 1500----------
total loss 137.74886000712047
main criterion 67.5200620945228
weighted_aux_loss 70.22879791259766
loss_r_bn_feature 70.22879791259766
------------iteration 1600----------
total loss 150.2847056111194
main criterion 76.91432902420533
weighted_aux_loss 73.37037658691406
loss_r_bn_feature 73.37037658691406
------------iteration 1700----------
total loss 119.79300567794115
main criterion 58.49077102828296
weighted_aux_loss 61.3022346496582
loss_r_bn_feature 61.3022346496582
------------iteration 1800----------
total loss 141.75618372027634
main criterion 70.43751916949508
weighted_aux_loss 71.31866455078125
loss_r_bn_feature 71.31866455078125
------------iteration 1900----------
total loss 143.06409428109623
main criterion 70.56996891488528
weighted_aux_loss 72.49412536621094
loss_r_bn_feature 72.49412536621094
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 550 end_cls 560
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 843.9954782303113
main criterion 204.7058053787488
weighted_aux_loss 639.2896728515625
loss_r_bn_feature 639.2896728515625
------------iteration 100----------
total loss 457.3129915821488
main criterion 109.19327112316445
weighted_aux_loss 348.1197204589844
loss_r_bn_feature 348.1197204589844
------------iteration 200----------
total loss 355.70357221460466
main criterion 92.59471601343277
weighted_aux_loss 263.1088562011719
loss_r_bn_feature 263.1088562011719
------------iteration 300----------
total loss 334.03070135616724
main criterion 101.45677252315942
weighted_aux_loss 232.5739288330078
loss_r_bn_feature 232.5739288330078
------------iteration 400----------
total loss 320.638934109052
main criterion 114.2291135524114
weighted_aux_loss 206.40982055664062
loss_r_bn_feature 206.40982055664062
------------iteration 500----------
total loss 321.29847639878665
main criterion 119.20892256577883
weighted_aux_loss 202.0895538330078
loss_r_bn_feature 202.0895538330078
------------iteration 600----------
total loss 264.32194432736577
main criterion 90.46456822873296
weighted_aux_loss 173.8573760986328
loss_r_bn_feature 173.8573760986328
------------iteration 700----------
total loss 377.3516334484867
main criterion 154.840235133057
weighted_aux_loss 222.5113983154297
loss_r_bn_feature 222.5113983154297
------------iteration 800----------
total loss 219.47816064163766
main criterion 82.26099755570017
weighted_aux_loss 137.2171630859375
loss_r_bn_feature 137.2171630859375
------------iteration 900----------
total loss 206.1382853606156
main criterion 75.33930464772497
weighted_aux_loss 130.79898071289062
loss_r_bn_feature 130.79898071289062
------------iteration 1000----------
total loss 211.3340987094412
main criterion 87.10996998629668
weighted_aux_loss 124.22412872314453
loss_r_bn_feature 124.22412872314453
------------iteration 1100----------
total loss 194.7217494599912
main criterion 77.39157978225683
weighted_aux_loss 117.33016967773438
loss_r_bn_feature 117.33016967773438
------------iteration 1200----------
total loss 167.29688102064463
main criterion 72.35448294935557
weighted_aux_loss 94.94239807128906
loss_r_bn_feature 94.94239807128906
------------iteration 1300----------
total loss 157.0967563177702
main criterion 71.54989657655926
weighted_aux_loss 85.54685974121094
loss_r_bn_feature 85.54685974121094
------------iteration 1400----------
total loss 210.25870275760147
main criterion 96.10594702029678
weighted_aux_loss 114.15275573730469
loss_r_bn_feature 114.15275573730469
------------iteration 1500----------
total loss 256.490641396995
main criterion 116.2966258940653
weighted_aux_loss 140.1940155029297
loss_r_bn_feature 140.1940155029297
------------iteration 1600----------
total loss 168.07431653792287
main criterion 85.45355848128224
weighted_aux_loss 82.62075805664062
loss_r_bn_feature 82.62075805664062
------------iteration 1700----------
total loss 121.92216984891688
main criterion 59.69950215482508
weighted_aux_loss 62.2226676940918
loss_r_bn_feature 62.2226676940918
------------iteration 1800----------
total loss 158.1534050893036
main criterion 70.75662316791687
weighted_aux_loss 87.39678192138672
loss_r_bn_feature 87.39678192138672
------------iteration 1900----------
total loss 110.62761546152953
main criterion 54.442118177593976
weighted_aux_loss 56.18549728393555
loss_r_bn_feature 56.18549728393555
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 560 end_cls 570
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 883.1205089407957
main criterion 211.529017241577
weighted_aux_loss 671.5914916992188
loss_r_bn_feature 671.5914916992188
------------iteration 100----------
total loss 582.6312141843272
main criterion 181.41383747534283
weighted_aux_loss 401.2173767089844
loss_r_bn_feature 401.2173767089844
------------iteration 200----------
total loss 372.05328049814415
main criterion 101.4216276661129
weighted_aux_loss 270.63165283203125
loss_r_bn_feature 270.63165283203125
------------iteration 300----------
total loss 312.0558094276011
main criterion 115.81966440806984
weighted_aux_loss 196.23614501953125
loss_r_bn_feature 196.23614501953125
------------iteration 400----------
total loss 387.58919466086957
main criterion 143.80704439231488
weighted_aux_loss 243.7821502685547
loss_r_bn_feature 243.7821502685547
------------iteration 500----------
total loss 274.6590346723713
main criterion 92.75907129346508
weighted_aux_loss 181.89996337890625
loss_r_bn_feature 181.89996337890625
------------iteration 600----------
total loss 258.5245065098911
main criterion 89.26660245715676
weighted_aux_loss 169.25790405273438
loss_r_bn_feature 169.25790405273438
------------iteration 700----------
total loss 262.72020505057566
main criterion 103.41887448416942
weighted_aux_loss 159.30133056640625
loss_r_bn_feature 159.30133056640625
------------iteration 800----------
total loss 395.63259084400806
main criterion 158.73548085865647
weighted_aux_loss 236.89710998535156
loss_r_bn_feature 236.89710998535156
------------iteration 900----------
total loss 191.96180405851058
main criterion 76.87226548429183
weighted_aux_loss 115.08953857421875
loss_r_bn_feature 115.08953857421875
------------iteration 1000----------
total loss 190.26854214832406
main criterion 74.54121670887093
weighted_aux_loss 115.72732543945312
loss_r_bn_feature 115.72732543945312
------------iteration 1100----------
total loss 268.53985293776157
main criterion 118.17664798170688
weighted_aux_loss 150.3632049560547
loss_r_bn_feature 150.3632049560547
------------iteration 1200----------
total loss 163.2986935973869
main criterion 70.28867620236737
weighted_aux_loss 93.01001739501953
loss_r_bn_feature 93.01001739501953
------------iteration 1300----------
total loss 334.58228807494964
main criterion 141.33141832397308
weighted_aux_loss 193.25086975097656
loss_r_bn_feature 193.25086975097656
------------iteration 1400----------
total loss 238.10627862085616
main criterion 113.17892571558272
weighted_aux_loss 124.92735290527344
loss_r_bn_feature 124.92735290527344
------------iteration 1500----------
total loss 135.32179910848834
main criterion 66.0631244869063
weighted_aux_loss 69.25867462158203
loss_r_bn_feature 69.25867462158203
------------iteration 1600----------
total loss 114.86597773608186
main criterion 56.22534129199006
weighted_aux_loss 58.6406364440918
loss_r_bn_feature 58.6406364440918
------------iteration 1700----------
total loss 118.8609027752305
main criterion 62.390237797203156
weighted_aux_loss 56.470664978027344
loss_r_bn_feature 56.470664978027344
------------iteration 1800----------
total loss 158.9475161785043
main criterion 82.27554199637538
weighted_aux_loss 76.6719741821289
loss_r_bn_feature 76.6719741821289
------------iteration 1900----------
total loss 102.6698885651379
main criterion 53.03627116401485
weighted_aux_loss 49.63361740112305
loss_r_bn_feature 49.63361740112305
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 570 end_cls 580
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 844.2248400334263
main criterion 214.47667108811382
weighted_aux_loss 629.7481689453125
loss_r_bn_feature 629.7481689453125
------------iteration 100----------
total loss 495.32236827573223
main criterion 130.68812144956036
weighted_aux_loss 364.6342468261719
loss_r_bn_feature 364.6342468261719
------------iteration 200----------
total loss 365.06855134081616
main criterion 104.19672516894119
weighted_aux_loss 260.871826171875
loss_r_bn_feature 260.871826171875
------------iteration 300----------
total loss 311.80922896414563
main criterion 98.99651534109874
weighted_aux_loss 212.81271362304688
loss_r_bn_feature 212.81271362304688
------------iteration 400----------
total loss 295.2296198562484
main criterion 96.38391673124838
weighted_aux_loss 198.845703125
loss_r_bn_feature 198.845703125
------------iteration 500----------
total loss 248.77326795641403
main criterion 90.59034559313278
weighted_aux_loss 158.18292236328125
loss_r_bn_feature 158.18292236328125
------------iteration 600----------
total loss 240.54737949781426
main criterion 90.14341831617364
weighted_aux_loss 150.40396118164062
loss_r_bn_feature 150.40396118164062
------------iteration 700----------
total loss 376.90054448782905
main criterion 158.00286992728218
weighted_aux_loss 218.89767456054688
loss_r_bn_feature 218.89767456054688
------------iteration 800----------
total loss 227.22382326220603
main criterion 91.21991701220603
weighted_aux_loss 136.00390625
loss_r_bn_feature 136.00390625
------------iteration 900----------
total loss 215.16086053184404
main criterion 89.5170311861409
weighted_aux_loss 125.64382934570312
loss_r_bn_feature 125.64382934570312
------------iteration 1000----------
total loss 212.57014176656952
main criterion 96.82787797262421
weighted_aux_loss 115.74226379394531
loss_r_bn_feature 115.74226379394531
------------iteration 1100----------
total loss 192.7250554616923
main criterion 83.69809318141888
weighted_aux_loss 109.02696228027344
loss_r_bn_feature 109.02696228027344
------------iteration 1200----------
total loss 173.69927176618157
main criterion 78.67443808698235
weighted_aux_loss 95.02483367919922
loss_r_bn_feature 95.02483367919922
------------iteration 1300----------
total loss 258.4974278680346
main criterion 115.72676746764395
weighted_aux_loss 142.77066040039062
loss_r_bn_feature 142.77066040039062
------------iteration 1400----------
total loss 200.4802199070989
main criterion 92.37670428209891
weighted_aux_loss 108.103515625
loss_r_bn_feature 108.103515625
------------iteration 1500----------
total loss 136.17127023321484
main criterion 67.50282083135939
weighted_aux_loss 68.66844940185547
loss_r_bn_feature 68.66844940185547
------------iteration 1600----------
total loss 123.51701713579902
main criterion 60.52795387285957
weighted_aux_loss 62.98906326293945
loss_r_bn_feature 62.98906326293945
------------iteration 1700----------
total loss 120.60046798058987
main criterion 62.936355889525416
weighted_aux_loss 57.66411209106445
loss_r_bn_feature 57.66411209106445
------------iteration 1800----------
total loss 310.19833642366336
main criterion 134.63797265413214
weighted_aux_loss 175.56036376953125
loss_r_bn_feature 175.56036376953125
------------iteration 1900----------
total loss 101.76659671169054
main criterion 54.4200657973839
weighted_aux_loss 47.34653091430664
loss_r_bn_feature 47.34653091430664
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 580 end_cls 590
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 850.1719702182605
main criterion 197.6706884799793
weighted_aux_loss 652.5012817382812
loss_r_bn_feature 652.5012817382812
------------iteration 100----------
total loss 473.7124978662019
main criterion 138.60992828612376
weighted_aux_loss 335.1025695800781
loss_r_bn_feature 335.1025695800781
------------iteration 200----------
total loss 362.77023596989255
main criterion 94.33203406559566
weighted_aux_loss 268.4382019042969
loss_r_bn_feature 268.4382019042969
------------iteration 300----------
total loss 301.9198577297095
main criterion 90.0846984279517
weighted_aux_loss 211.8351593017578
loss_r_bn_feature 211.8351593017578
------------iteration 400----------
total loss 266.3382900357262
main criterion 95.70883446931991
weighted_aux_loss 170.62945556640625
loss_r_bn_feature 170.62945556640625
------------iteration 500----------
total loss 379.26795040216393
main criterion 150.12335811700768
weighted_aux_loss 229.14459228515625
loss_r_bn_feature 229.14459228515625
------------iteration 600----------
total loss 409.35100030082185
main criterion 154.19964837211091
weighted_aux_loss 255.15135192871094
loss_r_bn_feature 255.15135192871094
------------iteration 700----------
total loss 349.1331621527812
main criterion 142.14013541938277
weighted_aux_loss 206.99302673339844
loss_r_bn_feature 206.99302673339844
------------iteration 800----------
total loss 220.07486035916725
main criterion 81.31965711209693
weighted_aux_loss 138.7552032470703
loss_r_bn_feature 138.7552032470703
------------iteration 900----------
total loss 200.29446081950175
main criterion 81.32881598307596
weighted_aux_loss 118.96564483642578
loss_r_bn_feature 118.96564483642578
------------iteration 1000----------
total loss 196.15789868241546
main criterion 79.00559307938812
weighted_aux_loss 117.15230560302734
loss_r_bn_feature 117.15230560302734
------------iteration 1100----------
total loss 214.19019625815173
main criterion 89.14325259360093
weighted_aux_loss 125.04694366455078
loss_r_bn_feature 125.04694366455078
------------iteration 1200----------
total loss 170.54960409933986
main criterion 70.97944418723048
weighted_aux_loss 99.57015991210938
loss_r_bn_feature 99.57015991210938
------------iteration 1300----------
total loss 179.14379633586964
main criterion 87.2032369486626
weighted_aux_loss 91.94055938720703
loss_r_bn_feature 91.94055938720703
------------iteration 1400----------
total loss 136.88942226010752
main criterion 63.29795344907237
weighted_aux_loss 73.59146881103516
loss_r_bn_feature 73.59146881103516
------------iteration 1500----------
total loss 142.14971683461653
main criterion 62.88738773305402
weighted_aux_loss 79.2623291015625
loss_r_bn_feature 79.2623291015625
------------iteration 1600----------
total loss 131.38488564187728
main criterion 63.163839194123376
weighted_aux_loss 68.2210464477539
loss_r_bn_feature 68.2210464477539
------------iteration 1700----------
total loss 136.3075162959824
main criterion 74.1558820796738
weighted_aux_loss 62.151634216308594
loss_r_bn_feature 62.151634216308594
------------iteration 1800----------
total loss 127.26812153857882
main criterion 67.73776417773898
weighted_aux_loss 59.530357360839844
loss_r_bn_feature 59.530357360839844
------------iteration 1900----------
total loss 107.91436406777305
main criterion 55.10855504677696
weighted_aux_loss 52.805809020996094
loss_r_bn_feature 52.805809020996094
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 590 end_cls 600
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 871.3514719246061
main criterion 198.16201879960613
weighted_aux_loss 673.189453125
loss_r_bn_feature 673.189453125
------------iteration 100----------
total loss 424.97393690550496
main criterion 91.22036634886436
weighted_aux_loss 333.7535705566406
loss_r_bn_feature 333.7535705566406
------------iteration 200----------
total loss 351.8113554457491
main criterion 111.35713181293657
weighted_aux_loss 240.4542236328125
loss_r_bn_feature 240.4542236328125
------------iteration 300----------
total loss 305.8923047848598
main criterion 92.61972177704727
weighted_aux_loss 213.2725830078125
loss_r_bn_feature 213.2725830078125
------------iteration 400----------
total loss 299.0357761148279
main criterion 105.90392491853885
weighted_aux_loss 193.13185119628906
loss_r_bn_feature 193.13185119628906
------------iteration 500----------
total loss 296.6915527024141
main criterion 115.17994076393757
weighted_aux_loss 181.51161193847656
loss_r_bn_feature 181.51161193847656
------------iteration 600----------
total loss 287.0508251800291
main criterion 104.81986814877911
weighted_aux_loss 182.23095703125
loss_r_bn_feature 182.23095703125
------------iteration 700----------
total loss 309.6087423775616
main criterion 91.63430084924127
weighted_aux_loss 217.9744415283203
loss_r_bn_feature 217.9744415283203
------------iteration 800----------
total loss 234.18057827433262
main criterion 87.174078030192
weighted_aux_loss 147.00650024414062
loss_r_bn_feature 147.00650024414062
------------iteration 900----------
total loss 214.40316754063505
main criterion 82.0523984976663
weighted_aux_loss 132.35076904296875
loss_r_bn_feature 132.35076904296875
------------iteration 1000----------
total loss 200.99139435742148
main criterion 78.04297669384727
weighted_aux_loss 122.94841766357422
loss_r_bn_feature 122.94841766357422
------------iteration 1100----------
total loss 183.4567805779575
main criterion 81.46008410578952
weighted_aux_loss 101.99669647216797
loss_r_bn_feature 101.99669647216797
------------iteration 1200----------
total loss 156.18650353589783
main criterion 67.44661248365172
weighted_aux_loss 88.7398910522461
loss_r_bn_feature 88.7398910522461
------------iteration 1300----------
total loss 150.53478638738835
main criterion 67.58546845525944
weighted_aux_loss 82.9493179321289
loss_r_bn_feature 82.9493179321289
------------iteration 1400----------
total loss 197.49644977586306
main criterion 98.83643085496462
weighted_aux_loss 98.66001892089844
loss_r_bn_feature 98.66001892089844
------------iteration 1500----------
total loss 146.01002528967925
main criterion 63.526993063116755
weighted_aux_loss 82.4830322265625
loss_r_bn_feature 82.4830322265625
------------iteration 1600----------
total loss 120.85229884938641
main criterion 59.55522548512861
weighted_aux_loss 61.29707336425781
loss_r_bn_feature 61.29707336425781
------------iteration 1700----------
total loss 114.40813339914706
main criterion 56.81086243357088
weighted_aux_loss 57.59727096557617
loss_r_bn_feature 57.59727096557617
------------iteration 1800----------
total loss 111.48424941515906
main criterion 56.51932174181921
weighted_aux_loss 54.964927673339844
loss_r_bn_feature 54.964927673339844
------------iteration 1900----------
total loss 105.66599599071893
main criterion 54.892321979488464
weighted_aux_loss 50.77367401123047
loss_r_bn_feature 50.77367401123047
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 600 end_cls 610
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 833.0039327391089
main criterion 202.91176965317138
weighted_aux_loss 630.0921630859375
loss_r_bn_feature 630.0921630859375
------------iteration 100----------
total loss 471.3540929891952
main criterion 99.71536007903892
weighted_aux_loss 371.63873291015625
loss_r_bn_feature 371.63873291015625
------------iteration 200----------
total loss 434.5426846038957
main criterion 149.01164822694255
weighted_aux_loss 285.5310363769531
loss_r_bn_feature 285.5310363769531
------------iteration 300----------
total loss 329.84220357295845
main criterion 104.15545125362252
weighted_aux_loss 225.68675231933594
loss_r_bn_feature 225.68675231933594
------------iteration 400----------
total loss 312.7852878903711
main criterion 123.39177897923825
weighted_aux_loss 189.3935089111328
loss_r_bn_feature 189.3935089111328
------------iteration 500----------
total loss 262.19682562797755
main criterion 90.2387872978994
weighted_aux_loss 171.95803833007812
loss_r_bn_feature 171.95803833007812
------------iteration 600----------
total loss 258.6790159491411
main criterion 88.30522139347707
weighted_aux_loss 170.37379455566406
loss_r_bn_feature 170.37379455566406
------------iteration 700----------
total loss 423.5800536373831
main criterion 174.18579704558624
weighted_aux_loss 249.39425659179688
loss_r_bn_feature 249.39425659179688
------------iteration 800----------
total loss 238.59468411621563
main criterion 93.28335904297344
weighted_aux_loss 145.3113250732422
loss_r_bn_feature 145.3113250732422
------------iteration 900----------
total loss 382.2817221302346
main criterion 159.5762320179299
weighted_aux_loss 222.7054901123047
loss_r_bn_feature 222.7054901123047
------------iteration 1000----------
total loss 323.3569768258552
main criterion 141.55070241179268
weighted_aux_loss 181.8062744140625
loss_r_bn_feature 181.8062744140625
------------iteration 1100----------
total loss 306.7388140401291
main criterion 137.35101191610565
weighted_aux_loss 169.38780212402344
loss_r_bn_feature 169.38780212402344
------------iteration 1200----------
total loss 181.5414630916742
main criterion 76.90232582360777
weighted_aux_loss 104.6391372680664
loss_r_bn_feature 104.6391372680664
------------iteration 1300----------
total loss 204.2632925925946
main criterion 94.34824590070009
weighted_aux_loss 109.91504669189453
loss_r_bn_feature 109.91504669189453
------------iteration 1400----------
total loss 140.511882604961
main criterion 65.88349515378913
weighted_aux_loss 74.62838745117188
loss_r_bn_feature 74.62838745117188
------------iteration 1500----------
total loss 154.02690719401318
main criterion 78.43268417155224
weighted_aux_loss 75.59422302246094
loss_r_bn_feature 75.59422302246094
------------iteration 1600----------
total loss 256.86288530622573
main criterion 123.2766884068117
weighted_aux_loss 133.58619689941406
loss_r_bn_feature 133.58619689941406
------------iteration 1700----------
total loss 195.0885295192953
main criterion 91.42032425806482
weighted_aux_loss 103.66820526123047
loss_r_bn_feature 103.66820526123047
------------iteration 1800----------
total loss 314.31543684518937
main criterion 126.29096174753312
weighted_aux_loss 188.02447509765625
loss_r_bn_feature 188.02447509765625
------------iteration 1900----------
total loss 171.8513747244824
main criterion 84.11329946813476
weighted_aux_loss 87.73807525634766
loss_r_bn_feature 87.73807525634766
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 610 end_cls 620
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 822.0405503273092
main criterion 193.21230325699676
weighted_aux_loss 628.8282470703125
loss_r_bn_feature 628.8282470703125
------------iteration 100----------
total loss 663.8331293819155
main criterion 185.97692821004046
weighted_aux_loss 477.856201171875
loss_r_bn_feature 477.856201171875
------------iteration 200----------
total loss 452.498009805274
main criterion 145.10006058652402
weighted_aux_loss 307.39794921875
loss_r_bn_feature 307.39794921875
------------iteration 300----------
total loss 352.9419501225032
main criterion 132.20952824750321
weighted_aux_loss 220.732421875
loss_r_bn_feature 220.732421875
------------iteration 400----------
total loss 268.05917128399636
main criterion 86.60368117169166
weighted_aux_loss 181.4554901123047
loss_r_bn_feature 181.4554901123047
------------iteration 500----------
total loss 347.2820964293655
main criterion 145.69882921745142
weighted_aux_loss 201.58326721191406
loss_r_bn_feature 201.58326721191406
------------iteration 600----------
total loss 244.16311998731823
main criterion 98.25227709181043
weighted_aux_loss 145.9108428955078
loss_r_bn_feature 145.9108428955078
------------iteration 700----------
total loss 245.62236492819488
main criterion 87.79915325827302
weighted_aux_loss 157.82321166992188
loss_r_bn_feature 157.82321166992188
------------iteration 800----------
total loss 350.40419256599495
main criterion 144.02089178474495
weighted_aux_loss 206.38330078125
loss_r_bn_feature 206.38330078125
------------iteration 900----------
total loss 259.21072090247077
main criterion 80.66094673254888
weighted_aux_loss 178.54977416992188
loss_r_bn_feature 178.54977416992188
------------iteration 1000----------
total loss 195.71241711243778
main criterion 77.19068096741825
weighted_aux_loss 118.52173614501953
loss_r_bn_feature 118.52173614501953
------------iteration 1100----------
total loss 230.79878340731312
main criterion 106.50613509188342
weighted_aux_loss 124.29264831542969
loss_r_bn_feature 124.29264831542969
------------iteration 1200----------
total loss 161.678985850752
main criterion 72.59500147575201
weighted_aux_loss 89.083984375
loss_r_bn_feature 89.083984375
------------iteration 1300----------
total loss 159.28545338989233
main criterion 72.08347279907203
weighted_aux_loss 87.20198059082031
loss_r_bn_feature 87.20198059082031
------------iteration 1400----------
total loss 252.56620017416782
main criterion 114.61353293783971
weighted_aux_loss 137.95266723632812
loss_r_bn_feature 137.95266723632812
------------iteration 1500----------
total loss 261.3136022538067
main criterion 112.14158992470513
weighted_aux_loss 149.17201232910156
loss_r_bn_feature 149.17201232910156
------------iteration 1600----------
total loss 120.9163813056353
main criterion 57.533885425508345
weighted_aux_loss 63.38249588012695
loss_r_bn_feature 63.38249588012695
------------iteration 1700----------
total loss 143.1947551230732
main criterion 74.76337389748726
weighted_aux_loss 68.43138122558594
loss_r_bn_feature 68.43138122558594
------------iteration 1800----------
total loss 172.11024155328
main criterion 68.42425980279172
weighted_aux_loss 103.68598175048828
loss_r_bn_feature 103.68598175048828
------------iteration 1900----------
total loss 126.29562162190695
main criterion 67.46263288289327
weighted_aux_loss 58.83298873901367
loss_r_bn_feature 58.83298873901367
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 620 end_cls 630
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 821.4094873489394
main criterion 200.0206323684706
weighted_aux_loss 621.3888549804688
loss_r_bn_feature 621.3888549804688
------------iteration 100----------
total loss 459.9162299547918
main criterion 104.37457346065119
weighted_aux_loss 355.5416564941406
loss_r_bn_feature 355.5416564941406
------------iteration 200----------
total loss 401.0931403694463
main criterion 110.11300731280568
weighted_aux_loss 290.9801330566406
loss_r_bn_feature 290.9801330566406
------------iteration 300----------
total loss 342.43998301159405
main criterion 109.75826609264875
weighted_aux_loss 232.6817169189453
loss_r_bn_feature 232.6817169189453
------------iteration 400----------
total loss 295.4548733879835
main criterion 95.99118930595225
weighted_aux_loss 199.46368408203125
loss_r_bn_feature 199.46368408203125
------------iteration 500----------
total loss 285.6438324008701
main criterion 94.73005981786231
weighted_aux_loss 190.9137725830078
loss_r_bn_feature 190.9137725830078
------------iteration 600----------
total loss 243.59491499163573
main criterion 87.86662824847166
weighted_aux_loss 155.72828674316406
loss_r_bn_feature 155.72828674316406
------------iteration 700----------
total loss 231.2717800744577
main criterion 83.66243559203582
weighted_aux_loss 147.60934448242188
loss_r_bn_feature 147.60934448242188
------------iteration 800----------
total loss 226.97977963981293
main criterion 87.53793088493013
weighted_aux_loss 139.4418487548828
loss_r_bn_feature 139.4418487548828
------------iteration 900----------
total loss 225.77288150733935
main criterion 84.04862308448779
weighted_aux_loss 141.72425842285156
loss_r_bn_feature 141.72425842285156
------------iteration 1000----------
total loss 226.22509613690147
main criterion 98.7228302067257
weighted_aux_loss 127.50226593017578
loss_r_bn_feature 127.50226593017578
------------iteration 1100----------
total loss 200.63286271498083
main criterion 80.6689344923246
weighted_aux_loss 119.96392822265625
loss_r_bn_feature 119.96392822265625
------------iteration 1200----------
total loss 201.21184418808292
main criterion 91.03427003036808
weighted_aux_loss 110.17757415771484
loss_r_bn_feature 110.17757415771484
------------iteration 1300----------
total loss 149.30965997697237
main criterion 66.1304683876169
weighted_aux_loss 83.17919158935547
loss_r_bn_feature 83.17919158935547
------------iteration 1400----------
total loss 217.18986889037194
main criterion 98.91691967162195
weighted_aux_loss 118.27294921875
loss_r_bn_feature 118.27294921875
------------iteration 1500----------
total loss 144.44451672410264
main criterion 67.45753247117294
weighted_aux_loss 76.98698425292969
loss_r_bn_feature 76.98698425292969
------------iteration 1600----------
total loss 134.91069764527876
main criterion 68.99824494752485
weighted_aux_loss 65.9124526977539
loss_r_bn_feature 65.9124526977539
------------iteration 1700----------
total loss 113.14568572327701
main criterion 51.930384209605144
weighted_aux_loss 61.215301513671875
loss_r_bn_feature 61.215301513671875
------------iteration 1800----------
total loss 104.69366446754671
main criterion 52.98214331886508
weighted_aux_loss 51.71152114868164
loss_r_bn_feature 51.71152114868164
------------iteration 1900----------
total loss 130.075502894963
main criterion 69.06222393378137
weighted_aux_loss 61.01327896118164
loss_r_bn_feature 61.01327896118164
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 630 end_cls 640
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 846.6871703638641
main criterion 197.8041747583953
weighted_aux_loss 648.8829956054688
loss_r_bn_feature 648.8829956054688
------------iteration 100----------
total loss 474.8230421688971
main criterion 101.63313128022519
weighted_aux_loss 373.1899108886719
loss_r_bn_feature 373.1899108886719
------------iteration 200----------
total loss 367.5170065457255
main criterion 97.02027192658487
weighted_aux_loss 270.4967346191406
loss_r_bn_feature 270.4967346191406
------------iteration 300----------
total loss 302.77798305462454
main criterion 94.97814784954645
weighted_aux_loss 207.79983520507812
loss_r_bn_feature 207.79983520507812
------------iteration 400----------
total loss 283.2632410384423
main criterion 92.39980720055169
weighted_aux_loss 190.86343383789062
loss_r_bn_feature 190.86343383789062
------------iteration 500----------
total loss 264.2721906638922
main criterion 90.67239818342347
weighted_aux_loss 173.59979248046875
loss_r_bn_feature 173.59979248046875
------------iteration 600----------
total loss 470.2559309686129
main criterion 182.5395918572848
weighted_aux_loss 287.7163391113281
loss_r_bn_feature 287.7163391113281
------------iteration 700----------
total loss 311.12408725614637
main criterion 137.55760471220108
weighted_aux_loss 173.5664825439453
loss_r_bn_feature 173.5664825439453
------------iteration 800----------
total loss 318.4974682230012
main criterion 142.76132320346997
weighted_aux_loss 175.73614501953125
loss_r_bn_feature 175.73614501953125
------------iteration 900----------
total loss 214.70465036017833
main criterion 80.19120736701429
weighted_aux_loss 134.51344299316406
loss_r_bn_feature 134.51344299316406
------------iteration 1000----------
total loss 233.26024647289
main criterion 95.12664051585874
weighted_aux_loss 138.13360595703125
loss_r_bn_feature 138.13360595703125
------------iteration 1100----------
total loss 175.03232381445196
main criterion 72.606428123534
weighted_aux_loss 102.42589569091797
loss_r_bn_feature 102.42589569091797
------------iteration 1200----------
total loss 323.4341930054515
main criterion 140.8452495240062
weighted_aux_loss 182.5889434814453
loss_r_bn_feature 182.5889434814453
------------iteration 1300----------
total loss 217.03596163912806
main criterion 97.48526431246789
weighted_aux_loss 119.55069732666016
loss_r_bn_feature 119.55069732666016
------------iteration 1400----------
total loss 166.73274419420036
main criterion 82.82959935777458
weighted_aux_loss 83.90314483642578
loss_r_bn_feature 83.90314483642578
------------iteration 1500----------
total loss 126.38995627430594
main criterion 59.25915030506767
weighted_aux_loss 67.13080596923828
loss_r_bn_feature 67.13080596923828
------------iteration 1600----------
total loss 117.98679377137759
main criterion 56.92606379090885
weighted_aux_loss 61.06072998046875
loss_r_bn_feature 61.06072998046875
------------iteration 1700----------
total loss 131.4900560089058
main criterion 65.43726822814409
weighted_aux_loss 66.05278778076172
loss_r_bn_feature 66.05278778076172
------------iteration 1800----------
total loss 113.0478165442545
main criterion 57.42691352911778
weighted_aux_loss 55.62090301513672
loss_r_bn_feature 55.62090301513672
------------iteration 1900----------
total loss 171.45000065259512
main criterion 78.72616184644276
weighted_aux_loss 92.72383880615234
loss_r_bn_feature 92.72383880615234
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 640 end_cls 650
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 801.7949348492004
main criterion 204.91700516170047
weighted_aux_loss 596.8779296875
loss_r_bn_feature 596.8779296875
------------iteration 100----------
total loss 500.17094570860684
main criterion 148.56446987852868
weighted_aux_loss 351.6064758300781
loss_r_bn_feature 351.6064758300781
------------iteration 200----------
total loss 400.8127008958284
main criterion 124.26860299543777
weighted_aux_loss 276.5440979003906
loss_r_bn_feature 276.5440979003906
------------iteration 300----------
total loss 367.2554354327649
main criterion 139.9709963458508
weighted_aux_loss 227.28443908691406
loss_r_bn_feature 227.28443908691406
------------iteration 400----------
total loss 292.2272495844774
main criterion 90.22393842725087
weighted_aux_loss 202.00331115722656
loss_r_bn_feature 202.00331115722656
------------iteration 500----------
total loss 271.81361917484173
main criterion 91.05811075199014
weighted_aux_loss 180.75550842285156
loss_r_bn_feature 180.75550842285156
------------iteration 600----------
total loss 253.5428474255592
main criterion 89.33567884645763
weighted_aux_loss 164.20716857910156
loss_r_bn_feature 164.20716857910156
------------iteration 700----------
total loss 267.1263314752664
main criterion 113.38550200749299
weighted_aux_loss 153.74082946777344
loss_r_bn_feature 153.74082946777344
------------iteration 800----------
total loss 247.5497220524986
main criterion 97.17859778492047
weighted_aux_loss 150.37112426757812
loss_r_bn_feature 150.37112426757812
------------iteration 900----------
total loss 195.53922556045933
main criterion 78.55105875137731
weighted_aux_loss 116.98816680908203
loss_r_bn_feature 116.98816680908203
------------iteration 1000----------
total loss 198.2837665974496
main criterion 84.1737278401254
weighted_aux_loss 114.11003875732422
loss_r_bn_feature 114.11003875732422
------------iteration 1100----------
total loss 170.3559083825554
main criterion 71.71014117064134
weighted_aux_loss 98.64576721191406
loss_r_bn_feature 98.64576721191406
------------iteration 1200----------
total loss 254.20072439361974
main criterion 118.75278738190099
weighted_aux_loss 135.44793701171875
loss_r_bn_feature 135.44793701171875
------------iteration 1300----------
total loss 164.79188768332176
main criterion 73.69098131125145
weighted_aux_loss 91.10090637207031
loss_r_bn_feature 91.10090637207031
------------iteration 1400----------
total loss 260.13870185278006
main criterion 115.45230048559259
weighted_aux_loss 144.6864013671875
loss_r_bn_feature 144.6864013671875
------------iteration 1500----------
total loss 141.581062604064
main criterion 61.060241986388206
weighted_aux_loss 80.52082061767578
loss_r_bn_feature 80.52082061767578
------------iteration 1600----------
total loss 113.19196665907194
main criterion 56.30878794813443
weighted_aux_loss 56.8831787109375
loss_r_bn_feature 56.8831787109375
------------iteration 1700----------
total loss 106.25477640126641
main criterion 55.04292337392267
weighted_aux_loss 51.21185302734375
loss_r_bn_feature 51.21185302734375
------------iteration 1800----------
total loss 119.88265146853145
main criterion 65.24138178469356
weighted_aux_loss 54.64126968383789
loss_r_bn_feature 54.64126968383789
------------iteration 1900----------
total loss 121.1448207500891
main criterion 54.0368877056555
weighted_aux_loss 67.1079330444336
loss_r_bn_feature 67.1079330444336
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 650 end_cls 660
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 806.0748112152904
main criterion 195.7676823090404
weighted_aux_loss 610.30712890625
loss_r_bn_feature 610.30712890625
------------iteration 100----------
total loss 582.9155031662522
main criterion 177.58978905492407
weighted_aux_loss 405.3257141113281
loss_r_bn_feature 405.3257141113281
------------iteration 200----------
total loss 402.95046144683613
main criterion 113.235343038633
weighted_aux_loss 289.7151184082031
loss_r_bn_feature 289.7151184082031
------------iteration 300----------
total loss 324.1688260287893
main criterion 104.69046299167991
weighted_aux_loss 219.47836303710938
loss_r_bn_feature 219.47836303710938
------------iteration 400----------
total loss 266.3838060828702
main criterion 89.06761345591705
weighted_aux_loss 177.31619262695312
loss_r_bn_feature 177.31619262695312
------------iteration 500----------
total loss 256.30463238151145
main criterion 93.53508587272238
weighted_aux_loss 162.76954650878906
loss_r_bn_feature 162.76954650878906
------------iteration 600----------
total loss 241.40498421091286
main criterion 84.7893683661863
weighted_aux_loss 156.61561584472656
loss_r_bn_feature 156.61561584472656
------------iteration 700----------
total loss 277.67595079383034
main criterion 109.66259935340067
weighted_aux_loss 168.0133514404297
loss_r_bn_feature 168.0133514404297
------------iteration 800----------
total loss 278.41588907529854
main criterion 112.62485819150947
weighted_aux_loss 165.79103088378906
loss_r_bn_feature 165.79103088378906
------------iteration 900----------
total loss 196.2961437620467
main criterion 76.81179927962485
weighted_aux_loss 119.48434448242188
loss_r_bn_feature 119.48434448242188
------------iteration 1000----------
total loss 227.72554242087202
main criterion 93.66331707907514
weighted_aux_loss 134.06222534179688
loss_r_bn_feature 134.06222534179688
------------iteration 1100----------
total loss 190.4084333403652
main criterion 76.28832378225975
weighted_aux_loss 114.12010955810547
loss_r_bn_feature 114.12010955810547
------------iteration 1200----------
total loss 232.61198582279266
main criterion 102.78833164798797
weighted_aux_loss 129.8236541748047
loss_r_bn_feature 129.8236541748047
------------iteration 1300----------
total loss 151.08538265295854
main criterion 67.08870143957964
weighted_aux_loss 83.9966812133789
loss_r_bn_feature 83.9966812133789
------------iteration 1400----------
total loss 193.7687780499939
main criterion 95.71542569403687
weighted_aux_loss 98.05335235595703
loss_r_bn_feature 98.05335235595703
------------iteration 1500----------
total loss 131.34948900177363
main criterion 60.47661760284786
weighted_aux_loss 70.87287139892578
loss_r_bn_feature 70.87287139892578
------------iteration 1600----------
total loss 121.1974014228833
main criterion 59.216188806916506
weighted_aux_loss 61.9812126159668
loss_r_bn_feature 61.9812126159668
------------iteration 1700----------
total loss 113.28560893190372
main criterion 55.80264918458927
weighted_aux_loss 57.48295974731445
loss_r_bn_feature 57.48295974731445
------------iteration 1800----------
total loss 174.99999471997685
main criterion 74.03823324536748
weighted_aux_loss 100.96176147460938
loss_r_bn_feature 100.96176147460938
------------iteration 1900----------
total loss 106.31864835732759
main criterion 55.31048109048188
weighted_aux_loss 51.0081672668457
loss_r_bn_feature 51.0081672668457
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 660 end_cls 670
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 818.4099576147896
main criterion 201.2748868140084
weighted_aux_loss 617.1350708007812
loss_r_bn_feature 617.1350708007812
------------iteration 100----------
total loss 446.53614877192206
main criterion 98.4799659105939
weighted_aux_loss 348.0561828613281
loss_r_bn_feature 348.0561828613281
------------iteration 200----------
total loss 357.70151620187556
main criterion 94.77396493234431
weighted_aux_loss 262.92755126953125
loss_r_bn_feature 262.92755126953125
------------iteration 300----------
total loss 310.1811848926331
main criterion 109.68557942388308
weighted_aux_loss 200.49560546875
loss_r_bn_feature 200.49560546875
------------iteration 400----------
total loss 338.9331442697386
main criterion 130.9029929025511
weighted_aux_loss 208.0301513671875
loss_r_bn_feature 208.0301513671875
------------iteration 500----------
total loss 381.8223956636184
main criterion 148.70307193314963
weighted_aux_loss 233.11932373046875
loss_r_bn_feature 233.11932373046875
------------iteration 600----------
total loss 391.80205071621265
main criterion 156.08856499843918
weighted_aux_loss 235.71348571777344
loss_r_bn_feature 235.71348571777344
------------iteration 700----------
total loss 235.81972122328898
main criterion 85.19911575453897
weighted_aux_loss 150.62060546875
loss_r_bn_feature 150.62060546875
------------iteration 800----------
total loss 389.2585466642971
main criterion 154.62474234300805
weighted_aux_loss 234.63380432128906
loss_r_bn_feature 234.63380432128906
------------iteration 900----------
total loss 218.5796788298892
main criterion 84.43237048027983
weighted_aux_loss 134.14730834960938
loss_r_bn_feature 134.14730834960938
------------iteration 1000----------
total loss 394.43181301038305
main criterion 157.62978053967996
weighted_aux_loss 236.80203247070312
loss_r_bn_feature 236.80203247070312
------------iteration 1100----------
total loss 183.00816163540526
main criterion 78.84355744839355
weighted_aux_loss 104.16460418701172
loss_r_bn_feature 104.16460418701172
------------iteration 1200----------
total loss 171.06339439444065
main criterion 71.64266380362035
weighted_aux_loss 99.42073059082031
loss_r_bn_feature 99.42073059082031
------------iteration 1300----------
total loss 165.53012161882629
main criterion 71.40498428972474
weighted_aux_loss 94.12513732910156
loss_r_bn_feature 94.12513732910156
------------iteration 1400----------
total loss 157.1191403593199
main criterion 77.6834180082457
weighted_aux_loss 79.43572235107422
loss_r_bn_feature 79.43572235107422
------------iteration 1500----------
total loss 198.01806384470927
main criterion 84.32279712107648
weighted_aux_loss 113.69526672363281
loss_r_bn_feature 113.69526672363281
------------iteration 1600----------
total loss 120.65306037413357
main criterion 57.28153556334256
weighted_aux_loss 63.371524810791016
loss_r_bn_feature 63.371524810791016
------------iteration 1700----------
total loss 113.79933601615286
main criterion 57.663399279092324
weighted_aux_loss 56.13593673706055
loss_r_bn_feature 56.13593673706055
------------iteration 1800----------
total loss 121.32437317933723
main criterion 61.80507081117317
weighted_aux_loss 59.51930236816406
loss_r_bn_feature 59.51930236816406
------------iteration 1900----------
total loss 336.9261455286454
main criterion 125.41793630012978
weighted_aux_loss 211.50820922851562
loss_r_bn_feature 211.50820922851562
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 670 end_cls 680
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 844.2351604756147
main criterion 201.13372004592725
weighted_aux_loss 643.1014404296875
loss_r_bn_feature 643.1014404296875
------------iteration 100----------
total loss 433.301744095759
main criterion 93.61378633208709
weighted_aux_loss 339.6879577636719
loss_r_bn_feature 339.6879577636719
------------iteration 200----------
total loss 377.2009945551241
main criterion 126.45703703559286
weighted_aux_loss 250.74395751953125
loss_r_bn_feature 250.74395751953125
------------iteration 300----------
total loss 337.31535868907383
main criterion 140.7521110084098
weighted_aux_loss 196.56324768066406
loss_r_bn_feature 196.56324768066406
------------iteration 400----------
total loss 273.12302494022975
main criterion 101.99538516972196
weighted_aux_loss 171.1276397705078
loss_r_bn_feature 171.1276397705078
------------iteration 500----------
total loss 284.6084872646083
main criterion 95.78567232320204
weighted_aux_loss 188.82281494140625
loss_r_bn_feature 188.82281494140625
------------iteration 600----------
total loss 243.14980266504338
main criterion 85.2358164589887
weighted_aux_loss 157.9139862060547
loss_r_bn_feature 157.9139862060547
------------iteration 700----------
total loss 246.36838460001528
main criterion 91.82883381876528
weighted_aux_loss 154.53955078125
loss_r_bn_feature 154.53955078125
------------iteration 800----------
total loss 245.29374949279543
main criterion 104.68431345763916
weighted_aux_loss 140.60943603515625
loss_r_bn_feature 140.60943603515625
------------iteration 900----------
total loss 200.7004071685258
main criterion 80.24788489069377
weighted_aux_loss 120.45252227783203
loss_r_bn_feature 120.45252227783203
------------iteration 1000----------
total loss 354.22671624789405
main criterion 160.1818096316831
weighted_aux_loss 194.04490661621094
loss_r_bn_feature 194.04490661621094
------------iteration 1100----------
total loss 192.30694581856886
main criterion 88.42877199044386
weighted_aux_loss 103.878173828125
loss_r_bn_feature 103.878173828125
------------iteration 1200----------
total loss 169.16463666712394
main criterion 73.26533704554193
weighted_aux_loss 95.89929962158203
loss_r_bn_feature 95.89929962158203
------------iteration 1300----------
total loss 157.59967628897675
main criterion 68.98641029776581
weighted_aux_loss 88.61326599121094
loss_r_bn_feature 88.61326599121094
------------iteration 1400----------
total loss 166.1708380149573
main criterion 84.50603546368777
weighted_aux_loss 81.66480255126953
loss_r_bn_feature 81.66480255126953
------------iteration 1500----------
total loss 158.63438992954104
main criterion 76.64608579135744
weighted_aux_loss 81.9883041381836
loss_r_bn_feature 81.9883041381836
------------iteration 1600----------
total loss 123.98922675850736
main criterion 60.08143562081204
weighted_aux_loss 63.90779113769531
loss_r_bn_feature 63.90779113769531
------------iteration 1700----------
total loss 119.25633366331525
main criterion 61.0223973535008
weighted_aux_loss 58.23393630981445
loss_r_bn_feature 58.23393630981445
------------iteration 1800----------
total loss 111.81930509620881
main criterion 54.963077221452956
weighted_aux_loss 56.85622787475586
loss_r_bn_feature 56.85622787475586
------------iteration 1900----------
total loss 420.33353641443864
main criterion 150.72712161951677
weighted_aux_loss 269.6064147949219
loss_r_bn_feature 269.6064147949219
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 680 end_cls 690
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 835.9116988319604
main criterion 210.0454268593042
weighted_aux_loss 625.8662719726562
loss_r_bn_feature 625.8662719726562
------------iteration 100----------
total loss 463.26675313706255
main criterion 102.42748922104695
weighted_aux_loss 360.8392639160156
loss_r_bn_feature 360.8392639160156
------------iteration 200----------
total loss 391.37643791749497
main criterion 101.25839592530748
weighted_aux_loss 290.1180419921875
loss_r_bn_feature 290.1180419921875
------------iteration 300----------
total loss 330.4127393200162
main criterion 109.19868902704745
weighted_aux_loss 221.21405029296875
loss_r_bn_feature 221.21405029296875
------------iteration 400----------
total loss 319.2543916246167
main criterion 110.78640456406981
weighted_aux_loss 208.46798706054688
loss_r_bn_feature 208.46798706054688
------------iteration 500----------
total loss 270.6075549318854
main criterion 91.35152771020572
weighted_aux_loss 179.2560272216797
loss_r_bn_feature 179.2560272216797
------------iteration 600----------
total loss 250.90926693902338
main criterion 83.98833798394527
weighted_aux_loss 166.92092895507812
loss_r_bn_feature 166.92092895507812
------------iteration 700----------
total loss 259.413186928912
main criterion 92.76806058613859
weighted_aux_loss 166.64512634277344
loss_r_bn_feature 166.64512634277344
------------iteration 800----------
total loss 231.81807842794575
main criterion 85.5030759865395
weighted_aux_loss 146.31500244140625
loss_r_bn_feature 146.31500244140625
------------iteration 900----------
total loss 203.7491453914688
main criterion 74.72427356529691
weighted_aux_loss 129.02487182617188
loss_r_bn_feature 129.02487182617188
------------iteration 1000----------
total loss 218.60725128764042
main criterion 81.02435028666386
weighted_aux_loss 137.58290100097656
loss_r_bn_feature 137.58290100097656
------------iteration 1100----------
total loss 227.1874007214712
main criterion 96.68560018436182
weighted_aux_loss 130.50180053710938
loss_r_bn_feature 130.50180053710938
------------iteration 1200----------
total loss 200.95405166561395
main criterion 86.35463302547723
weighted_aux_loss 114.59941864013672
loss_r_bn_feature 114.59941864013672
------------iteration 1300----------
total loss 172.077715447968
main criterion 78.04648070675707
weighted_aux_loss 94.03123474121094
loss_r_bn_feature 94.03123474121094
------------iteration 1400----------
total loss 156.01290658810285
main criterion 71.20318368771223
weighted_aux_loss 84.80972290039062
loss_r_bn_feature 84.80972290039062
------------iteration 1500----------
total loss 175.38839413168665
main criterion 82.55880428793665
weighted_aux_loss 92.82958984375
loss_r_bn_feature 92.82958984375
------------iteration 1600----------
total loss 120.04746813441741
main criterion 57.0200938668393
weighted_aux_loss 63.027374267578125
loss_r_bn_feature 63.027374267578125
------------iteration 1700----------
total loss 192.1080257039723
main criterion 86.27530017906994
weighted_aux_loss 105.83272552490234
loss_r_bn_feature 105.83272552490234
------------iteration 1800----------
total loss 119.76446034226427
main criterion 59.203276412820905
weighted_aux_loss 60.56118392944336
loss_r_bn_feature 60.56118392944336
------------iteration 1900----------
total loss 182.27819150585827
main criterion 90.75144284863173
weighted_aux_loss 91.52674865722656
loss_r_bn_feature 91.52674865722656
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 690 end_cls 700
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 818.1025735306714
main criterion 198.90085233926516
weighted_aux_loss 619.2017211914062
loss_r_bn_feature 619.2017211914062
------------iteration 100----------
total loss 412.0317590561883
main criterion 96.4342248765008
weighted_aux_loss 315.5975341796875
loss_r_bn_feature 315.5975341796875
------------iteration 200----------
total loss 339.96669821053115
main criterion 106.31700948982802
weighted_aux_loss 233.64968872070312
loss_r_bn_feature 233.64968872070312
------------iteration 300----------
total loss 326.6143425915278
main criterion 126.96317376828563
weighted_aux_loss 199.6511688232422
loss_r_bn_feature 199.6511688232422
------------iteration 400----------
total loss 251.20012613160876
main criterion 86.33985086305407
weighted_aux_loss 164.8602752685547
loss_r_bn_feature 164.8602752685547
------------iteration 500----------
total loss 254.05909219453326
main criterion 92.30135598847859
weighted_aux_loss 161.7577362060547
loss_r_bn_feature 161.7577362060547
------------iteration 600----------
total loss 239.31459189947833
main criterion 89.6546950488924
weighted_aux_loss 149.65989685058594
loss_r_bn_feature 149.65989685058594
------------iteration 700----------
total loss 232.17583333012476
main criterion 90.40811787602318
weighted_aux_loss 141.76771545410156
loss_r_bn_feature 141.76771545410156
------------iteration 800----------
total loss 199.85331581662666
main criterion 79.94737862180246
weighted_aux_loss 119.90593719482422
loss_r_bn_feature 119.90593719482422
------------iteration 900----------
total loss 232.69554715651455
main criterion 103.60106473463955
weighted_aux_loss 129.094482421875
loss_r_bn_feature 129.094482421875
------------iteration 1000----------
total loss 342.6965541515502
main criterion 145.9788875255736
weighted_aux_loss 196.71766662597656
loss_r_bn_feature 196.71766662597656
------------iteration 1100----------
total loss 166.32899612226586
main criterion 74.17178481855493
weighted_aux_loss 92.15721130371094
loss_r_bn_feature 92.15721130371094
------------iteration 1200----------
total loss 161.60352106495748
main criterion 69.39437647267232
weighted_aux_loss 92.20914459228516
loss_r_bn_feature 92.20914459228516
------------iteration 1300----------
total loss 174.69314374108453
main criterion 84.607931033565
weighted_aux_loss 90.08521270751953
loss_r_bn_feature 90.08521270751953
------------iteration 1400----------
total loss 164.4130467695851
main criterion 76.10645955034683
weighted_aux_loss 88.30658721923828
loss_r_bn_feature 88.30658721923828
------------iteration 1500----------
total loss 135.83607466412934
main criterion 63.24804671002778
weighted_aux_loss 72.58802795410156
loss_r_bn_feature 72.58802795410156
------------iteration 1600----------
total loss 114.68284137223687
main criterion 58.96813113664116
weighted_aux_loss 55.7147102355957
loss_r_bn_feature 55.7147102355957
------------iteration 1700----------
total loss 108.594927802753
main criterion 56.81127835817293
weighted_aux_loss 51.78364944458008
loss_r_bn_feature 51.78364944458008
------------iteration 1800----------
total loss 147.71843555707852
main criterion 75.11920765180507
weighted_aux_loss 72.59922790527344
loss_r_bn_feature 72.59922790527344
------------iteration 1900----------
total loss 234.3914581340056
main criterion 96.90712891037279
weighted_aux_loss 137.4843292236328
loss_r_bn_feature 137.4843292236328
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 700 end_cls 710
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 783.4931716037249
main criterion 194.86029806856868
weighted_aux_loss 588.6328735351562
loss_r_bn_feature 588.6328735351562
------------iteration 100----------
total loss 467.49434253082876
main criterion 126.11067553864126
weighted_aux_loss 341.3836669921875
loss_r_bn_feature 341.3836669921875
------------iteration 200----------
total loss 321.9592462186052
main criterion 89.46837097446456
weighted_aux_loss 232.49087524414062
loss_r_bn_feature 232.49087524414062
------------iteration 300----------
total loss 355.7710779849354
main criterion 127.19183909333383
weighted_aux_loss 228.57923889160156
loss_r_bn_feature 228.57923889160156
------------iteration 400----------
total loss 302.01384166430546
main criterion 115.70382884692262
weighted_aux_loss 186.3100128173828
loss_r_bn_feature 186.3100128173828
------------iteration 500----------
total loss 247.1440907937394
main criterion 87.67817892850502
weighted_aux_loss 159.46591186523438
loss_r_bn_feature 159.46591186523438
------------iteration 600----------
total loss 373.8394700382728
main criterion 155.26007855878063
weighted_aux_loss 218.5793914794922
loss_r_bn_feature 218.5793914794922
------------iteration 700----------
total loss 243.25342846482232
main criterion 95.4015150126739
weighted_aux_loss 147.85191345214844
loss_r_bn_feature 147.85191345214844
------------iteration 800----------
total loss 275.7286198910839
main criterion 112.83794911471674
weighted_aux_loss 162.8906707763672
loss_r_bn_feature 162.8906707763672
------------iteration 900----------
total loss 355.67392626121614
main criterion 153.57881822898955
weighted_aux_loss 202.09510803222656
loss_r_bn_feature 202.09510803222656
------------iteration 1000----------
total loss 195.52684105900352
main criterion 80.45223320988241
weighted_aux_loss 115.0746078491211
loss_r_bn_feature 115.0746078491211
------------iteration 1100----------
total loss 198.30606352236518
main criterion 89.95139585879096
weighted_aux_loss 108.35466766357422
loss_r_bn_feature 108.35466766357422
------------iteration 1200----------
total loss 341.2557042396448
main criterion 150.1483128822229
weighted_aux_loss 191.10739135742188
loss_r_bn_feature 191.10739135742188
------------iteration 1300----------
total loss 184.83664421628896
main criterion 87.73859123777332
weighted_aux_loss 97.09805297851562
loss_r_bn_feature 97.09805297851562
------------iteration 1400----------
total loss 140.77519285606274
main criterion 65.90925657676588
weighted_aux_loss 74.86593627929688
loss_r_bn_feature 74.86593627929688
------------iteration 1500----------
total loss 193.26563847342013
main criterion 95.77921879568575
weighted_aux_loss 97.48641967773438
loss_r_bn_feature 97.48641967773438
------------iteration 1600----------
total loss 125.96768670049062
main criterion 62.16959099736562
weighted_aux_loss 63.798095703125
loss_r_bn_feature 63.798095703125
------------iteration 1700----------
total loss 119.89174375449207
main criterion 60.86835203085926
weighted_aux_loss 59.02339172363281
loss_r_bn_feature 59.02339172363281
------------iteration 1800----------
total loss 108.3112918152527
main criterion 57.273678900213646
weighted_aux_loss 51.03761291503906
loss_r_bn_feature 51.03761291503906
------------iteration 1900----------
total loss 104.8291620455383
main criterion 53.25514165979611
weighted_aux_loss 51.57402038574219
loss_r_bn_feature 51.57402038574219
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 710 end_cls 720
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 820.3118168232118
main criterion 199.04332317086804
weighted_aux_loss 621.2684936523438
loss_r_bn_feature 621.2684936523438
------------iteration 100----------
total loss 533.349519559875
main criterion 136.07687551690626
weighted_aux_loss 397.27264404296875
loss_r_bn_feature 397.27264404296875
------------iteration 200----------
total loss 356.8032542207071
main criterion 95.91681012891026
weighted_aux_loss 260.8864440917969
loss_r_bn_feature 260.8864440917969
------------iteration 300----------
total loss 335.50919344649867
main criterion 109.25644686446743
weighted_aux_loss 226.25274658203125
loss_r_bn_feature 226.25274658203125
------------iteration 400----------
total loss 262.90335494859994
main criterion 86.2038767991859
weighted_aux_loss 176.69947814941406
loss_r_bn_feature 176.69947814941406
------------iteration 500----------
total loss 278.40677932550585
main criterion 92.39820388605273
weighted_aux_loss 186.00857543945312
loss_r_bn_feature 186.00857543945312
------------iteration 600----------
total loss 273.645356870628
main criterion 101.53428814504208
weighted_aux_loss 172.11106872558594
loss_r_bn_feature 172.11106872558594
------------iteration 700----------
total loss 244.86956989958523
main criterion 89.89341938688992
weighted_aux_loss 154.9761505126953
loss_r_bn_feature 154.9761505126953
------------iteration 800----------
total loss 299.9041982626562
main criterion 122.38875636812497
weighted_aux_loss 177.51544189453125
loss_r_bn_feature 177.51544189453125
------------iteration 900----------
total loss 200.4996418598239
main criterion 77.95077558785125
weighted_aux_loss 122.54886627197266
loss_r_bn_feature 122.54886627197266
------------iteration 1000----------
total loss 247.95009537069353
main criterion 101.56574173299823
weighted_aux_loss 146.3843536376953
loss_r_bn_feature 146.3843536376953
------------iteration 1100----------
total loss 186.06908895865672
main criterion 73.21774771109811
weighted_aux_loss 112.8513412475586
loss_r_bn_feature 112.8513412475586
------------iteration 1200----------
total loss 182.40947553031407
main criterion 78.85336896293126
weighted_aux_loss 103.55610656738281
loss_r_bn_feature 103.55610656738281
------------iteration 1300----------
total loss 236.05202729710896
main criterion 110.09423310765582
weighted_aux_loss 125.95779418945312
loss_r_bn_feature 125.95779418945312
------------iteration 1400----------
total loss 189.70869503850457
main criterion 96.76259671086787
weighted_aux_loss 92.94609832763672
loss_r_bn_feature 92.94609832763672
------------iteration 1500----------
total loss 198.75448486203658
main criterion 94.86216613645064
weighted_aux_loss 103.89231872558594
loss_r_bn_feature 103.89231872558594
------------iteration 1600----------
total loss 135.28677319141397
main criterion 63.44367168994912
weighted_aux_loss 71.84310150146484
loss_r_bn_feature 71.84310150146484
------------iteration 1700----------
total loss 205.20050948058463
main criterion 105.19460432921744
weighted_aux_loss 100.00590515136719
loss_r_bn_feature 100.00590515136719
------------iteration 1800----------
total loss 139.4353141521704
main criterion 70.22465893976805
weighted_aux_loss 69.21065521240234
loss_r_bn_feature 69.21065521240234
------------iteration 1900----------
total loss 172.66687636257132
main criterion 87.31779341579396
weighted_aux_loss 85.34908294677734
loss_r_bn_feature 85.34908294677734
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 720 end_cls 730
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 807.3620281218227
main criterion 200.55630302416643
weighted_aux_loss 606.8057250976562
loss_r_bn_feature 606.8057250976562
------------iteration 100----------
total loss 454.04401807528933
main criterion 108.94434766513308
weighted_aux_loss 345.09967041015625
loss_r_bn_feature 345.09967041015625
------------iteration 200----------
total loss 375.0973900464345
main criterion 104.58405386479387
weighted_aux_loss 270.5133361816406
loss_r_bn_feature 270.5133361816406
------------iteration 300----------
total loss 305.74301677403673
main criterion 92.94788127598987
weighted_aux_loss 212.79513549804688
loss_r_bn_feature 212.79513549804688
------------iteration 400----------
total loss 435.5503785897485
main criterion 170.77688005459225
weighted_aux_loss 264.77349853515625
loss_r_bn_feature 264.77349853515625
------------iteration 500----------
total loss 330.9437764040657
main criterion 135.7062580935188
weighted_aux_loss 195.23751831054688
loss_r_bn_feature 195.23751831054688
------------iteration 600----------
total loss 268.9448574520673
main criterion 109.84306607023137
weighted_aux_loss 159.10179138183594
loss_r_bn_feature 159.10179138183594
------------iteration 700----------
total loss 224.37806942211404
main criterion 84.48529293285621
weighted_aux_loss 139.8927764892578
loss_r_bn_feature 139.8927764892578
------------iteration 800----------
total loss 325.77045672534723
main criterion 137.91272967456598
weighted_aux_loss 187.85772705078125
loss_r_bn_feature 187.85772705078125
------------iteration 900----------
total loss 230.85324251851787
main criterion 101.37019503316631
weighted_aux_loss 129.48304748535156
loss_r_bn_feature 129.48304748535156
------------iteration 1000----------
total loss 290.81873279595226
main criterion 133.07694202446788
weighted_aux_loss 157.74179077148438
loss_r_bn_feature 157.74179077148438
------------iteration 1100----------
total loss 222.87946867467235
main criterion 100.94594358922315
weighted_aux_loss 121.93352508544922
loss_r_bn_feature 121.93352508544922
------------iteration 1200----------
total loss 176.50372977256893
main criterion 76.28649039268613
weighted_aux_loss 100.21723937988281
loss_r_bn_feature 100.21723937988281
------------iteration 1300----------
total loss 177.3128100991258
main criterion 85.41735569238753
weighted_aux_loss 91.89545440673828
loss_r_bn_feature 91.89545440673828
------------iteration 1400----------
total loss 191.67254997306642
main criterion 94.25510002189455
weighted_aux_loss 97.41744995117188
loss_r_bn_feature 97.41744995117188
------------iteration 1500----------
total loss 193.37143376367845
main criterion 92.64171269434253
weighted_aux_loss 100.72972106933594
loss_r_bn_feature 100.72972106933594
------------iteration 1600----------
total loss 120.1660790887752
main criterion 59.560392901031065
weighted_aux_loss 60.60568618774414
loss_r_bn_feature 60.60568618774414
------------iteration 1700----------
total loss 121.9942790254611
main criterion 59.3950244173068
weighted_aux_loss 62.5992546081543
loss_r_bn_feature 62.5992546081543
------------iteration 1800----------
total loss 129.85525230934488
main criterion 71.72308449318277
weighted_aux_loss 58.13216781616211
loss_r_bn_feature 58.13216781616211
------------iteration 1900----------
total loss 121.0502900054734
main criterion 60.91861047056129
weighted_aux_loss 60.13167953491211
loss_r_bn_feature 60.13167953491211
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 730 end_cls 740
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 833.809732022561
main criterion 204.27079159287345
weighted_aux_loss 629.5389404296875
loss_r_bn_feature 629.5389404296875
------------iteration 100----------
total loss 420.7987514495561
main criterion 109.21202049252487
weighted_aux_loss 311.58673095703125
loss_r_bn_feature 311.58673095703125
------------iteration 200----------
total loss 372.5899742259193
main criterion 128.60640794173963
weighted_aux_loss 243.9835662841797
loss_r_bn_feature 243.9835662841797
------------iteration 300----------
total loss 301.62956891978246
main criterion 96.37100873911841
weighted_aux_loss 205.25856018066406
loss_r_bn_feature 205.25856018066406
------------iteration 400----------
total loss 249.49311737762554
main criterion 86.09764008270368
weighted_aux_loss 163.39547729492188
loss_r_bn_feature 163.39547729492188
------------iteration 500----------
total loss 298.49328186842854
main criterion 119.36620667311601
weighted_aux_loss 179.1270751953125
loss_r_bn_feature 179.1270751953125
------------iteration 600----------
total loss 288.7231513444175
main criterion 113.2212287369956
weighted_aux_loss 175.50192260742188
loss_r_bn_feature 175.50192260742188
------------iteration 700----------
total loss 210.2654118427279
main criterion 80.80430649604821
weighted_aux_loss 129.4611053466797
loss_r_bn_feature 129.4611053466797
------------iteration 800----------
total loss 370.1786964748537
main criterion 155.55508502465838
weighted_aux_loss 214.6236114501953
loss_r_bn_feature 214.6236114501953
------------iteration 900----------
total loss 194.96162909457996
main criterion 80.54875678012685
weighted_aux_loss 114.41287231445312
loss_r_bn_feature 114.41287231445312
------------iteration 1000----------
total loss 179.32348571347302
main criterion 73.35847411679332
weighted_aux_loss 105.96501159667969
loss_r_bn_feature 105.96501159667969
------------iteration 1100----------
total loss 181.47963552686628
main criterion 76.71478872510848
weighted_aux_loss 104.76484680175781
loss_r_bn_feature 104.76484680175781
------------iteration 1200----------
total loss 312.45676991559134
main criterion 136.51559254742727
weighted_aux_loss 175.94117736816406
loss_r_bn_feature 175.94117736816406
------------iteration 1300----------
total loss 153.3204167166934
main criterion 69.36679580604886
weighted_aux_loss 83.95362091064453
loss_r_bn_feature 83.95362091064453
------------iteration 1400----------
total loss 142.7111408622444
main criterion 65.8822376640022
weighted_aux_loss 76.82890319824219
loss_r_bn_feature 76.82890319824219
------------iteration 1500----------
total loss 158.30430298668227
main criterion 82.86710816246352
weighted_aux_loss 75.43719482421875
loss_r_bn_feature 75.43719482421875
------------iteration 1600----------
total loss 136.07928049224046
main criterion 64.98547708647872
weighted_aux_loss 71.09380340576172
loss_r_bn_feature 71.09380340576172
------------iteration 1700----------
total loss 151.7818246003747
main criterion 78.35576106277705
weighted_aux_loss 73.42606353759766
loss_r_bn_feature 73.42606353759766
------------iteration 1800----------
total loss 141.41037372212767
main criterion 69.70814136128783
weighted_aux_loss 71.70223236083984
loss_r_bn_feature 71.70223236083984
------------iteration 1900----------
total loss 162.0176171443764
main criterion 79.0585641048256
weighted_aux_loss 82.95905303955078
loss_r_bn_feature 82.95905303955078
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 740 end_cls 750
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 833.5807463970227
main criterion 194.46349786186641
weighted_aux_loss 639.1172485351562
loss_r_bn_feature 639.1172485351562
------------iteration 100----------
total loss 486.7056845519145
main criterion 102.01440037222702
weighted_aux_loss 384.6912841796875
loss_r_bn_feature 384.6912841796875
------------iteration 200----------
total loss 420.36335281891195
main criterion 152.14670853180257
weighted_aux_loss 268.2166442871094
loss_r_bn_feature 268.2166442871094
------------iteration 300----------
total loss 312.7661875570028
main criterion 99.0700205648153
weighted_aux_loss 213.6961669921875
loss_r_bn_feature 213.6961669921875
------------iteration 400----------
total loss 258.32219312607083
main criterion 86.7717475694302
weighted_aux_loss 171.55044555664062
loss_r_bn_feature 171.55044555664062
------------iteration 500----------
total loss 344.89196986013854
main criterion 119.06292933279481
weighted_aux_loss 225.82904052734375
loss_r_bn_feature 225.82904052734375
------------iteration 600----------
total loss 238.73899588879
main criterion 86.01659903820405
weighted_aux_loss 152.72239685058594
loss_r_bn_feature 152.72239685058594
------------iteration 700----------
total loss 303.91279233967765
main criterion 129.68711484944328
weighted_aux_loss 174.22567749023438
loss_r_bn_feature 174.22567749023438
------------iteration 800----------
total loss 257.8093569625554
main criterion 101.72505215298509
weighted_aux_loss 156.0843048095703
loss_r_bn_feature 156.0843048095703
------------iteration 900----------
total loss 224.92612800780546
main criterion 90.14713630858672
weighted_aux_loss 134.77899169921875
loss_r_bn_feature 134.77899169921875
------------iteration 1000----------
total loss 203.4981141429243
main criterion 81.99991468003365
weighted_aux_loss 121.49819946289062
loss_r_bn_feature 121.49819946289062
------------iteration 1100----------
total loss 281.49304601946596
main criterion 129.9200327138019
weighted_aux_loss 151.57301330566406
loss_r_bn_feature 151.57301330566406
------------iteration 1200----------
total loss 207.77349236319907
main criterion 97.50451805899984
weighted_aux_loss 110.26897430419922
loss_r_bn_feature 110.26897430419922
------------iteration 1300----------
total loss 271.88773878409444
main criterion 114.3393684227663
weighted_aux_loss 157.54837036132812
loss_r_bn_feature 157.54837036132812
------------iteration 1400----------
total loss 153.36069933178396
main criterion 72.50535265209648
weighted_aux_loss 80.8553466796875
loss_r_bn_feature 80.8553466796875
------------iteration 1500----------
total loss 152.95319798367382
main criterion 67.24622776883007
weighted_aux_loss 85.70697021484375
loss_r_bn_feature 85.70697021484375
------------iteration 1600----------
total loss 275.29388429117097
main criterion 115.7766113419522
weighted_aux_loss 159.51727294921875
loss_r_bn_feature 159.51727294921875
------------iteration 1700----------
total loss 141.88662872031685
main criterion 73.73973235801216
weighted_aux_loss 68.14689636230469
loss_r_bn_feature 68.14689636230469
------------iteration 1800----------
total loss 113.92237239613866
main criterion 55.666947896626944
weighted_aux_loss 58.25542449951172
loss_r_bn_feature 58.25542449951172
------------iteration 1900----------
total loss 112.39665980349255
main criterion 56.52779383669567
weighted_aux_loss 55.868865966796875
loss_r_bn_feature 55.868865966796875
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 750 end_cls 760
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 827.023727015858
main criterion 204.16978414476418
weighted_aux_loss 622.8539428710938
loss_r_bn_feature 622.8539428710938
------------iteration 100----------
total loss 458.2725973718154
main criterion 102.31309419798725
weighted_aux_loss 355.9595031738281
loss_r_bn_feature 355.9595031738281
------------iteration 200----------
total loss 368.50038905283964
main criterion 117.55248255869898
weighted_aux_loss 250.94790649414062
loss_r_bn_feature 250.94790649414062
------------iteration 300----------
total loss 287.076657196086
main criterion 86.89918222050007
weighted_aux_loss 200.17747497558594
loss_r_bn_feature 200.17747497558594
------------iteration 400----------
total loss 281.3222325381636
main criterion 90.48634081453078
weighted_aux_loss 190.8358917236328
loss_r_bn_feature 190.8358917236328
------------iteration 500----------
total loss 248.37558055741277
main criterion 85.02229381424873
weighted_aux_loss 163.35328674316406
loss_r_bn_feature 163.35328674316406
------------iteration 600----------
total loss 247.18832249437475
main criterion 89.19213719164038
weighted_aux_loss 157.99618530273438
loss_r_bn_feature 157.99618530273438
------------iteration 700----------
total loss 228.74625817505878
main criterion 83.10990563599628
weighted_aux_loss 145.6363525390625
loss_r_bn_feature 145.6363525390625
------------iteration 800----------
total loss 235.89816030607517
main criterion 93.47466787443453
weighted_aux_loss 142.42349243164062
loss_r_bn_feature 142.42349243164062
------------iteration 900----------
total loss 242.68261626839848
main criterion 108.9440756189844
weighted_aux_loss 133.73854064941406
loss_r_bn_feature 133.73854064941406
------------iteration 1000----------
total loss 185.65815686657513
main criterion 76.47147321178997
weighted_aux_loss 109.18668365478516
loss_r_bn_feature 109.18668365478516
------------iteration 1100----------
total loss 163.77979238106423
main criterion 70.00616414620093
weighted_aux_loss 93.77362823486328
loss_r_bn_feature 93.77362823486328
------------iteration 1200----------
total loss 470.7250675225847
main criterion 179.1612552667253
weighted_aux_loss 291.5638122558594
loss_r_bn_feature 291.5638122558594
------------iteration 1300----------
total loss 150.74505872690878
main criterion 71.41008253061973
weighted_aux_loss 79.33497619628906
loss_r_bn_feature 79.33497619628906
------------iteration 1400----------
total loss 176.07824025492698
main criterion 87.67950520854026
weighted_aux_loss 88.39873504638672
loss_r_bn_feature 88.39873504638672
------------iteration 1500----------
total loss 144.48511335091763
main criterion 68.26962873177702
weighted_aux_loss 76.21548461914062
loss_r_bn_feature 76.21548461914062
------------iteration 1600----------
total loss 115.32963712688868
main criterion 56.50724180218166
weighted_aux_loss 58.82239532470703
loss_r_bn_feature 58.82239532470703
------------iteration 1700----------
total loss 110.0819843910224
main criterion 55.25649153213568
weighted_aux_loss 54.82549285888672
loss_r_bn_feature 54.82549285888672
------------iteration 1800----------
total loss 139.15853580455655
main criterion 70.1608627698886
weighted_aux_loss 68.99767303466797
loss_r_bn_feature 68.99767303466797
------------iteration 1900----------
total loss 102.11134721333633
main criterion 53.08424760396132
weighted_aux_loss 49.027099609375
loss_r_bn_feature 49.027099609375
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 760 end_cls 770
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 835.9605226136968
main criterion 199.12696548479053
weighted_aux_loss 636.8335571289062
loss_r_bn_feature 636.8335571289062
------------iteration 100----------
total loss 558.8121590771095
main criterion 117.35354091304704
weighted_aux_loss 441.4586181640625
loss_r_bn_feature 441.4586181640625
------------iteration 200----------
total loss 348.55064555881665
main criterion 89.45186015842604
weighted_aux_loss 259.0987854003906
loss_r_bn_feature 259.0987854003906
------------iteration 300----------
total loss 299.23964600038494
main criterion 93.50044922304122
weighted_aux_loss 205.73919677734375
loss_r_bn_feature 205.73919677734375
------------iteration 400----------
total loss 308.4770778223443
main criterion 98.7969020410943
weighted_aux_loss 209.68017578125
loss_r_bn_feature 209.68017578125
------------iteration 500----------
total loss 250.06479839748462
main criterion 90.07048992580494
weighted_aux_loss 159.9943084716797
loss_r_bn_feature 159.9943084716797
------------iteration 600----------
total loss 248.97615847738206
main criterion 87.74878726156174
weighted_aux_loss 161.2273712158203
loss_r_bn_feature 161.2273712158203
------------iteration 700----------
total loss 352.5673903528574
main criterion 139.91962423957614
weighted_aux_loss 212.64776611328125
loss_r_bn_feature 212.64776611328125
------------iteration 800----------
total loss 246.43600563811498
main criterion 101.1351786117478
weighted_aux_loss 145.3008270263672
loss_r_bn_feature 145.3008270263672
------------iteration 900----------
total loss 213.5835453265046
main criterion 89.24125254086007
weighted_aux_loss 124.34229278564453
loss_r_bn_feature 124.34229278564453
------------iteration 1000----------
total loss 223.36322773644736
main criterion 101.22021473595906
weighted_aux_loss 122.14301300048828
loss_r_bn_feature 122.14301300048828
------------iteration 1100----------
total loss 311.22892293967254
main criterion 129.01621542014126
weighted_aux_loss 182.21270751953125
loss_r_bn_feature 182.21270751953125
------------iteration 1200----------
total loss 231.34951223892494
main criterion 100.62050833267494
weighted_aux_loss 130.72900390625
loss_r_bn_feature 130.72900390625
------------iteration 1300----------
total loss 169.3458523756683
main criterion 80.33668184344175
weighted_aux_loss 89.00917053222656
loss_r_bn_feature 89.00917053222656
------------iteration 1400----------
total loss 142.08873367287185
main criterion 67.21455764748124
weighted_aux_loss 74.87417602539062
loss_r_bn_feature 74.87417602539062
------------iteration 1500----------
total loss 125.37487672553468
main criterion 59.33238862739014
weighted_aux_loss 66.04248809814453
loss_r_bn_feature 66.04248809814453
------------iteration 1600----------
total loss 283.4030069461061
main criterion 120.44222203399673
weighted_aux_loss 162.96078491210938
loss_r_bn_feature 162.96078491210938
------------iteration 1700----------
total loss 108.69369029680155
main criterion 53.897044178637486
weighted_aux_loss 54.79664611816406
loss_r_bn_feature 54.79664611816406
------------iteration 1800----------
total loss 156.01845530971778
main criterion 70.65395335659278
weighted_aux_loss 85.364501953125
loss_r_bn_feature 85.364501953125
------------iteration 1900----------
total loss 176.13259849656566
main criterion 80.66501579392893
weighted_aux_loss 95.46758270263672
loss_r_bn_feature 95.46758270263672
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 770 end_cls 780
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 842.9466905533617
main criterion 204.73886584633044
weighted_aux_loss 638.2078247070312
loss_r_bn_feature 638.2078247070312
------------iteration 100----------
total loss 468.36611525746173
main criterion 116.58498732777423
weighted_aux_loss 351.7811279296875
loss_r_bn_feature 351.7811279296875
------------iteration 200----------
total loss 354.512315242025
main criterion 109.63295122835312
weighted_aux_loss 244.87936401367188
loss_r_bn_feature 244.87936401367188
------------iteration 300----------
total loss 296.34095242903817
main criterion 102.27260831282724
weighted_aux_loss 194.06834411621094
loss_r_bn_feature 194.06834411621094
------------iteration 400----------
total loss 294.1205552803849
main criterion 99.43246018761145
weighted_aux_loss 194.68809509277344
loss_r_bn_feature 194.68809509277344
------------iteration 500----------
total loss 255.36001492425805
main criterion 91.28756619378929
weighted_aux_loss 164.07244873046875
loss_r_bn_feature 164.07244873046875
------------iteration 600----------
total loss 228.81336115590926
main criterion 84.57826899282333
weighted_aux_loss 144.23509216308594
loss_r_bn_feature 144.23509216308594
------------iteration 700----------
total loss 254.67323431549042
main criterion 94.70791754302948
weighted_aux_loss 159.96531677246094
loss_r_bn_feature 159.96531677246094
------------iteration 800----------
total loss 207.46267002655208
main criterion 82.69926518036065
weighted_aux_loss 124.7634048461914
loss_r_bn_feature 124.7634048461914
------------iteration 900----------
total loss 246.66327770844066
main criterion 109.36432751312815
weighted_aux_loss 137.2989501953125
loss_r_bn_feature 137.2989501953125
------------iteration 1000----------
total loss 200.79614090164807
main criterion 79.62969040115979
weighted_aux_loss 121.16645050048828
loss_r_bn_feature 121.16645050048828
------------iteration 1100----------
total loss 239.48471483814586
main criterion 110.69989428150525
weighted_aux_loss 128.78482055664062
loss_r_bn_feature 128.78482055664062
------------iteration 1200----------
total loss 229.95133030142563
main criterion 104.39530002798811
weighted_aux_loss 125.5560302734375
loss_r_bn_feature 125.5560302734375
------------iteration 1300----------
total loss 294.63382083487517
main criterion 135.37840396475798
weighted_aux_loss 159.2554168701172
loss_r_bn_feature 159.2554168701172
------------iteration 1400----------
total loss 139.00141009416524
main criterion 65.183477965259
weighted_aux_loss 73.81793212890625
loss_r_bn_feature 73.81793212890625
------------iteration 1500----------
total loss 206.54714378710491
main criterion 105.98040947314009
weighted_aux_loss 100.56673431396484
loss_r_bn_feature 100.56673431396484
------------iteration 1600----------
total loss 182.50005819801095
main criterion 89.93795492652656
weighted_aux_loss 92.56210327148438
loss_r_bn_feature 92.56210327148438
------------iteration 1700----------
total loss 114.04422280990545
main criterion 59.23089883529608
weighted_aux_loss 54.813323974609375
loss_r_bn_feature 54.813323974609375
------------iteration 1800----------
total loss 213.78841440463583
main criterion 102.47555582309288
weighted_aux_loss 111.31285858154297
loss_r_bn_feature 111.31285858154297
------------iteration 1900----------
total loss 292.60748792817236
main criterion 127.95305372407078
weighted_aux_loss 164.65443420410156
loss_r_bn_feature 164.65443420410156
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 780 end_cls 790
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 827.1811650974213
main criterion 197.68482720679629
weighted_aux_loss 629.496337890625
loss_r_bn_feature 629.496337890625
------------iteration 100----------
total loss 464.6666516621109
main criterion 103.5910901386734
weighted_aux_loss 361.0755615234375
loss_r_bn_feature 361.0755615234375
------------iteration 200----------
total loss 414.33929010976897
main criterion 147.0069231664096
weighted_aux_loss 267.3323669433594
loss_r_bn_feature 267.3323669433594
------------iteration 300----------
total loss 312.42899953124015
main criterion 99.13723622557609
weighted_aux_loss 213.29176330566406
loss_r_bn_feature 213.29176330566406
------------iteration 400----------
total loss 276.56173274637723
main criterion 90.31481502176784
weighted_aux_loss 186.24691772460938
loss_r_bn_feature 186.24691772460938
------------iteration 500----------
total loss 255.99203542482485
main criterion 89.82586721193421
weighted_aux_loss 166.16616821289062
loss_r_bn_feature 166.16616821289062
------------iteration 600----------
total loss 332.9314283165948
main criterion 126.29026925897762
weighted_aux_loss 206.6411590576172
loss_r_bn_feature 206.6411590576172
------------iteration 700----------
total loss 248.02639544574953
main criterion 95.88461077778078
weighted_aux_loss 152.14178466796875
loss_r_bn_feature 152.14178466796875
------------iteration 800----------
total loss 254.7585874237405
main criterion 108.17214638370142
weighted_aux_loss 146.58644104003906
loss_r_bn_feature 146.58644104003906
------------iteration 900----------
total loss 209.98659218442378
main criterion 77.75696266782222
weighted_aux_loss 132.22962951660156
loss_r_bn_feature 132.22962951660156
------------iteration 1000----------
total loss 242.05393444363978
main criterion 90.36347423856164
weighted_aux_loss 151.69046020507812
loss_r_bn_feature 151.69046020507812
------------iteration 1100----------
total loss 181.15031208881763
main criterion 71.14568104633716
weighted_aux_loss 110.00463104248047
loss_r_bn_feature 110.00463104248047
------------iteration 1200----------
total loss 170.12732487778422
main criterion 74.27422886948344
weighted_aux_loss 95.85309600830078
loss_r_bn_feature 95.85309600830078
------------iteration 1300----------
total loss 148.11585896505983
main criterion 65.10725300802856
weighted_aux_loss 83.00860595703125
loss_r_bn_feature 83.00860595703125
------------iteration 1400----------
total loss 141.72219630082805
main criterion 61.96747370561319
weighted_aux_loss 79.75472259521484
loss_r_bn_feature 79.75472259521484
------------iteration 1500----------
total loss 129.92955320895106
main criterion 60.50781553805261
weighted_aux_loss 69.42173767089844
loss_r_bn_feature 69.42173767089844
------------iteration 1600----------
total loss 315.646402575996
main criterion 120.56089232208971
weighted_aux_loss 195.08551025390625
loss_r_bn_feature 195.08551025390625
------------iteration 1700----------
total loss 147.61227400011552
main criterion 76.56481916613116
weighted_aux_loss 71.04745483398438
loss_r_bn_feature 71.04745483398438
------------iteration 1800----------
total loss 473.5633662664843
main criterion 158.1460383856249
weighted_aux_loss 315.4173278808594
loss_r_bn_feature 315.4173278808594
------------iteration 1900----------
total loss 118.3082456364931
main criterion 58.48688790944232
weighted_aux_loss 59.82135772705078
loss_r_bn_feature 59.82135772705078
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 790 end_cls 800
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 791.4061046641528
main criterion 198.05124626571532
weighted_aux_loss 593.3548583984375
loss_r_bn_feature 593.3548583984375
------------iteration 100----------
total loss 549.9007933080745
main criterion 173.94235824948072
weighted_aux_loss 375.95843505859375
loss_r_bn_feature 375.95843505859375
------------iteration 200----------
total loss 381.69568243205
main criterion 115.00677862345623
weighted_aux_loss 266.68890380859375
loss_r_bn_feature 266.68890380859375
------------iteration 300----------
total loss 430.54663945702816
main criterion 154.11237432030944
weighted_aux_loss 276.43426513671875
loss_r_bn_feature 276.43426513671875
------------iteration 400----------
total loss 441.6448612140008
main criterion 169.3610477374383
weighted_aux_loss 272.2838134765625
loss_r_bn_feature 272.2838134765625
------------iteration 500----------
total loss 263.3088249269989
main criterion 91.25808945336611
weighted_aux_loss 172.0507354736328
loss_r_bn_feature 172.0507354736328
------------iteration 600----------
total loss 320.9505976814065
main criterion 125.87051955640652
weighted_aux_loss 195.080078125
loss_r_bn_feature 195.080078125
------------iteration 700----------
total loss 242.55821052472885
main criterion 90.77982917707259
weighted_aux_loss 151.77838134765625
loss_r_bn_feature 151.77838134765625
------------iteration 800----------
total loss 219.70841041411543
main criterion 86.39012733306075
weighted_aux_loss 133.3182830810547
loss_r_bn_feature 133.3182830810547
------------iteration 900----------
total loss 192.94165686310313
main criterion 76.53021124542735
weighted_aux_loss 116.41144561767578
loss_r_bn_feature 116.41144561767578
------------iteration 1000----------
total loss 237.92275015634706
main criterion 78.31520621103456
weighted_aux_loss 159.6075439453125
loss_r_bn_feature 159.6075439453125
------------iteration 1100----------
total loss 164.71733075601782
main criterion 68.08411389810765
weighted_aux_loss 96.63321685791016
loss_r_bn_feature 96.63321685791016
------------iteration 1200----------
total loss 183.81927341556639
main criterion 77.42623752689452
weighted_aux_loss 106.39303588867188
loss_r_bn_feature 106.39303588867188
------------iteration 1300----------
total loss 152.59850641622324
main criterion 68.7421450270631
weighted_aux_loss 83.85636138916016
loss_r_bn_feature 83.85636138916016
------------iteration 1400----------
total loss 131.98998177342042
main criterion 63.36999428562744
weighted_aux_loss 68.61998748779297
loss_r_bn_feature 68.61998748779297
------------iteration 1500----------
total loss 131.3351863492615
main criterion 64.1189769376404
weighted_aux_loss 67.2162094116211
loss_r_bn_feature 67.2162094116211
------------iteration 1600----------
total loss 121.25467099470455
main criterion 60.53689374250729
weighted_aux_loss 60.717777252197266
loss_r_bn_feature 60.717777252197266
------------iteration 1700----------
total loss 103.85416005329535
main criterion 52.5752675362055
weighted_aux_loss 51.278892517089844
loss_r_bn_feature 51.278892517089844
------------iteration 1800----------
total loss 111.09778669409465
main criterion 58.01971128515911
weighted_aux_loss 53.07807540893555
loss_r_bn_feature 53.07807540893555
------------iteration 1900----------
total loss 100.30452508918248
main criterion 50.832112979807484
weighted_aux_loss 49.472412109375
loss_r_bn_feature 49.472412109375
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 800 end_cls 810
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 812.3961053294124
main criterion 203.01750425519364
weighted_aux_loss 609.3786010742188
loss_r_bn_feature 609.3786010742188
------------iteration 100----------
total loss 460.4669458377099
main criterion 117.04494876739736
weighted_aux_loss 343.4219970703125
loss_r_bn_feature 343.4219970703125
------------iteration 200----------
total loss 337.206435078616
main criterion 102.05509840869408
weighted_aux_loss 235.15133666992188
loss_r_bn_feature 235.15133666992188
------------iteration 300----------
total loss 297.2143763250577
main criterion 93.1970423406827
weighted_aux_loss 204.017333984375
loss_r_bn_feature 204.017333984375
------------iteration 400----------
total loss 356.75467799600574
main criterion 140.20459865030264
weighted_aux_loss 216.55007934570312
loss_r_bn_feature 216.55007934570312
------------iteration 500----------
total loss 505.69315431523785
main criterion 182.01212404180038
weighted_aux_loss 323.6810302734375
loss_r_bn_feature 323.6810302734375
------------iteration 600----------
total loss 374.06445277404214
main criterion 160.62272608947185
weighted_aux_loss 213.4417266845703
loss_r_bn_feature 213.4417266845703
------------iteration 700----------
total loss 239.53158417891714
main criterion 86.19181672286246
weighted_aux_loss 153.3397674560547
loss_r_bn_feature 153.3397674560547
------------iteration 800----------
total loss 351.41378242999826
main criterion 153.1408484700373
weighted_aux_loss 198.27293395996094
loss_r_bn_feature 198.27293395996094
------------iteration 900----------
total loss 209.20005637356465
main criterion 83.06247550198262
weighted_aux_loss 126.13758087158203
loss_r_bn_feature 126.13758087158203
------------iteration 1000----------
total loss 213.82222803074043
main criterion 80.69207056003731
weighted_aux_loss 133.13015747070312
loss_r_bn_feature 133.13015747070312
------------iteration 1100----------
total loss 535.9913839865876
main criterion 187.6188864279938
weighted_aux_loss 348.37249755859375
loss_r_bn_feature 348.37249755859375
------------iteration 1200----------
total loss 170.35714544150898
main criterion 80.26555456016133
weighted_aux_loss 90.09159088134766
loss_r_bn_feature 90.09159088134766
------------iteration 1300----------
total loss 138.1048457916677
main criterion 63.893671780437245
weighted_aux_loss 74.21117401123047
loss_r_bn_feature 74.21117401123047
------------iteration 1400----------
total loss 141.44023842109482
main criterion 64.64896339668076
weighted_aux_loss 76.79127502441406
loss_r_bn_feature 76.79127502441406
------------iteration 1500----------
total loss 163.14944735556605
main criterion 81.48873415976526
weighted_aux_loss 81.66071319580078
loss_r_bn_feature 81.66071319580078
------------iteration 1600----------
total loss 146.6301982907841
main criterion 74.15970115943645
weighted_aux_loss 72.47049713134766
loss_r_bn_feature 72.47049713134766
------------iteration 1700----------
total loss 102.82200509391367
main criterion 53.01959115348398
weighted_aux_loss 49.80241394042969
loss_r_bn_feature 49.80241394042969
------------iteration 1800----------
total loss 228.06363643186398
main criterion 106.88231624143431
weighted_aux_loss 121.18132019042969
loss_r_bn_feature 121.18132019042969
------------iteration 1900----------
total loss 120.47309195550801
main criterion 62.82850348504902
weighted_aux_loss 57.644588470458984
loss_r_bn_feature 57.644588470458984
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 810 end_cls 820
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 830.2572046859725
main criterion 201.5067774398787
weighted_aux_loss 628.7504272460938
loss_r_bn_feature 628.7504272460938
------------iteration 100----------
total loss 474.78273671929145
main criterion 111.48866933647896
weighted_aux_loss 363.2940673828125
loss_r_bn_feature 363.2940673828125
------------iteration 200----------
total loss 447.12247709440607
main criterion 140.9008279244842
weighted_aux_loss 306.2216491699219
loss_r_bn_feature 306.2216491699219
------------iteration 300----------
total loss 362.1897058232136
main criterion 126.68659303024485
weighted_aux_loss 235.50311279296875
loss_r_bn_feature 235.50311279296875
------------iteration 400----------
total loss 286.6625813279411
main criterion 92.75793350079267
weighted_aux_loss 193.90464782714844
loss_r_bn_feature 193.90464782714844
------------iteration 500----------
total loss 489.4328747316098
main criterion 175.44755368668794
weighted_aux_loss 313.9853210449219
loss_r_bn_feature 313.9853210449219
------------iteration 600----------
total loss 291.1163927843556
main criterion 106.45777767205088
weighted_aux_loss 184.6586151123047
loss_r_bn_feature 184.6586151123047
------------iteration 700----------
total loss 241.08351028756576
main criterion 92.2609852631517
weighted_aux_loss 148.82252502441406
loss_r_bn_feature 148.82252502441406
------------iteration 800----------
total loss 274.4740113464832
main criterion 97.92164318242071
weighted_aux_loss 176.5523681640625
loss_r_bn_feature 176.5523681640625
------------iteration 900----------
total loss 243.77680336101452
main criterion 101.74900184734264
weighted_aux_loss 142.02780151367188
loss_r_bn_feature 142.02780151367188
------------iteration 1000----------
total loss 201.72326507253234
main criterion 79.32023467702453
weighted_aux_loss 122.40303039550781
loss_r_bn_feature 122.40303039550781
------------iteration 1100----------
total loss 194.4335092957876
main criterion 82.3027719911001
weighted_aux_loss 112.1307373046875
loss_r_bn_feature 112.1307373046875
------------iteration 1200----------
total loss 181.98442547146988
main criterion 84.72530071561049
weighted_aux_loss 97.25912475585938
loss_r_bn_feature 97.25912475585938
------------iteration 1300----------
total loss 168.30585575977096
main criterion 76.22760106006393
weighted_aux_loss 92.07825469970703
loss_r_bn_feature 92.07825469970703
------------iteration 1400----------
total loss 196.21096723521296
main criterion 94.1439353748614
weighted_aux_loss 102.06703186035156
loss_r_bn_feature 102.06703186035156
------------iteration 1500----------
total loss 128.57839436351213
main criterion 60.94155354319962
weighted_aux_loss 67.6368408203125
loss_r_bn_feature 67.6368408203125
------------iteration 1600----------
total loss 192.07829491286054
main criterion 90.39541832594648
weighted_aux_loss 101.68287658691406
loss_r_bn_feature 101.68287658691406
------------iteration 1700----------
total loss 121.9740483906289
main criterion 62.8982198383828
weighted_aux_loss 59.075828552246094
loss_r_bn_feature 59.075828552246094
------------iteration 1800----------
total loss 106.42956343785447
main criterion 53.40858260289353
weighted_aux_loss 53.02098083496094
loss_r_bn_feature 53.02098083496094
------------iteration 1900----------
total loss 114.45010445953477
main criterion 55.21517251373398
weighted_aux_loss 59.23493194580078
loss_r_bn_feature 59.23493194580078
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 820 end_cls 830
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 832.1837225342766
main criterion 202.32379821787035
weighted_aux_loss 629.8599243164062
loss_r_bn_feature 629.8599243164062
------------iteration 100----------
total loss 475.4349655901598
main criterion 123.98962257258168
weighted_aux_loss 351.4453430175781
loss_r_bn_feature 351.4453430175781
------------iteration 200----------
total loss 334.3341766092327
main criterion 88.12645871372492
weighted_aux_loss 246.2077178955078
loss_r_bn_feature 246.2077178955078
------------iteration 300----------
total loss 290.3068130547029
main criterion 99.5101059013826
weighted_aux_loss 190.7967071533203
loss_r_bn_feature 190.7967071533203
------------iteration 400----------
total loss 285.98011248179614
main criterion 93.47045366831959
weighted_aux_loss 192.50965881347656
loss_r_bn_feature 192.50965881347656
------------iteration 500----------
total loss 314.4641406955644
main criterion 121.94686774634567
weighted_aux_loss 192.51727294921875
loss_r_bn_feature 192.51727294921875
------------iteration 600----------
total loss 236.32792751737816
main criterion 81.11151211210473
weighted_aux_loss 155.21641540527344
loss_r_bn_feature 155.21641540527344
------------iteration 700----------
total loss 337.5782882430077
main criterion 128.71029202718742
weighted_aux_loss 208.8679962158203
loss_r_bn_feature 208.8679962158203
------------iteration 800----------
total loss 223.6418736788264
main criterion 93.85995229210768
weighted_aux_loss 129.78192138671875
loss_r_bn_feature 129.78192138671875
------------iteration 900----------
total loss 178.73854843926767
main criterion 73.05957810235361
weighted_aux_loss 105.67897033691406
loss_r_bn_feature 105.67897033691406
------------iteration 1000----------
total loss 189.8717022477233
main criterion 74.43710141764518
weighted_aux_loss 115.43460083007812
loss_r_bn_feature 115.43460083007812
------------iteration 1100----------
total loss 185.98747987056868
main criterion 75.27499360347882
weighted_aux_loss 110.71248626708984
loss_r_bn_feature 110.71248626708984
------------iteration 1200----------
total loss 165.08803231805973
main criterion 73.029873443548
weighted_aux_loss 92.05815887451172
loss_r_bn_feature 92.05815887451172
------------iteration 1300----------
total loss 150.22453272730877
main criterion 67.44209254176191
weighted_aux_loss 82.78244018554688
loss_r_bn_feature 82.78244018554688
------------iteration 1400----------
total loss 312.4107230146627
main criterion 126.05558995802211
weighted_aux_loss 186.35513305664062
loss_r_bn_feature 186.35513305664062
------------iteration 1500----------
total loss 161.85549135562263
main criterion 77.09609957095468
weighted_aux_loss 84.75939178466797
loss_r_bn_feature 84.75939178466797
------------iteration 1600----------
total loss 113.51013663844034
main criterion 55.61113074854776
weighted_aux_loss 57.89900588989258
loss_r_bn_feature 57.89900588989258
------------iteration 1700----------
total loss 341.69207619720424
main criterion 125.1671433358761
weighted_aux_loss 216.52493286132812
loss_r_bn_feature 216.52493286132812
------------iteration 1800----------
total loss 126.23179002242726
main criterion 69.61997743087453
weighted_aux_loss 56.611812591552734
loss_r_bn_feature 56.611812591552734
------------iteration 1900----------
total loss 147.22318675536474
main criterion 75.04801585692725
weighted_aux_loss 72.1751708984375
loss_r_bn_feature 72.1751708984375
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 830 end_cls 840
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 843.9539766826523
main criterion 195.7678194560898
weighted_aux_loss 648.1861572265625
loss_r_bn_feature 648.1861572265625
------------iteration 100----------
total loss 477.3012917225661
main criterion 99.27864767959733
weighted_aux_loss 378.02264404296875
loss_r_bn_feature 378.02264404296875
------------iteration 200----------
total loss 378.6585362003579
main criterion 95.14721417887353
weighted_aux_loss 283.5113220214844
loss_r_bn_feature 283.5113220214844
------------iteration 300----------
total loss 307.72893882062425
main criterion 97.99041342999924
weighted_aux_loss 209.738525390625
loss_r_bn_feature 209.738525390625
------------iteration 400----------
total loss 285.4176611084665
main criterion 92.9258855957712
weighted_aux_loss 192.4917755126953
loss_r_bn_feature 192.4917755126953
------------iteration 500----------
total loss 275.4480364072317
main criterion 85.83428213477076
weighted_aux_loss 189.61375427246094
loss_r_bn_feature 189.61375427246094
------------iteration 600----------
total loss 256.4737627293597
main criterion 99.95264456529722
weighted_aux_loss 156.5211181640625
loss_r_bn_feature 156.5211181640625
------------iteration 700----------
total loss 220.365154122182
main criterion 83.7098349083148
weighted_aux_loss 136.6553192138672
loss_r_bn_feature 136.6553192138672
------------iteration 800----------
total loss 200.68533469932686
main criterion 74.71454002159248
weighted_aux_loss 125.97079467773438
loss_r_bn_feature 125.97079467773438
------------iteration 900----------
total loss 206.78739850745148
main criterion 80.1895286344046
weighted_aux_loss 126.59786987304688
loss_r_bn_feature 126.59786987304688
------------iteration 1000----------
total loss 197.39880387815168
main criterion 80.82688157590559
weighted_aux_loss 116.5719223022461
loss_r_bn_feature 116.5719223022461
------------iteration 1100----------
total loss 199.43719745827826
main criterion 87.09948993874703
weighted_aux_loss 112.33770751953125
loss_r_bn_feature 112.33770751953125
------------iteration 1200----------
total loss 158.04011061404339
main criterion 66.35428908084025
weighted_aux_loss 91.68582153320312
loss_r_bn_feature 91.68582153320312
------------iteration 1300----------
total loss 175.370077406865
main criterion 71.39830616663063
weighted_aux_loss 103.97177124023438
loss_r_bn_feature 103.97177124023438
------------iteration 1400----------
total loss 144.2136940651706
main criterion 63.433504551010444
weighted_aux_loss 80.78018951416016
loss_r_bn_feature 80.78018951416016
------------iteration 1500----------
total loss 124.86349543549119
main criterion 58.36141261078415
weighted_aux_loss 66.50208282470703
loss_r_bn_feature 66.50208282470703
------------iteration 1600----------
total loss 348.3692270675573
main criterion 139.27726234587757
weighted_aux_loss 209.0919647216797
loss_r_bn_feature 209.0919647216797
------------iteration 1700----------
total loss 107.8027352713124
main criterion 54.152337016917876
weighted_aux_loss 53.65039825439453
loss_r_bn_feature 53.65039825439453
------------iteration 1800----------
total loss 110.46985689295795
main criterion 53.56552187098529
weighted_aux_loss 56.904335021972656
loss_r_bn_feature 56.904335021972656
------------iteration 1900----------
total loss 289.05421631647835
main criterion 122.67770569636114
weighted_aux_loss 166.3765106201172
loss_r_bn_feature 166.3765106201172
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 840 end_cls 850
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 795.5200064202102
main criterion 195.62706208427272
weighted_aux_loss 599.8929443359375
loss_r_bn_feature 599.8929443359375
------------iteration 100----------
total loss 468.6065687323946
main criterion 107.69629041208208
weighted_aux_loss 360.9102783203125
loss_r_bn_feature 360.9102783203125
------------iteration 200----------
total loss 378.68355209974914
main criterion 95.24431259779602
weighted_aux_loss 283.4392395019531
loss_r_bn_feature 283.4392395019531
------------iteration 300----------
total loss 320.40874658753773
main criterion 112.89183984925648
weighted_aux_loss 207.51690673828125
loss_r_bn_feature 207.51690673828125
------------iteration 400----------
total loss 372.43344207696737
main criterion 152.37872405938924
weighted_aux_loss 220.05471801757812
loss_r_bn_feature 220.05471801757812
------------iteration 500----------
total loss 277.63562300174243
main criterion 94.1209898230315
weighted_aux_loss 183.51463317871094
loss_r_bn_feature 183.51463317871094
------------iteration 600----------
total loss 272.8434262727817
main criterion 107.6211057161411
weighted_aux_loss 165.22232055664062
loss_r_bn_feature 165.22232055664062
------------iteration 700----------
total loss 229.04657668156156
main criterion 82.97109145206937
weighted_aux_loss 146.0754852294922
loss_r_bn_feature 146.0754852294922
------------iteration 800----------
total loss 217.71901530413427
main criterion 83.77597636370459
weighted_aux_loss 133.9430389404297
loss_r_bn_feature 133.9430389404297
------------iteration 900----------
total loss 225.22831091143058
main criterion 94.48267492510244
weighted_aux_loss 130.74563598632812
loss_r_bn_feature 130.74563598632812
------------iteration 1000----------
total loss 250.29943684500427
main criterion 107.81947163504333
weighted_aux_loss 142.47996520996094
loss_r_bn_feature 142.47996520996094
------------iteration 1100----------
total loss 213.23743662439938
main criterion 80.60932383143064
weighted_aux_loss 132.62811279296875
loss_r_bn_feature 132.62811279296875
------------iteration 1200----------
total loss 241.4644235647366
main criterion 110.64812412626002
weighted_aux_loss 130.81629943847656
loss_r_bn_feature 130.81629943847656
------------iteration 1300----------
total loss 158.59749166483937
main criterion 70.22979299540579
weighted_aux_loss 88.3676986694336
loss_r_bn_feature 88.3676986694336
------------iteration 1400----------
total loss 278.1870681882501
main criterion 120.14443513160947
weighted_aux_loss 158.04263305664062
loss_r_bn_feature 158.04263305664062
------------iteration 1500----------
total loss 201.75997392887098
main criterion 98.41616289371473
weighted_aux_loss 103.34381103515625
loss_r_bn_feature 103.34381103515625
------------iteration 1600----------
total loss 170.00814969775666
main criterion 84.58040006396759
weighted_aux_loss 85.42774963378906
loss_r_bn_feature 85.42774963378906
------------iteration 1700----------
total loss 136.5220546165794
main criterion 73.94372362292705
weighted_aux_loss 62.578330993652344
loss_r_bn_feature 62.578330993652344
------------iteration 1800----------
total loss 128.20043083935698
main criterion 64.24386116772612
weighted_aux_loss 63.95656967163086
loss_r_bn_feature 63.95656967163086
------------iteration 1900----------
total loss 111.83058087987638
main criterion 55.6311942831967
weighted_aux_loss 56.19938659667969
loss_r_bn_feature 56.19938659667969
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 850 end_cls 860
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 830.3963782640641
main criterion 200.4263465257828
weighted_aux_loss 629.9700317382812
loss_r_bn_feature 629.9700317382812
------------iteration 100----------
total loss 603.0719562265806
main criterion 179.04674870704937
weighted_aux_loss 424.02520751953125
loss_r_bn_feature 424.02520751953125
------------iteration 200----------
total loss 374.01049358825014
main criterion 102.92461712340638
weighted_aux_loss 271.08587646484375
loss_r_bn_feature 271.08587646484375
------------iteration 300----------
total loss 330.22391443328996
main criterion 109.57804040985246
weighted_aux_loss 220.6458740234375
loss_r_bn_feature 220.6458740234375
------------iteration 400----------
total loss 263.928198675304
main criterion 89.24352155128054
weighted_aux_loss 174.68467712402344
loss_r_bn_feature 174.68467712402344
------------iteration 500----------
total loss 266.64656031393076
main criterion 101.1213680531886
weighted_aux_loss 165.5251922607422
loss_r_bn_feature 165.5251922607422
------------iteration 600----------
total loss 276.7243491178521
main criterion 112.52506933269585
weighted_aux_loss 164.19927978515625
loss_r_bn_feature 164.19927978515625
------------iteration 700----------
total loss 230.33738882601688
main criterion 83.58067495882939
weighted_aux_loss 146.7567138671875
loss_r_bn_feature 146.7567138671875
------------iteration 800----------
total loss 213.83908449803351
main criterion 83.02725588475226
weighted_aux_loss 130.81182861328125
loss_r_bn_feature 130.81182861328125
------------iteration 900----------
total loss 204.1222978328123
main criterion 80.59162003740215
weighted_aux_loss 123.53067779541016
loss_r_bn_feature 123.53067779541016
------------iteration 1000----------
total loss 253.80007057422188
main criterion 108.95320778125314
weighted_aux_loss 144.84686279296875
loss_r_bn_feature 144.84686279296875
------------iteration 1100----------
total loss 204.61671523471352
main criterion 87.51263503451821
weighted_aux_loss 117.10408020019531
loss_r_bn_feature 117.10408020019531
------------iteration 1200----------
total loss 448.58688546375936
main criterion 169.5697956200094
weighted_aux_loss 279.01708984375
loss_r_bn_feature 279.01708984375
------------iteration 1300----------
total loss 151.17560651774522
main criterion 70.65111616129991
weighted_aux_loss 80.52449035644531
loss_r_bn_feature 80.52449035644531
------------iteration 1400----------
total loss 279.24371793464536
main criterion 124.7911727686297
weighted_aux_loss 154.45254516601562
loss_r_bn_feature 154.45254516601562
------------iteration 1500----------
total loss 182.2849802485813
main criterion 83.54170174516334
weighted_aux_loss 98.74327850341797
loss_r_bn_feature 98.74327850341797
------------iteration 1600----------
total loss 138.75045700775163
main criterion 71.24356766448992
weighted_aux_loss 67.50688934326172
loss_r_bn_feature 67.50688934326172
------------iteration 1700----------
total loss 166.7607693971213
main criterion 62.17861607680881
weighted_aux_loss 104.5821533203125
loss_r_bn_feature 104.5821533203125
------------iteration 1800----------
total loss 105.72873466402761
main criterion 54.301286391322535
weighted_aux_loss 51.42744827270508
loss_r_bn_feature 51.42744827270508
------------iteration 1900----------
total loss 226.6777386512037
main criterion 99.10637329475838
weighted_aux_loss 127.57136535644531
loss_r_bn_feature 127.57136535644531
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 860 end_cls 870
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 838.0673105423598
main criterion 200.83964940954735
weighted_aux_loss 637.2276611328125
loss_r_bn_feature 637.2276611328125
------------iteration 100----------
total loss 439.8254076194999
main criterion 105.37814199449991
weighted_aux_loss 334.447265625
loss_r_bn_feature 334.447265625
------------iteration 200----------
total loss 350.12605802068225
main criterion 125.67511502751817
weighted_aux_loss 224.45094299316406
loss_r_bn_feature 224.45094299316406
------------iteration 300----------
total loss 288.5008300728199
main criterion 88.58563842242927
weighted_aux_loss 199.91519165039062
loss_r_bn_feature 199.91519165039062
------------iteration 400----------
total loss 278.9867491064639
main criterion 97.70156233888578
weighted_aux_loss 181.28518676757812
loss_r_bn_feature 181.28518676757812
------------iteration 500----------
total loss 251.9337776297089
main criterion 89.08830338654485
weighted_aux_loss 162.84547424316406
loss_r_bn_feature 162.84547424316406
------------iteration 600----------
total loss 224.08940937916987
main criterion 82.96816304127923
weighted_aux_loss 141.12124633789062
loss_r_bn_feature 141.12124633789062
------------iteration 700----------
total loss 237.60271711556712
main criterion 88.46023054330149
weighted_aux_loss 149.14248657226562
loss_r_bn_feature 149.14248657226562
------------iteration 800----------
total loss 230.66051071330588
main criterion 91.25506942912618
weighted_aux_loss 139.4054412841797
loss_r_bn_feature 139.4054412841797
------------iteration 900----------
total loss 477.03942702400354
main criterion 170.5565779029098
weighted_aux_loss 306.48284912109375
loss_r_bn_feature 306.48284912109375
------------iteration 1000----------
total loss 422.88912643578067
main criterion 158.83388961937445
weighted_aux_loss 264.05523681640625
loss_r_bn_feature 264.05523681640625
------------iteration 1100----------
total loss 216.85987239454448
main criterion 95.65219264600933
weighted_aux_loss 121.20767974853516
loss_r_bn_feature 121.20767974853516
------------iteration 1200----------
total loss 387.41836326259204
main criterion 151.80648582118576
weighted_aux_loss 235.61187744140625
loss_r_bn_feature 235.61187744140625
------------iteration 1300----------
total loss 155.59489967278756
main criterion 68.87487556390083
weighted_aux_loss 86.72002410888672
loss_r_bn_feature 86.72002410888672
------------iteration 1400----------
total loss 250.16520212281554
main criterion 110.77424142945617
weighted_aux_loss 139.39096069335938
loss_r_bn_feature 139.39096069335938
------------iteration 1500----------
total loss 147.51073209509045
main criterion 77.14716092809826
weighted_aux_loss 70.36357116699219
loss_r_bn_feature 70.36357116699219
------------iteration 1600----------
total loss 154.83078837395237
main criterion 81.27051615715548
weighted_aux_loss 73.56027221679688
loss_r_bn_feature 73.56027221679688
------------iteration 1700----------
total loss 116.64370507016856
main criterion 61.35647743955333
weighted_aux_loss 55.287227630615234
loss_r_bn_feature 55.287227630615234
------------iteration 1800----------
total loss 95.9495383165154
main criterion 49.889048661974385
weighted_aux_loss 46.060489654541016
loss_r_bn_feature 46.060489654541016
------------iteration 1900----------
total loss 106.4110856304317
main criterion 54.3161492595821
weighted_aux_loss 52.09493637084961
loss_r_bn_feature 52.09493637084961
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 870 end_cls 880
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 849.8612119623899
main criterion 206.91559428660864
weighted_aux_loss 642.9456176757812
loss_r_bn_feature 642.9456176757812
------------iteration 100----------
total loss 442.133057806366
main criterion 98.06054804074101
weighted_aux_loss 344.072509765625
loss_r_bn_feature 344.072509765625
------------iteration 200----------
total loss 344.90841659819756
main criterion 97.98773178374445
weighted_aux_loss 246.92068481445312
loss_r_bn_feature 246.92068481445312
------------iteration 300----------
total loss 305.482088494684
main criterion 97.48550646343396
weighted_aux_loss 207.99658203125
loss_r_bn_feature 207.99658203125
------------iteration 400----------
total loss 271.5115228525995
main criterion 88.26106508892762
weighted_aux_loss 183.25045776367188
loss_r_bn_feature 183.25045776367188
------------iteration 500----------
total loss 270.7612412919176
main criterion 91.39959273234726
weighted_aux_loss 179.3616485595703
loss_r_bn_feature 179.3616485595703
------------iteration 600----------
total loss 242.46224083525172
main criterion 89.40994896513453
weighted_aux_loss 153.0522918701172
loss_r_bn_feature 153.0522918701172
------------iteration 700----------
total loss 257.6463561047621
main criterion 94.94974660769178
weighted_aux_loss 162.6966094970703
loss_r_bn_feature 162.6966094970703
------------iteration 800----------
total loss 246.94926002396244
main criterion 96.79481056107183
weighted_aux_loss 150.15444946289062
loss_r_bn_feature 150.15444946289062
------------iteration 900----------
total loss 192.1001886923671
main criterion 74.32514901951554
weighted_aux_loss 117.77503967285156
loss_r_bn_feature 117.77503967285156
------------iteration 1000----------
total loss 385.2451558160746
main criterion 150.4173970270121
weighted_aux_loss 234.8277587890625
loss_r_bn_feature 234.8277587890625
------------iteration 1100----------
total loss 186.66382525419348
main criterion 75.92656634306067
weighted_aux_loss 110.73725891113281
loss_r_bn_feature 110.73725891113281
------------iteration 1200----------
total loss 162.91212475538805
main criterion 68.61764538527085
weighted_aux_loss 94.29447937011719
loss_r_bn_feature 94.29447937011719
------------iteration 1300----------
total loss 187.74051576834637
main criterion 86.74323946219401
weighted_aux_loss 100.99727630615234
loss_r_bn_feature 100.99727630615234
------------iteration 1400----------
total loss 141.45843359218344
main criterion 65.33716436610923
weighted_aux_loss 76.12126922607422
loss_r_bn_feature 76.12126922607422
------------iteration 1500----------
total loss 135.7940443561164
main criterion 64.95811448551093
weighted_aux_loss 70.83592987060547
loss_r_bn_feature 70.83592987060547
------------iteration 1600----------
total loss 273.3823026154854
main criterion 113.04271826489949
weighted_aux_loss 160.33958435058594
loss_r_bn_feature 160.33958435058594
------------iteration 1700----------
total loss 111.58316198221706
main criterion 57.33129659525418
weighted_aux_loss 54.25186538696289
loss_r_bn_feature 54.25186538696289
------------iteration 1800----------
total loss 161.0410398694927
main criterion 81.29794447154347
weighted_aux_loss 79.74309539794922
loss_r_bn_feature 79.74309539794922
------------iteration 1900----------
total loss 115.62248837844331
main criterion 60.434744237818315
weighted_aux_loss 55.187744140625
loss_r_bn_feature 55.187744140625
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 880 end_cls 890
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 819.8337363437331
main criterion 202.1253012851393
weighted_aux_loss 617.7084350585938
loss_r_bn_feature 617.7084350585938
------------iteration 100----------
total loss 474.73763032552176
main criterion 135.87547822591242
weighted_aux_loss 338.8621520996094
loss_r_bn_feature 338.8621520996094
------------iteration 200----------
total loss 402.2806208010027
main criterion 110.67753242209649
weighted_aux_loss 291.60308837890625
loss_r_bn_feature 291.60308837890625
------------iteration 300----------
total loss 337.15916756406153
main criterion 105.59694222226464
weighted_aux_loss 231.56222534179688
loss_r_bn_feature 231.56222534179688
------------iteration 400----------
total loss 417.17176726096113
main criterion 158.58000090353923
weighted_aux_loss 258.5917663574219
loss_r_bn_feature 258.5917663574219
------------iteration 500----------
total loss 291.2531918018893
main criterion 109.83598599134238
weighted_aux_loss 181.41720581054688
loss_r_bn_feature 181.41720581054688
------------iteration 600----------
total loss 238.95221693866483
main criterion 83.95777113788357
weighted_aux_loss 154.99444580078125
loss_r_bn_feature 154.99444580078125
------------iteration 700----------
total loss 392.3430871195116
main criterion 153.14652339880848
weighted_aux_loss 239.19656372070312
loss_r_bn_feature 239.19656372070312
------------iteration 800----------
total loss 310.5519272331693
main criterion 138.70399632496614
weighted_aux_loss 171.84793090820312
loss_r_bn_feature 171.84793090820312
------------iteration 900----------
total loss 220.30329156907493
main criterion 94.0243990519851
weighted_aux_loss 126.27889251708984
loss_r_bn_feature 126.27889251708984
------------iteration 1000----------
total loss 201.51728976588703
main criterion 82.11322177272297
weighted_aux_loss 119.40406799316406
loss_r_bn_feature 119.40406799316406
------------iteration 1100----------
total loss 205.8634567948938
main criterion 87.56544501510866
weighted_aux_loss 118.29801177978516
loss_r_bn_feature 118.29801177978516
------------iteration 1200----------
total loss 170.7581753578947
main criterion 73.47190521629314
weighted_aux_loss 97.28627014160156
loss_r_bn_feature 97.28627014160156
------------iteration 1300----------
total loss 148.07164471883192
main criterion 66.87908643025769
weighted_aux_loss 81.19255828857422
loss_r_bn_feature 81.19255828857422
------------iteration 1400----------
total loss 148.1264189486356
main criterion 71.68534076260043
weighted_aux_loss 76.44107818603516
loss_r_bn_feature 76.44107818603516
------------iteration 1500----------
total loss 409.21202483460877
main criterion 155.51788726136658
weighted_aux_loss 253.6941375732422
loss_r_bn_feature 253.6941375732422
------------iteration 1600----------
total loss 119.0923849946596
main criterion 60.15037602249163
weighted_aux_loss 58.94200897216797
loss_r_bn_feature 58.94200897216797
------------iteration 1700----------
total loss 138.4744460666737
main criterion 73.12873531227915
weighted_aux_loss 65.34571075439453
loss_r_bn_feature 65.34571075439453
------------iteration 1800----------
total loss 109.5501863440675
main criterion 58.739418216626085
weighted_aux_loss 50.810768127441406
loss_r_bn_feature 50.810768127441406
------------iteration 1900----------
total loss 119.83985169799212
main criterion 57.96167786986712
weighted_aux_loss 61.878173828125
loss_r_bn_feature 61.878173828125
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 890 end_cls 900
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 802.4138171474849
main criterion 201.25421021389116
weighted_aux_loss 601.1596069335938
loss_r_bn_feature 601.1596069335938
------------iteration 100----------
total loss 442.8022323057622
main criterion 104.5860762999028
weighted_aux_loss 338.2161560058594
loss_r_bn_feature 338.2161560058594
------------iteration 200----------
total loss 392.5095429353832
main criterion 109.82530831624256
weighted_aux_loss 282.6842346191406
loss_r_bn_feature 282.6842346191406
------------iteration 300----------
total loss 358.36384819252385
main criterion 125.41426323158635
weighted_aux_loss 232.9495849609375
loss_r_bn_feature 232.9495849609375
------------iteration 400----------
total loss 288.5657658334583
main criterion 103.95352218111451
weighted_aux_loss 184.61224365234375
loss_r_bn_feature 184.61224365234375
------------iteration 500----------
total loss 269.24044287937147
main criterion 91.27070105808242
weighted_aux_loss 177.96974182128906
loss_r_bn_feature 177.96974182128906
------------iteration 600----------
total loss 277.84928916069043
main criterion 107.41181967826857
weighted_aux_loss 170.43746948242188
loss_r_bn_feature 170.43746948242188
------------iteration 700----------
total loss 324.7725194672109
main criterion 130.76078545842182
weighted_aux_loss 194.01173400878906
loss_r_bn_feature 194.01173400878906
------------iteration 800----------
total loss 227.01996459993939
main criterion 81.30757751497846
weighted_aux_loss 145.71238708496094
loss_r_bn_feature 145.71238708496094
------------iteration 900----------
total loss 246.31859396095882
main criterion 101.51178548927915
weighted_aux_loss 144.8068084716797
loss_r_bn_feature 144.8068084716797
------------iteration 1000----------
total loss 327.3893862681708
main criterion 143.93850431016298
weighted_aux_loss 183.4508819580078
loss_r_bn_feature 183.4508819580078
------------iteration 1100----------
total loss 353.02347471650955
main criterion 154.8300237887752
weighted_aux_loss 198.19345092773438
loss_r_bn_feature 198.19345092773438
------------iteration 1200----------
total loss 176.3923202540435
main criterion 75.24477539320367
weighted_aux_loss 101.14754486083984
loss_r_bn_feature 101.14754486083984
------------iteration 1300----------
total loss 174.03205026563853
main criterion 75.94518960890025
weighted_aux_loss 98.08686065673828
loss_r_bn_feature 98.08686065673828
------------iteration 1400----------
total loss 236.32797092477387
main criterion 111.21462101022308
weighted_aux_loss 125.11334991455078
loss_r_bn_feature 125.11334991455078
------------iteration 1500----------
total loss 128.30194890613717
main criterion 59.857307792855906
weighted_aux_loss 68.44464111328125
loss_r_bn_feature 68.44464111328125
------------iteration 1600----------
total loss 125.03822237243139
main criterion 62.32755953063452
weighted_aux_loss 62.710662841796875
loss_r_bn_feature 62.710662841796875
------------iteration 1700----------
total loss 111.77203328480118
main criterion 54.98265035023087
weighted_aux_loss 56.78938293457031
loss_r_bn_feature 56.78938293457031
------------iteration 1800----------
total loss 105.92541377404493
main criterion 54.13797633508009
weighted_aux_loss 51.787437438964844
loss_r_bn_feature 51.787437438964844
------------iteration 1900----------
total loss 137.23884152356007
main criterion 68.77430294934132
weighted_aux_loss 68.46453857421875
loss_r_bn_feature 68.46453857421875
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 900 end_cls 910
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 812.8528867630497
main criterion 194.4107480911747
weighted_aux_loss 618.442138671875
loss_r_bn_feature 618.442138671875
------------iteration 100----------
total loss 472.3219493933758
main criterion 107.64271965704768
weighted_aux_loss 364.6792297363281
loss_r_bn_feature 364.6792297363281
------------iteration 200----------
total loss 375.27060475579464
main criterion 109.35715260735715
weighted_aux_loss 265.9134521484375
loss_r_bn_feature 265.9134521484375
------------iteration 300----------
total loss 330.9371956269515
main criterion 102.24841388867026
weighted_aux_loss 228.68878173828125
loss_r_bn_feature 228.68878173828125
------------iteration 400----------
total loss 316.6131655404352
main criterion 105.51840846035708
weighted_aux_loss 211.09475708007812
loss_r_bn_feature 211.09475708007812
------------iteration 500----------
total loss 283.2983348757508
main criterion 96.74500540797736
weighted_aux_loss 186.55332946777344
loss_r_bn_feature 186.55332946777344
------------iteration 600----------
total loss 256.8087669849201
main criterion 89.15270009038885
weighted_aux_loss 167.65606689453125
loss_r_bn_feature 167.65606689453125
------------iteration 700----------
total loss 241.34826122099167
main criterion 84.22677074247606
weighted_aux_loss 157.12149047851562
loss_r_bn_feature 157.12149047851562
------------iteration 800----------
total loss 227.68481829796139
main criterion 83.67818072471921
weighted_aux_loss 144.0066375732422
loss_r_bn_feature 144.0066375732422
------------iteration 900----------
total loss 217.04884517638078
main criterion 82.37637508360736
weighted_aux_loss 134.67247009277344
loss_r_bn_feature 134.67247009277344
------------iteration 1000----------
total loss 225.2231422571869
main criterion 90.16723405406191
weighted_aux_loss 135.055908203125
loss_r_bn_feature 135.055908203125
------------iteration 1100----------
total loss 188.26291560738673
main criterion 77.14602565377345
weighted_aux_loss 111.11688995361328
loss_r_bn_feature 111.11688995361328
------------iteration 1200----------
total loss 193.5419484225486
main criterion 83.13488207733374
weighted_aux_loss 110.40706634521484
loss_r_bn_feature 110.40706634521484
------------iteration 1300----------
total loss 151.26886284749378
main criterion 65.97411644856801
weighted_aux_loss 85.29474639892578
loss_r_bn_feature 85.29474639892578
------------iteration 1400----------
total loss 138.96170124045082
main criterion 63.07494434347817
weighted_aux_loss 75.88675689697266
loss_r_bn_feature 75.88675689697266
------------iteration 1500----------
total loss 130.52634432083656
main criterion 60.78095247513344
weighted_aux_loss 69.74539184570312
loss_r_bn_feature 69.74539184570312
------------iteration 1600----------
total loss 308.79046978090366
main criterion 127.08459819887241
weighted_aux_loss 181.70587158203125
loss_r_bn_feature 181.70587158203125
------------iteration 1700----------
total loss 133.95450206046905
main criterion 69.00958628898466
weighted_aux_loss 64.94491577148438
loss_r_bn_feature 64.94491577148438
------------iteration 1800----------
total loss 124.62449319596598
main criterion 61.11835153336832
weighted_aux_loss 63.506141662597656
loss_r_bn_feature 63.506141662597656
------------iteration 1900----------
total loss 105.95410656251994
main criterion 50.90364956178752
weighted_aux_loss 55.05045700073242
loss_r_bn_feature 55.05045700073242
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 910 end_cls 920
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 827.6094815807895
main criterion 193.93974281125827
weighted_aux_loss 633.6697387695312
loss_r_bn_feature 633.6697387695312
------------iteration 100----------
total loss 435.7734110396692
main criterion 100.4249002974817
weighted_aux_loss 335.3485107421875
loss_r_bn_feature 335.3485107421875
------------iteration 200----------
total loss 313.97143725653973
main criterion 88.89625720282876
weighted_aux_loss 225.07518005371094
loss_r_bn_feature 225.07518005371094
------------iteration 300----------
total loss 279.33706825195486
main criterion 85.1240402978533
weighted_aux_loss 194.21302795410156
loss_r_bn_feature 194.21302795410156
------------iteration 400----------
total loss 257.9386386030649
main criterion 84.76014128861172
weighted_aux_loss 173.17849731445312
loss_r_bn_feature 173.17849731445312
------------iteration 500----------
total loss 468.4381385285784
main criterion 177.125302835219
weighted_aux_loss 291.3128356933594
loss_r_bn_feature 291.3128356933594
------------iteration 600----------
total loss 319.73990143512106
main criterion 123.86885346148821
weighted_aux_loss 195.8710479736328
loss_r_bn_feature 195.8710479736328
------------iteration 700----------
total loss 250.51654779226843
main criterion 95.14855157644811
weighted_aux_loss 155.3679962158203
loss_r_bn_feature 155.3679962158203
------------iteration 800----------
total loss 225.57322859708023
main criterion 89.18170332852556
weighted_aux_loss 136.3915252685547
loss_r_bn_feature 136.3915252685547
------------iteration 900----------
total loss 229.01392280337404
main criterion 98.83917915103028
weighted_aux_loss 130.17474365234375
loss_r_bn_feature 130.17474365234375
------------iteration 1000----------
total loss 196.30500971867457
main criterion 76.90198695256127
weighted_aux_loss 119.40302276611328
loss_r_bn_feature 119.40302276611328
------------iteration 1100----------
total loss 185.52318933322215
main criterion 72.46709039523388
weighted_aux_loss 113.05609893798828
loss_r_bn_feature 113.05609893798828
------------iteration 1200----------
total loss 172.9544486327984
main criterion 74.48523323973198
weighted_aux_loss 98.4692153930664
loss_r_bn_feature 98.4692153930664
------------iteration 1300----------
total loss 158.75806919883593
main criterion 73.72272221397265
weighted_aux_loss 85.03534698486328
loss_r_bn_feature 85.03534698486328
------------iteration 1400----------
total loss 129.85779422356796
main criterion 61.00410312249374
weighted_aux_loss 68.85369110107422
loss_r_bn_feature 68.85369110107422
------------iteration 1500----------
total loss 176.5557455877305
main criterion 80.30045841732036
weighted_aux_loss 96.25528717041016
loss_r_bn_feature 96.25528717041016
------------iteration 1600----------
total loss 149.35514945048033
main criterion 70.96379965799984
weighted_aux_loss 78.39134979248047
loss_r_bn_feature 78.39134979248047
------------iteration 1700----------
total loss 140.51198149499916
main criterion 69.45088530359291
weighted_aux_loss 71.06109619140625
loss_r_bn_feature 71.06109619140625
------------iteration 1800----------
total loss 98.39956381398966
main criterion 49.194241548364666
weighted_aux_loss 49.205322265625
loss_r_bn_feature 49.205322265625
------------iteration 1900----------
total loss 109.02826995556723
main criterion 51.30760116284261
weighted_aux_loss 57.72066879272461
loss_r_bn_feature 57.72066879272461
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 920 end_cls 930
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 789.8574548049552
main criterion 201.95089962917393
weighted_aux_loss 587.9065551757812
loss_r_bn_feature 587.9065551757812
------------iteration 100----------
total loss 403.84074054723334
main criterion 88.77778278356148
weighted_aux_loss 315.0629577636719
loss_r_bn_feature 315.0629577636719
------------iteration 200----------
total loss 351.1533582678076
main criterion 92.56931285765137
weighted_aux_loss 258.58404541015625
loss_r_bn_feature 258.58404541015625
------------iteration 300----------
total loss 312.3268839000476
main criterion 92.03249608266484
weighted_aux_loss 220.2943878173828
loss_r_bn_feature 220.2943878173828
------------iteration 400----------
total loss 281.1501174673054
main criterion 85.23843533839914
weighted_aux_loss 195.91168212890625
loss_r_bn_feature 195.91168212890625
------------iteration 500----------
total loss 319.6628849771436
main criterion 126.49107101229988
weighted_aux_loss 193.17181396484375
loss_r_bn_feature 193.17181396484375
------------iteration 600----------
total loss 286.3552070831295
main criterion 106.87472307434047
weighted_aux_loss 179.48048400878906
loss_r_bn_feature 179.48048400878906
------------iteration 700----------
total loss 240.68624252257953
main criterion 86.56022018371235
weighted_aux_loss 154.1260223388672
loss_r_bn_feature 154.1260223388672
------------iteration 800----------
total loss 200.5238576140939
main criterion 75.7098164763986
weighted_aux_loss 124.81404113769531
loss_r_bn_feature 124.81404113769531
------------iteration 900----------
total loss 195.90168617636516
main criterion 72.39252327353314
weighted_aux_loss 123.50916290283203
loss_r_bn_feature 123.50916290283203
------------iteration 1000----------
total loss 239.21246328358134
main criterion 100.8073271751829
weighted_aux_loss 138.40513610839844
loss_r_bn_feature 138.40513610839844
------------iteration 1100----------
total loss 175.6754828622058
main criterion 69.26680671474485
weighted_aux_loss 106.40867614746094
loss_r_bn_feature 106.40867614746094
------------iteration 1200----------
total loss 164.4730392998376
main criterion 66.68654027640011
weighted_aux_loss 97.7864990234375
loss_r_bn_feature 97.7864990234375
------------iteration 1300----------
total loss 304.94760149822366
main criterion 134.58300799236426
weighted_aux_loss 170.36459350585938
loss_r_bn_feature 170.36459350585938
------------iteration 1400----------
total loss 134.7837989928381
main criterion 60.43025285025997
weighted_aux_loss 74.35354614257812
loss_r_bn_feature 74.35354614257812
------------iteration 1500----------
total loss 147.48192910784016
main criterion 67.16352395647297
weighted_aux_loss 80.31840515136719
loss_r_bn_feature 80.31840515136719
------------iteration 1600----------
total loss 113.64925738051517
main criterion 56.94557162001712
weighted_aux_loss 56.70368576049805
loss_r_bn_feature 56.70368576049805
------------iteration 1700----------
total loss 174.18815662257094
main criterion 76.83036090723893
weighted_aux_loss 97.35779571533203
loss_r_bn_feature 97.35779571533203
------------iteration 1800----------
total loss 108.94797340564116
main criterion 55.50721946887359
weighted_aux_loss 53.44075393676758
loss_r_bn_feature 53.44075393676758
------------iteration 1900----------
total loss 402.84690559365157
main criterion 157.61762702919845
weighted_aux_loss 245.22927856445312
loss_r_bn_feature 245.22927856445312
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 930 end_cls 940
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 777.9222215059892
main criterion 216.99717267786423
weighted_aux_loss 560.925048828125
loss_r_bn_feature 560.925048828125
------------iteration 100----------
total loss 483.0742612961435
main criterion 159.18836652075288
weighted_aux_loss 323.8858947753906
loss_r_bn_feature 323.8858947753906
------------iteration 200----------
total loss 331.5092133390058
main criterion 89.54971016517767
weighted_aux_loss 241.95950317382812
loss_r_bn_feature 241.95950317382812
------------iteration 300----------
total loss 310.08211983479544
main criterion 93.04189766682666
weighted_aux_loss 217.04022216796875
loss_r_bn_feature 217.04022216796875
------------iteration 400----------
total loss 279.8946476997691
main criterion 86.53682909625347
weighted_aux_loss 193.35781860351562
loss_r_bn_feature 193.35781860351562
------------iteration 500----------
total loss 269.716120212291
main criterion 89.48487326404882
weighted_aux_loss 180.2312469482422
loss_r_bn_feature 180.2312469482422
------------iteration 600----------
total loss 606.360285744466
main criterion 210.2947339866534
weighted_aux_loss 396.0655517578125
loss_r_bn_feature 396.0655517578125
------------iteration 700----------
total loss 238.55078246751106
main criterion 81.49398925462042
weighted_aux_loss 157.05679321289062
loss_r_bn_feature 157.05679321289062
------------iteration 800----------
total loss 219.617938905183
main criterion 85.04622259658923
weighted_aux_loss 134.57171630859375
loss_r_bn_feature 134.57171630859375
------------iteration 900----------
total loss 282.1148993289052
main criterion 127.41102664824116
weighted_aux_loss 154.70387268066406
loss_r_bn_feature 154.70387268066406
------------iteration 1000----------
total loss 187.83284392306115
main criterion 71.2456246847799
weighted_aux_loss 116.58721923828125
loss_r_bn_feature 116.58721923828125
------------iteration 1100----------
total loss 168.8016745294298
main criterion 68.3889090265001
weighted_aux_loss 100.41276550292969
loss_r_bn_feature 100.41276550292969
------------iteration 1200----------
total loss 153.03324939385448
main criterion 65.12240649834668
weighted_aux_loss 87.91084289550781
loss_r_bn_feature 87.91084289550781
------------iteration 1300----------
total loss 151.299475545395
main criterion 70.81591689060984
weighted_aux_loss 80.48355865478516
loss_r_bn_feature 80.48355865478516
------------iteration 1400----------
total loss 143.77322440604686
main criterion 61.39534812430858
weighted_aux_loss 82.37787628173828
loss_r_bn_feature 82.37787628173828
------------iteration 1500----------
total loss 152.60295038936033
main criterion 68.33652430293455
weighted_aux_loss 84.26642608642578
loss_r_bn_feature 84.26642608642578
------------iteration 1600----------
total loss 133.79182662772652
main criterion 66.55523910331246
weighted_aux_loss 67.23658752441406
loss_r_bn_feature 67.23658752441406
------------iteration 1700----------
total loss 248.71895210476535
main criterion 100.93126289578097
weighted_aux_loss 147.78768920898438
loss_r_bn_feature 147.78768920898438
------------iteration 1800----------
total loss 196.0085541742349
main criterion 93.23890848331692
weighted_aux_loss 102.76964569091797
loss_r_bn_feature 102.76964569091797
------------iteration 1900----------
total loss 132.30365791824028
main criterion 66.74345436599417
weighted_aux_loss 65.5602035522461
loss_r_bn_feature 65.5602035522461
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 940 end_cls 950
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 882.7947263779333
main criterion 210.9719114365271
weighted_aux_loss 671.8228149414062
loss_r_bn_feature 671.8228149414062
------------iteration 100----------
total loss 396.630313012281
main criterion 89.246523949781
weighted_aux_loss 307.3837890625
loss_r_bn_feature 307.3837890625
------------iteration 200----------
total loss 346.5058202485252
main criterion 89.45485589305646
weighted_aux_loss 257.05096435546875
loss_r_bn_feature 257.05096435546875
------------iteration 300----------
total loss 339.84727351404376
main criterion 119.50771968103595
weighted_aux_loss 220.3395538330078
loss_r_bn_feature 220.3395538330078
------------iteration 400----------
total loss 330.1876788605111
main criterion 106.29587893375326
weighted_aux_loss 223.8917999267578
loss_r_bn_feature 223.8917999267578
------------iteration 500----------
total loss 271.92146101875994
main criterion 82.92255965157244
weighted_aux_loss 188.9989013671875
loss_r_bn_feature 188.9989013671875
------------iteration 600----------
total loss 396.7078203758841
main criterion 149.37317987295447
weighted_aux_loss 247.3346405029297
loss_r_bn_feature 247.3346405029297
------------iteration 700----------
total loss 245.32294725247928
main criterion 80.47425340482303
weighted_aux_loss 164.84869384765625
loss_r_bn_feature 164.84869384765625
------------iteration 800----------
total loss 275.34163425128503
main criterion 104.38429782550378
weighted_aux_loss 170.95733642578125
loss_r_bn_feature 170.95733642578125
------------iteration 900----------
total loss 280.6157444572243
main criterion 110.42925153730246
weighted_aux_loss 170.18649291992188
loss_r_bn_feature 170.18649291992188
------------iteration 1000----------
total loss 199.74416341536812
main criterion 75.12059774153998
weighted_aux_loss 124.62356567382812
loss_r_bn_feature 124.62356567382812
------------iteration 1100----------
total loss 245.00094749588277
main criterion 102.02929832596088
weighted_aux_loss 142.97164916992188
loss_r_bn_feature 142.97164916992188
------------iteration 1200----------
total loss 213.95169330808943
main criterion 74.73036457273787
weighted_aux_loss 139.22132873535156
loss_r_bn_feature 139.22132873535156
------------iteration 1300----------
total loss 359.4971893719494
main criterion 160.60413822448845
weighted_aux_loss 198.89305114746094
loss_r_bn_feature 198.89305114746094
------------iteration 1400----------
total loss 128.0078916614979
main criterion 57.715807921263526
weighted_aux_loss 70.29208374023438
loss_r_bn_feature 70.29208374023438
------------iteration 1500----------
total loss 153.9002179360226
main criterion 59.132960245592926
weighted_aux_loss 94.76725769042969
loss_r_bn_feature 94.76725769042969
------------iteration 1600----------
total loss 146.7071710061706
main criterion 72.98888639923699
weighted_aux_loss 73.7182846069336
loss_r_bn_feature 73.7182846069336
------------iteration 1700----------
total loss 131.83051290281412
main criterion 66.46095266111489
weighted_aux_loss 65.36956024169922
loss_r_bn_feature 65.36956024169922
------------iteration 1800----------
total loss 156.0122304437805
main criterion 63.640900182550034
weighted_aux_loss 92.37133026123047
loss_r_bn_feature 92.37133026123047
------------iteration 1900----------
total loss 113.98880949364678
main criterion 60.657983687982714
weighted_aux_loss 53.33082580566406
loss_r_bn_feature 53.33082580566406
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 950 end_cls 960
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 834.939093090863
main criterion 212.80475471195683
weighted_aux_loss 622.1343383789062
loss_r_bn_feature 622.1343383789062
------------iteration 100----------
total loss 387.9136212003665
main criterion 93.60411192302271
weighted_aux_loss 294.30950927734375
loss_r_bn_feature 294.30950927734375
------------iteration 200----------
total loss 409.2912220083672
main criterion 148.62636605133596
weighted_aux_loss 260.66485595703125
loss_r_bn_feature 260.66485595703125
------------iteration 300----------
total loss 307.81649221251723
main criterion 87.67975820372816
weighted_aux_loss 220.13673400878906
loss_r_bn_feature 220.13673400878906
------------iteration 400----------
total loss 268.992395696382
main criterion 81.78013068173358
weighted_aux_loss 187.21226501464844
loss_r_bn_feature 187.21226501464844
------------iteration 500----------
total loss 260.791769810295
main criterion 81.65711099681847
weighted_aux_loss 179.13465881347656
loss_r_bn_feature 179.13465881347656
------------iteration 600----------
total loss 253.18285837143594
main criterion 85.20940866440468
weighted_aux_loss 167.97344970703125
loss_r_bn_feature 167.97344970703125
------------iteration 700----------
total loss 234.46443699586877
main criterion 84.9635672448922
weighted_aux_loss 149.50086975097656
loss_r_bn_feature 149.50086975097656
------------iteration 800----------
total loss 201.80797829984618
main criterion 73.91804757474854
weighted_aux_loss 127.88993072509766
loss_r_bn_feature 127.88993072509766
------------iteration 900----------
total loss 227.0401751450324
main criterion 97.85458249366522
weighted_aux_loss 129.1855926513672
loss_r_bn_feature 129.1855926513672
------------iteration 1000----------
total loss 265.6849336749955
main criterion 114.59623433417514
weighted_aux_loss 151.0886993408203
loss_r_bn_feature 151.0886993408203
------------iteration 1100----------
total loss 233.13584127937855
main criterion 107.0951079419762
weighted_aux_loss 126.04073333740234
loss_r_bn_feature 126.04073333740234
------------iteration 1200----------
total loss 216.5000603216276
main criterion 99.3792336004362
weighted_aux_loss 117.1208267211914
loss_r_bn_feature 117.1208267211914
------------iteration 1300----------
total loss 146.75837651514115
main criterion 66.34059850000445
weighted_aux_loss 80.41777801513672
loss_r_bn_feature 80.41777801513672
------------iteration 1400----------
total loss 259.2990022056965
main criterion 121.9678712242512
weighted_aux_loss 137.3311309814453
loss_r_bn_feature 137.3311309814453
------------iteration 1500----------
total loss 121.66291580567938
main criterion 55.769666293960626
weighted_aux_loss 65.89324951171875
loss_r_bn_feature 65.89324951171875
------------iteration 1600----------
total loss 168.6819486184063
main criterion 82.94011207055473
weighted_aux_loss 85.74183654785156
loss_r_bn_feature 85.74183654785156
------------iteration 1700----------
total loss 124.44939081858449
main criterion 63.39609949778371
weighted_aux_loss 61.05329132080078
loss_r_bn_feature 61.05329132080078
------------iteration 1800----------
total loss 99.01557441008316
main criterion 53.82876487028824
weighted_aux_loss 45.18680953979492
loss_r_bn_feature 45.18680953979492
------------iteration 1900----------
total loss 295.7587529963179
main criterion 129.43883722483355
weighted_aux_loss 166.31991577148438
loss_r_bn_feature 166.31991577148438
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 960 end_cls 970
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 796.5539770469683
main criterion 203.42513183212452
weighted_aux_loss 593.1288452148438
loss_r_bn_feature 593.1288452148438
------------iteration 100----------
total loss 434.6667957258231
main criterion 91.37266730785433
weighted_aux_loss 343.29412841796875
loss_r_bn_feature 343.29412841796875
------------iteration 200----------
total loss 596.922667666214
main criterion 187.66140667988583
weighted_aux_loss 409.2612609863281
loss_r_bn_feature 409.2612609863281
------------iteration 300----------
total loss 314.4301692835181
main criterion 89.98015707648689
weighted_aux_loss 224.45001220703125
loss_r_bn_feature 224.45001220703125
------------iteration 400----------
total loss 276.2867206471098
main criterion 89.67899359632857
weighted_aux_loss 186.60772705078125
loss_r_bn_feature 186.60772705078125
------------iteration 500----------
total loss 275.94079458709587
main criterion 90.34318411346302
weighted_aux_loss 185.5976104736328
loss_r_bn_feature 185.5976104736328
------------iteration 600----------
total loss 368.25342903284206
main criterion 156.26229438928738
weighted_aux_loss 211.9911346435547
loss_r_bn_feature 211.9911346435547
------------iteration 700----------
total loss 216.0287489146741
main criterion 78.41052381701786
weighted_aux_loss 137.61822509765625
loss_r_bn_feature 137.61822509765625
------------iteration 800----------
total loss 229.23114112911463
main criterion 85.19459632930993
weighted_aux_loss 144.0365447998047
loss_r_bn_feature 144.0365447998047
------------iteration 900----------
total loss 204.73934862711764
main criterion 78.12080309489106
weighted_aux_loss 126.61854553222656
loss_r_bn_feature 126.61854553222656
------------iteration 1000----------
total loss 199.78531902458877
main criterion 84.85570018913954
weighted_aux_loss 114.92961883544922
loss_r_bn_feature 114.92961883544922
------------iteration 1100----------
total loss /home/techt/anaconda3/envs/hl/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/techt/anaconda3/envs/hl/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
/home/techt/anaconda3/envs/hl/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
459.8671756631807
main criterion 179.55711706943072
weighted_aux_loss 280.31005859375
loss_r_bn_feature 280.31005859375
------------iteration 1200----------
total loss 185.37974899427587
main criterion 81.48694198743992
weighted_aux_loss 103.89280700683594
loss_r_bn_feature 103.89280700683594
------------iteration 1300----------
total loss 150.87108081282548
main criterion 66.97020953597
weighted_aux_loss 83.90087127685547
loss_r_bn_feature 83.90087127685547
------------iteration 1400----------
total loss 145.25601453382848
main criterion 64.55209607679723
weighted_aux_loss 80.70391845703125
loss_r_bn_feature 80.70391845703125
------------iteration 1500----------
total loss 123.82972403446291
main criterion 57.71672507206058
weighted_aux_loss 66.11299896240234
loss_r_bn_feature 66.11299896240234
------------iteration 1600----------
total loss 121.16899576428733
main criterion 58.58221140149436
weighted_aux_loss 62.58678436279297
loss_r_bn_feature 62.58678436279297
------------iteration 1700----------
total loss 264.1383995652672
main criterion 116.69726797347032
weighted_aux_loss 147.44113159179688
loss_r_bn_feature 147.44113159179688
------------iteration 1800----------
total loss 99.08851528295564
main criterion 51.832068444576734
weighted_aux_loss 47.256446838378906
loss_r_bn_feature 47.256446838378906
------------iteration 1900----------
total loss 206.60263928128356
main criterion 103.9712763662445
weighted_aux_loss 102.63136291503906
loss_r_bn_feature 102.63136291503906
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 970 end_cls 980
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 778.701453615204
main criterion 176.51047461129767
weighted_aux_loss 602.1909790039062
loss_r_bn_feature 602.1909790039062
------------iteration 100----------
total loss 416.34943585497615
main criterion 89.73325543505428
weighted_aux_loss 326.6161804199219
loss_r_bn_feature 326.6161804199219
------------iteration 200----------
total loss 445.53224319810295
main criterion 140.9493269383373
weighted_aux_loss 304.5829162597656
loss_r_bn_feature 304.5829162597656
------------iteration 300----------
total loss 400.5794796940713
main criterion 94.52940034836817
weighted_aux_loss 306.0500793457031
loss_r_bn_feature 306.0500793457031
------------iteration 400----------
total loss 294.78264281796623
main criterion 87.63710448788811
weighted_aux_loss 207.14553833007812
loss_r_bn_feature 207.14553833007812
------------iteration 500----------
total loss 335.62801321963366
main criterion 114.37987479189928
weighted_aux_loss 221.24813842773438
loss_r_bn_feature 221.24813842773438
------------iteration 600----------
total loss 261.08132971516756
main criterion 82.32675207844879
weighted_aux_loss 178.75457763671875
loss_r_bn_feature 178.75457763671875
------------iteration 700----------
total loss 319.8095247309058
main criterion 118.74102802680424
weighted_aux_loss 201.06849670410156
loss_r_bn_feature 201.06849670410156
------------iteration 800----------
total loss 237.02257010984508
main criterion 83.30630729246226
weighted_aux_loss 153.7162628173828
loss_r_bn_feature 153.7162628173828
------------iteration 900----------
total loss 330.3519882079315
main criterion 143.28912199699406
weighted_aux_loss 187.0628662109375
loss_r_bn_feature 187.0628662109375
------------iteration 1000----------
total loss 302.7824786627201
main criterion 135.26964602111858
weighted_aux_loss 167.51283264160156
loss_r_bn_feature 167.51283264160156
------------iteration 1100----------
total loss 170.9584390769528
main criterion 67.22137853007779
weighted_aux_loss 103.737060546875
loss_r_bn_feature 103.737060546875
------------iteration 1200----------
total loss 194.91010217741874
main criterion 80.39856653288749
weighted_aux_loss 114.51153564453125
loss_r_bn_feature 114.51153564453125
------------iteration 1300----------
total loss 183.8279672130357
main criterion 77.56540160024272
weighted_aux_loss 106.26256561279297
loss_r_bn_feature 106.26256561279297
------------iteration 1400----------
total loss 128.50936317964087
main criterion 56.83417702241431
weighted_aux_loss 71.67518615722656
loss_r_bn_feature 71.67518615722656
------------iteration 1500----------
total loss 132.2638801224581
main criterion 61.384058345114326
weighted_aux_loss 70.87982177734375
loss_r_bn_feature 70.87982177734375
------------iteration 1600----------
total loss 142.9028194979632
main criterion 56.27293668546321
weighted_aux_loss 86.6298828125
loss_r_bn_feature 86.6298828125
------------iteration 1700----------
total loss 102.81196117069108
main criterion 48.05076503421647
weighted_aux_loss 54.76119613647461
loss_r_bn_feature 54.76119613647461
------------iteration 1800----------
total loss 284.5779467030038
main criterion 127.77780632214444
weighted_aux_loss 156.80014038085938
loss_r_bn_feature 156.80014038085938
------------iteration 1900----------
total loss 90.53865246194607
main criterion 45.062707942903096
weighted_aux_loss 45.47594451904297
loss_r_bn_feature 45.47594451904297
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 980 end_cls 990
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 869.1005331040643
main criterion 203.83551845562684
weighted_aux_loss 665.2650146484375
loss_r_bn_feature 665.2650146484375
------------iteration 100----------
total loss 515.8394364423086
main criterion 156.9312333173086
weighted_aux_loss 358.908203125
loss_r_bn_feature 358.908203125
------------iteration 200----------
total loss 449.6179749716455
main criterion 152.9778382528955
weighted_aux_loss 296.64013671875
loss_r_bn_feature 296.64013671875
------------iteration 300----------
total loss 384.54996831090943
main criterion 131.96096379430787
weighted_aux_loss 252.58900451660156
loss_r_bn_feature 252.58900451660156
------------iteration 400----------
total loss 370.9910818980353
main criterion 94.37554234725404
weighted_aux_loss 276.61553955078125
loss_r_bn_feature 276.61553955078125
------------iteration 500----------
total loss 352.9827338229381
main criterion 116.83222112762557
weighted_aux_loss 236.1505126953125
loss_r_bn_feature 236.1505126953125
------------iteration 600----------
total loss 240.7535758959436
main criterion 76.48865280024049
weighted_aux_loss 164.26492309570312
loss_r_bn_feature 164.26492309570312
------------iteration 700----------
total loss 216.8095220606159
main criterion 75.71067562506902
weighted_aux_loss 141.09884643554688
loss_r_bn_feature 141.09884643554688
------------iteration 800----------
total loss 247.41943329578282
main criterion 100.65820282703284
weighted_aux_loss 146.76123046875
loss_r_bn_feature 146.76123046875
------------iteration 900----------
total loss 227.83465991626935
main criterion 83.44905505787092
weighted_aux_loss 144.38560485839844
loss_r_bn_feature 144.38560485839844
------------iteration 1000----------
total loss 183.37436610710594
main criterion 70.91429072868797
weighted_aux_loss 112.46007537841797
loss_r_bn_feature 112.46007537841797
------------iteration 1100----------
total loss 182.1277036202889
main criterion 72.23440070280846
weighted_aux_loss 109.89330291748047
loss_r_bn_feature 109.89330291748047
------------iteration 1200----------
total loss 304.97869347830397
main criterion 133.70815514822587
weighted_aux_loss 171.27053833007812
loss_r_bn_feature 171.27053833007812
------------iteration 1300----------
total loss 143.4181573905182
main criterion 60.72669773475648
weighted_aux_loss 82.69145965576172
loss_r_bn_feature 82.69145965576172
------------iteration 1400----------
total loss 152.11100248666622
main criterion 69.04236082406855
weighted_aux_loss 83.06864166259766
loss_r_bn_feature 83.06864166259766
------------iteration 1500----------
total loss 157.08637965962853
main criterion 79.76518977925744
weighted_aux_loss 77.3211898803711
loss_r_bn_feature 77.3211898803711
------------iteration 1600----------
total loss 113.36223454813772
main criterion 54.58193822245413
weighted_aux_loss 58.780296325683594
loss_r_bn_feature 58.780296325683594
------------iteration 1700----------
total loss 120.12732754339243
main criterion 57.61148892034555
weighted_aux_loss 62.515838623046875
loss_r_bn_feature 62.515838623046875
------------iteration 1800----------
total loss 124.96493540225549
main criterion 60.44205484805627
weighted_aux_loss 64.52288055419922
loss_r_bn_feature 64.52288055419922
------------iteration 1900----------
total loss 106.40093661566974
main criterion 57.67874575873614
weighted_aux_loss 48.722190856933594
loss_r_bn_feature 48.722190856933594
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
start_cls 990 end_cls 1000
Length of input 100 is different from effective batch size 100
------------iteration 0----------
total loss 793.7239874517389
main criterion 208.4763067876764
weighted_aux_loss 585.2476806640625
loss_r_bn_feature 585.2476806640625
------------iteration 100----------
total loss 393.8965692874204
main criterion 98.3098993655454
weighted_aux_loss 295.586669921875
loss_r_bn_feature 295.586669921875
------------iteration 200----------
total loss 325.30180759851754
main criterion 92.80656834070506
weighted_aux_loss 232.4952392578125
loss_r_bn_feature 232.4952392578125
------------iteration 300----------
total loss 280.6427730243561
main criterion 93.28243672064514
weighted_aux_loss 187.36033630371094
loss_r_bn_feature 187.36033630371094
------------iteration 400----------
total loss 312.32551222866914
main criterion 130.26682692593474
weighted_aux_loss 182.05868530273438
loss_r_bn_feature 182.05868530273438
------------iteration 500----------
total loss 248.12700350897882
main criterion 84.26248629706474
weighted_aux_loss 163.86451721191406
loss_r_bn_feature 163.86451721191406
------------iteration 600----------
total loss 234.97319111342324
main criterion 84.04716572279823
weighted_aux_loss 150.926025390625
loss_r_bn_feature 150.926025390625
------------iteration 700----------
total loss 403.58227666563573
main criterion 161.6177686089951
weighted_aux_loss 241.96450805664062
loss_r_bn_feature 241.96450805664062
------------iteration 800----------
total loss 270.6584229804008
main criterion 128.16026929387735
weighted_aux_loss 142.49815368652344
loss_r_bn_feature 142.49815368652344
------------iteration 900----------
total loss 189.3157443219138
main criterion 74.76644317445286
weighted_aux_loss 114.54930114746094
loss_r_bn_feature 114.54930114746094
------------iteration 1000----------
total loss 181.03027540168563
main criterion 76.89612012824813
weighted_aux_loss 104.1341552734375
loss_r_bn_feature 104.1341552734375
------------iteration 1100----------
total loss 283.6126489297832
main criterion 128.748101200291
weighted_aux_loss 154.8645477294922
loss_r_bn_feature 154.8645477294922
------------iteration 1200----------
total loss 145.4412022006413
main criterion 66.00800517915692
weighted_aux_loss 79.43319702148438
loss_r_bn_feature 79.43319702148438
------------iteration 1300----------
total loss 133.47464400324887
main criterion 63.181019125319175
weighted_aux_loss 70.29362487792969
loss_r_bn_feature 70.29362487792969
------------iteration 1400----------
total loss 123.38874326885363
main criterion 59.9640415781798
weighted_aux_loss 63.42470169067383
loss_r_bn_feature 63.42470169067383
------------iteration 1500----------
total loss 160.4900280096801
main criterion 67.29109154727776
weighted_aux_loss 93.19893646240234
loss_r_bn_feature 93.19893646240234
------------iteration 1600----------
total loss 127.99637046768689
main criterion 65.40286079361462
weighted_aux_loss 62.593509674072266
loss_r_bn_feature 62.593509674072266
------------iteration 1700----------
total loss 139.8589433988385
main criterion 66.16801780069399
weighted_aux_loss 73.69092559814453
loss_r_bn_feature 73.69092559814453
------------iteration 1800----------
total loss 93.01780697799343
main criterion 50.43204495406765
weighted_aux_loss 42.58576202392578
loss_r_bn_feature 42.58576202392578
------------iteration 1900----------
total loss 87.18602705382287
main criterion 47.85761976622522
weighted_aux_loss 39.328407287597656
loss_r_bn_feature 39.328407287597656
=> using pytorch pre-trained model 'resnet18'
process data from ../recover/syn_data/imagenet/303
  0%|          | 0/300 [00:00<?, ?it/s]  0%|          | 1/300 [00:06<33:57,  6.82s/it]  1%|          | 2/300 [00:10<25:15,  5.09s/it]  1%|          | 3/300 [00:14<22:27,  4.54s/it]  1%|▏         | 4/300 [00:18<21:12,  4.30s/it]  2%|▏         | 5/300 [00:22<20:31,  4.17s/it]  2%|▏         | 6/300 [00:26<19:59,  4.08s/it]  2%|▏         | 7/300 [00:30<19:38,  4.02s/it]  3%|▎         | 8/300 [00:34<19:27,  4.00s/it]  3%|▎         | 9/300 [00:38<19:15,  3.97s/it]  3%|▎         | 10/300 [00:42<19:06,  3.95s/it]  4%|▎         | 11/300 [00:45<18:58,  3.94s/it]  4%|▍         | 12/300 [00:49<18:51,  3.93s/it]  4%|▍         | 13/300 [00:53<18:47,  3.93s/it]  5%|▍         | 14/300 [00:57<18:43,  3.93s/it]  5%|▌         | 15/300 [01:01<18:39,  3.93s/it]  5%|▌         | 16/300 [01:05<18:42,  3.95s/it]  6%|▌         | 17/300 [01:09<18:33,  3.93s/it]  6%|▌         | 18/300 [01:13<18:33,  3.95s/it]  6%|▋         | 19/300 [01:17<18:29,  3.95s/it]  7%|▋         | 20/300 [01:21<18:31,  3.97s/it]  7%|▋         | 21/300 [01:25<18:24,  3.96s/it]  7%|▋         | 22/300 [01:29<18:14,  3.94s/it]  8%|▊         | 23/300 [01:33<18:09,  3.93s/it]  8%|▊         | 24/300 [01:37<18:06,  3.94s/it]  8%|▊         | 25/300 [01:41<18:01,  3.93s/it]  9%|▊         | 26/300 [01:45<17:58,  3.94s/it]  9%|▉         | 27/300 [01:49<17:57,  3.95s/it]  9%|▉         | 28/300 [01:52<17:53,  3.95s/it] 10%|▉         | 29/300 [01:56<17:49,  3.95s/it] 10%|█         | 30/300 [02:00<17:55,  3.98s/it] 10%|█         | 31/300 [02:04<17:49,  3.98s/it] 11%|█         | 32/300 [02:08<17:41,  3.96s/it] 11%|█         | 33/300 [02:12<17:37,  3.96s/it] 11%|█▏        | 34/300 [02:16<17:33,  3.96s/it] 12%|█▏        | 35/300 [02:20<17:30,  3.96s/it] 12%|█▏        | 36/300 [02:24<17:30,  3.98s/it] 12%|█▏        | 37/300 [02:28<17:28,  3.99s/it] 13%|█▎        | 38/300 [02:32<17:21,  3.97s/it] 13%|█▎        | 39/300 [02:36<17:17,  3.97s/it] 13%|█▎        | 40/300 [02:40<17:13,  3.97s/it] 14%|█▎        | 41/300 [02:44<17:11,  3.98s/it] 14%|█▍        | 42/300 [02:48<17:09,  3.99s/it] 14%|█▍        | 43/300 [02:52<17:10,  4.01s/it] 15%|█▍        | 44/300 [02:56<17:03,  4.00s/it] 15%|█▌        | 45/300 [03:00<16:58,  3.99s/it] 15%|█▌        | 46/300 [03:04<17:10,  4.06s/it] 16%|█▌        | 47/300 [03:09<17:17,  4.10s/it] 16%|█▌        | 48/300 [03:13<17:13,  4.10s/it] 16%|█▋        | 49/300 [03:17<17:04,  4.08s/it] 17%|█▋        | 50/300 [03:21<16:50,  4.04s/it] 17%|█▋        | 51/300 [03:25<16:45,  4.04s/it] 17%|█▋        | 52/300 [03:29<16:32,  4.00s/it] 18%|█▊        | 53/300 [03:33<16:36,  4.04s/it] 18%|█▊        | 54/300 [03:37<16:38,  4.06s/it] 18%|█▊        | 55/300 [03:41<16:34,  4.06s/it] 19%|█▊        | 56/300 [03:45<16:22,  4.03s/it] 19%|█▉        | 57/300 [03:49<16:17,  4.02s/it] 19%|█▉        | 58/300 [03:53<16:09,  4.01s/it] 20%|█▉        | 59/300 [03:57<16:18,  4.06s/it] 20%|██        | 60/300 [04:01<16:05,  4.02s/it] 20%|██        | 61/300 [04:05<16:02,  4.03s/it] 21%|██        | 62/300 [04:09<16:00,  4.04s/it] 21%|██        | 63/300 [04:13<15:50,  4.01s/it] 21%|██▏       | 64/300 [04:17<15:48,  4.02s/it] 22%|██▏       | 65/300 [04:21<15:45,  4.02s/it] 22%|██▏       | 66/300 [04:25<15:37,  4.01s/it] 22%|██▏       | 67/300 [04:29<15:32,  4.00s/it] 23%|██▎       | 68/300 [04:33<15:29,  4.00s/it] 23%|██▎       | 69/300 [04:37<15:20,  3.99s/it] 23%|██▎       | 70/300 [04:41<15:23,  4.01s/it] 24%|██▎       | 71/300 [04:45<15:22,  4.03s/it] 24%|██▍       | 72/300 [04:49<15:13,  4.01s/it] 24%|██▍       | 73/300 [04:53<15:09,  4.01s/it] 25%|██▍       | 74/300 [04:57<15:08,  4.02s/it] 25%|██▌       | 75/300 [05:01<15:00,  4.00s/it] 25%|██▌       | 76/300 [05:05<14:59,  4.02s/it] 26%|██▌       | 77/300 [05:09<14:55,  4.02s/it] 26%|██▌       | 78/300 [05:13<14:51,  4.02s/it] 26%|██▋       | 79/300 [05:17<14:43,  4.00s/it] 27%|██▋       | 80/300 [05:21<14:39,  4.00s/it] 27%|██▋       | 81/300 [05:25<14:52,  4.07s/it] 27%|██▋       | 82/300 [05:29<14:46,  4.07s/it] 28%|██▊       | 83/300 [05:33<14:38,  4.05s/it] 28%|██▊       | 84/300 [05:37<14:28,  4.02s/it] 28%|██▊       | 85/300 [05:41<14:23,  4.02s/it] 29%|██▊       | 86/300 [05:45<14:19,  4.02s/it] 29%|██▉       | 87/300 [05:49<14:10,  3.99s/it] 29%|██▉       | 88/300 [05:53<14:13,  4.02s/it] 30%|██▉       | 89/300 [05:57<14:04,  4.00s/it] 30%|███       | 90/300 [06:02<14:05,  4.02s/it] 30%|███       | 91/300 [06:06<14:07,  4.06s/it] 31%|███       | 92/300 [06:10<14:02,  4.05s/it] 31%|███       | 93/300 [06:14<14:00,  4.06s/it] 31%|███▏      | 94/300 [06:18<13:56,  4.06s/it] 32%|███▏      | 95/300 [06:22<13:50,  4.05s/it] 32%|███▏      | 96/300 [06:26<13:43,  4.04s/it] 32%|███▏      | 97/300 [06:30<13:42,  4.05s/it] 33%|███▎      | 98/300 [06:34<13:31,  4.02s/it] 33%|███▎      | 99/300 [06:38<13:26,  4.01s/it] 33%|███▎      | 100/300 [06:42<13:22,  4.01s/it] 34%|███▎      | 101/300 [06:46<13:21,  4.03s/it] 34%|███▍      | 102/300 [06:50<13:18,  4.03s/it] 34%|███▍      | 103/300 [06:54<13:14,  4.03s/it] 35%|███▍      | 104/300 [06:58<13:07,  4.02s/it] 35%|███▌      | 105/300 [07:02<13:00,  4.00s/it] 35%|███▌      | 106/300 [07:06<13:00,  4.02s/it] 36%|███▌      | 107/300 [07:10<13:05,  4.07s/it] 36%|███▌      | 108/300 [07:14<13:05,  4.09s/it] 36%|███▋      | 109/300 [07:19<13:03,  4.10s/it] 37%|███▋      | 110/300 [07:23<13:00,  4.11s/it] 37%|███▋      | 111/300 [07:27<12:50,  4.08s/it] 37%|███▋      | 112/300 [07:31<12:44,  4.06s/it] 38%|███▊      | 113/300 [07:35<12:34,  4.04s/it] 38%|███▊      | 114/300 [07:39<12:26,  4.02s/it] 38%|███▊      | 115/300 [07:43<12:19,  3.99s/it] 39%|███▊      | 116/300 [07:47<12:18,  4.01s/it] 39%|███▉      | 117/300 [07:51<12:10,  3.99s/it] 39%|███▉      | 118/300 [07:55<12:05,  3.99s/it] 40%|███▉      | 119/300 [07:58<11:58,  3.97s/it] 40%|████      | 120/300 [08:02<11:55,  3.98s/it] 40%|████      | 121/300 [08:06<11:50,  3.97s/it] 41%|████      | 122/300 [08:10<11:44,  3.96s/it] 41%|████      | 123/300 [08:14<11:38,  3.95s/it] 41%|████▏     | 124/300 [08:18<11:33,  3.94s/it] 42%|████▏     | 125/300 [08:22<11:32,  3.96s/it] 42%|████▏     | 126/300 [08:26<11:25,  3.94s/it] 42%|████▏     | 127/300 [08:30<11:23,  3.95s/it] 43%|████▎     | 128/300 [08:34<11:20,  3.96s/it] 43%|████▎     | 129/300 [08:38<11:16,  3.95s/it] 43%|████▎     | 130/300 [08:42<11:09,  3.94s/it] 44%|████▎     | 131/300 [08:46<11:07,  3.95s/it] 44%|████▍     | 132/300 [08:50<11:03,  3.95s/it] 44%|████▍     | 133/300 [08:54<11:06,  3.99s/it] 45%|████▍     | 134/300 [08:58<10:57,  3.96s/it] 45%|████▌     | 135/300 [09:02<11:01,  4.01s/it] 45%|████▌     | 136/300 [09:06<10:54,  3.99s/it] 46%|████▌     | 137/300 [09:10<10:48,  3.98s/it] 46%|████▌     | 138/300 [09:14<10:42,  3.97s/it] 46%|████▋     | 139/300 [09:18<10:37,  3.96s/it] 47%|████▋     | 140/300 [09:22<10:34,  3.96s/it] 47%|████▋     | 141/300 [09:26<10:33,  3.98s/it] 47%|████▋     | 142/300 [09:30<10:29,  3.98s/it] 48%|████▊     | 143/300 [09:34<10:27,  3.99s/it] 48%|████▊     | 144/300 [09:38<10:21,  3.98s/it] 48%|████▊     | 145/300 [09:42<10:19,  4.00s/it] 49%|████▊     | 146/300 [09:46<10:17,  4.01s/it] 49%|████▉     | 147/300 [09:50<10:10,  3.99s/it] 49%|████▉     | 148/300 [09:54<10:12,  4.03s/it] 50%|████▉     | 149/300 [09:58<10:10,  4.04s/it] 50%|█████     | 150/300 [10:02<10:07,  4.05s/it] 50%|█████     | 151/300 [10:06<10:01,  4.04s/it] 51%|█████     | 152/300 [10:10<09:56,  4.03s/it] 51%|█████     | 153/300 [10:14<09:50,  4.02s/it] 51%|█████▏    | 154/300 [10:18<09:50,  4.04s/it] 52%|█████▏    | 155/300 [10:22<09:48,  4.06s/it] 52%|█████▏    | 156/300 [10:26<09:45,  4.07s/it] 52%|█████▏    | 157/300 [10:30<09:40,  4.06s/it] 53%|█████▎    | 158/300 [10:34<09:32,  4.03s/it] 53%|█████▎    | 159/300 [10:38<09:27,  4.03s/it] 53%|█████▎    | 160/300 [10:42<09:20,  4.00s/it] 54%|█████▎    | 161/300 [10:46<09:14,  3.99s/it] 54%|█████▍    | 162/300 [10:50<09:11,  4.00s/it] 54%|█████▍    | 163/300 [10:54<09:14,  4.05s/it] 55%|█████▍    | 164/300 [10:58<09:10,  4.05s/it] 55%|█████▌    | 165/300 [11:02<09:06,  4.05s/it] 55%|█████▌    | 166/300 [11:06<08:58,  4.02s/it] 56%|█████▌    | 167/300 [11:10<08:52,  4.00s/it] 56%|█████▌    | 168/300 [11:15<09:07,  4.15s/it] 56%|█████▋    | 169/300 [11:19<09:22,  4.30s/it] 57%|█████▋    | 170/300 [11:24<09:14,  4.26s/it] 57%|█████▋    | 171/300 [11:28<09:04,  4.22s/it] 57%|█████▋    | 172/300 [11:32<08:56,  4.19s/it] 58%|█████▊    | 173/300 [11:36<08:46,  4.15s/it] 58%|█████▊    | 174/300 [11:40<08:40,  4.13s/it] 58%|█████▊    | 175/300 [11:44<08:33,  4.10s/it] 59%|█████▊    | 176/300 [11:48<08:27,  4.09s/it] 59%|█████▉    | 177/300 [11:52<08:24,  4.10s/it] 59%|█████▉    | 178/300 [11:56<08:17,  4.08s/it] 60%|█████▉    | 179/300 [12:00<08:13,  4.08s/it] 60%|██████    | 180/300 [12:04<08:03,  4.03s/it] 60%|██████    | 181/300 [12:08<07:56,  4.01s/it] 61%|██████    | 182/300 [12:12<07:59,  4.07s/it] 61%|██████    | 183/300 [12:17<07:56,  4.07s/it] 61%|██████▏   | 184/300 [12:21<07:52,  4.08s/it] 62%|██████▏   | 185/300 [12:25<07:51,  4.10s/it] 62%|██████▏   | 186/300 [12:29<07:43,  4.06s/it] 62%|██████▏   | 187/300 [12:33<07:34,  4.02s/it] 63%|██████▎   | 188/300 [12:37<07:29,  4.01s/it] 63%|██████▎   | 189/300 [12:41<07:30,  4.06s/it] 63%|██████▎   | 190/300 [12:45<07:26,  4.06s/it] 64%|██████▎   | 191/300 [12:49<07:19,  4.03s/it] 64%|██████▍   | 192/300 [12:53<07:18,  4.06s/it] 64%|██████▍   | 193/300 [12:57<07:11,  4.04s/it] 65%|██████▍   | 194/300 [13:01<07:06,  4.03s/it] 65%|██████▌   | 195/300 [13:05<07:01,  4.02s/it] 65%|██████▌   | 196/300 [13:09<06:56,  4.00s/it] 66%|██████▌   | 197/300 [13:13<06:51,  3.99s/it] 66%|██████▌   | 198/300 [13:17<06:47,  3.99s/it] 66%|██████▋   | 199/300 [13:21<06:46,  4.02s/it] 67%|██████▋   | 200/300 [13:25<06:40,  4.00s/it] 67%|██████▋   | 201/300 [13:29<06:35,  4.00s/it] 67%|██████▋   | 202/300 [13:33<06:28,  3.97s/it] 68%|██████▊   | 203/300 [13:37<06:31,  4.04s/it] 68%|██████▊   | 204/300 [13:41<06:23,  4.00s/it] 68%|██████▊   | 205/300 [13:45<06:25,  4.06s/it] 69%|██████▊   | 206/300 [13:49<06:18,  4.03s/it] 69%|██████▉   | 207/300 [13:53<06:12,  4.01s/it] 69%|██████▉   | 208/300 [13:57<06:09,  4.02s/it] 70%|██████▉   | 209/300 [14:01<06:04,  4.01s/it] 70%|███████   | 210/300 [14:05<06:03,  4.04s/it] 70%|███████   | 211/300 [14:09<05:59,  4.04s/it] 71%|███████   | 212/300 [14:13<05:56,  4.05s/it] 71%|███████   | 213/300 [14:17<05:55,  4.09s/it] 71%|███████▏  | 214/300 [14:22<05:49,  4.07s/it] 72%|███████▏  | 215/300 [14:26<05:45,  4.07s/it] 72%|███████▏  | 216/300 [14:30<05:41,  4.07s/it] 72%|███████▏  | 217/300 [14:34<05:35,  4.05s/it] 73%|███████▎  | 218/300 [14:38<05:28,  4.01s/it] 73%|███████▎  | 219/300 [14:42<05:23,  4.00s/it] 73%|███████▎  | 220/300 [14:46<05:20,  4.01s/it] 74%|███████▎  | 221/300 [14:50<05:16,  4.00s/it] 74%|███████▍  | 222/300 [14:54<05:12,  4.01s/it] 74%|███████▍  | 223/300 [14:58<05:09,  4.02s/it] 75%|███████▍  | 224/300 [15:02<05:04,  4.00s/it] 75%|███████▌  | 225/300 [15:06<04:59,  3.99s/it] 75%|███████▌  | 226/300 [15:10<04:58,  4.03s/it] 76%|███████▌  | 227/300 [15:14<04:55,  4.05s/it] 76%|███████▌  | 228/300 [15:18<04:48,  4.01s/it] 76%|███████▋  | 229/300 [15:22<04:44,  4.01s/it] 77%|███████▋  | 230/300 [15:26<04:42,  4.03s/it] 77%|███████▋  | 231/300 [15:30<04:39,  4.05s/it] 77%|███████▋  | 232/300 [15:34<04:35,  4.05s/it] 78%|███████▊  | 233/300 [15:38<04:32,  4.07s/it] 78%|███████▊  | 234/300 [15:42<04:27,  4.05s/it] 78%|███████▊  | 235/300 [15:46<04:23,  4.06s/it] 79%|███████▊  | 236/300 [15:50<04:19,  4.06s/it] 79%|███████▉  | 237/300 [15:54<04:13,  4.03s/it] 79%|███████▉  | 238/300 [15:58<04:07,  4.00s/it] 80%|███████▉  | 239/300 [16:02<04:05,  4.03s/it] 80%|████████  | 240/300 [16:06<04:00,  4.01s/it] 80%|████████  | 241/300 [16:10<03:58,  4.04s/it] 81%|████████  | 242/300 [16:14<03:55,  4.06s/it] 81%|████████  | 243/300 [16:18<03:51,  4.05s/it] 81%|████████▏ | 244/300 [16:22<03:45,  4.02s/it] 82%|████████▏ | 245/300 [16:26<03:41,  4.03s/it] 82%|████████▏ | 246/300 [16:30<03:37,  4.03s/it] 82%|████████▏ | 247/300 [16:34<03:33,  4.04s/it] 83%|████████▎ | 248/300 [16:39<03:30,  4.04s/it] 83%|████████▎ | 249/300 [16:43<03:25,  4.03s/it] 83%|████████▎ | 250/300 [16:47<03:22,  4.05s/it] 84%|████████▎ | 251/300 [16:51<03:19,  4.08s/it] 84%|████████▍ | 252/300 [16:55<03:16,  4.09s/it] 84%|████████▍ | 253/300 [16:59<03:12,  4.09s/it] 85%|████████▍ | 254/300 [17:03<03:07,  4.07s/it] 85%|████████▌ | 255/300 [17:07<03:05,  4.12s/it] 85%|████████▌ | 256/300 [17:11<02:58,  4.07s/it] 86%|████████▌ | 257/300 [17:15<02:53,  4.05s/it] 86%|████████▌ | 258/300 [17:19<02:48,  4.00s/it] 86%|████████▋ | 259/300 [17:23<02:43,  3.98s/it] 87%|████████▋ | 260/300 [17:27<02:39,  3.98s/it] 87%|████████▋ | 261/300 [17:31<02:37,  4.03s/it] 87%|████████▋ | 262/300 [17:35<02:33,  4.05s/it] 88%|████████▊ | 263/300 [17:39<02:29,  4.04s/it] 88%|████████▊ | 264/300 [17:43<02:24,  4.02s/it] 88%|████████▊ | 265/300 [17:47<02:20,  4.01s/it] 89%|████████▊ | 266/300 [17:51<02:16,  4.00s/it] 89%|████████▉ | 267/300 [17:55<02:13,  4.05s/it] 89%|████████▉ | 268/300 [17:59<02:10,  4.06s/it] 90%|████████▉ | 269/300 [18:03<02:05,  4.04s/it] 90%|█████████ | 270/300 [18:07<02:01,  4.04s/it] 90%|█████████ | 271/300 [18:11<01:56,  4.03s/it] 91%|█████████ | 272/300 [18:15<01:52,  4.02s/it] 91%|█████████ | 273/300 [18:19<01:47,  4.00s/it] 91%|█████████▏| 274/300 [18:23<01:43,  3.99s/it] 92%|█████████▏| 275/300 [18:27<01:39,  3.98s/it] 92%|█████████▏| 276/300 [18:31<01:34,  3.96s/it] 92%|█████████▏| 277/300 [18:35<01:30,  3.95s/it] 93%|█████████▎| 278/300 [18:39<01:26,  3.95s/it] 93%|█████████▎| 279/300 [18:43<01:22,  3.95s/it] 93%|█████████▎| 280/300 [18:47<01:19,  3.98s/it] 94%|█████████▎| 281/300 [18:51<01:15,  3.97s/it] 94%|█████████▍| 282/300 [18:55<01:11,  3.96s/it] 94%|█████████▍| 283/300 [18:59<01:07,  3.96s/it] 95%|█████████▍| 284/300 [19:03<01:03,  3.97s/it] 95%|█████████▌| 285/300 [19:07<00:59,  3.98s/it] 95%|█████████▌| 286/300 [19:11<00:55,  3.96s/it] 96%|█████████▌| 287/300 [19:15<00:51,  3.94s/it] 96%|█████████▌| 288/300 [19:19<00:47,  3.95s/it] 96%|█████████▋| 289/300 [19:23<00:45,  4.10s/it] 97%|█████████▋| 290/300 [19:27<00:40,  4.09s/it] 97%|█████████▋| 291/300 [19:31<00:36,  4.05s/it] 97%|█████████▋| 292/300 [19:35<00:32,  4.01s/it] 98%|█████████▊| 293/300 [19:39<00:27,  3.99s/it] 98%|█████████▊| 294/300 [19:43<00:23,  4.00s/it] 98%|█████████▊| 295/300 [19:47<00:19,  3.98s/it] 99%|█████████▊| 296/300 [19:51<00:15,  3.95s/it] 99%|█████████▉| 297/300 [19:55<00:11,  3.95s/it] 99%|█████████▉| 298/300 [19:59<00:07,  3.94s/it]100%|█████████▉| 299/300 [20:03<00:03,  3.93s/it]100%|██████████| 300/300 [20:07<00:00,  3.96s/it]100%|██████████| 300/300 [20:07<00:00,  4.02s/it]
W&B disabled.
W&B online. Running your script from this directory will now sync to the cloud.
wandb: Currently logged in as: hl57. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.12 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.10
wandb: Run data is saved locally in /media/techt/One Touch/DD/SRe2L/train/wandb/run-20231023_100354-3ph2kybc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run valiant-glitter-476
wandb: ⭐️ View project at https://wandb.ai/hl57/final_rn18_fkd
wandb: 🚀 View run at https://wandb.ai/hl57/final_rn18_fkd/runs/3ph2kybc
======= FKD: dataset info ======
path: ../relabel/FKD_cutmix_fp16/imagenet/303/
num img: 10000
batch size: 100
max epoch: 300
================================
load data successfully
=> loading student model 'resnet18'
TRAIN Iter 0: lr = 0.001000,	loss = 0.005215,	Top-1 err = 99.730000,	Top-5 err = 98.710000,	train_time = 19.626024
