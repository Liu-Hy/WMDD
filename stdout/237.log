/home/techt/anaconda3/envs/hl/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/techt/anaconda3/envs/hl/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
/home/techt/anaconda3/envs/hl/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
r_bn:  10.0
lr:  0.1
bc shape torch.Size([200, 10, 512])
get_images call
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 3807.7820319157845
main criterion 131.8328131657845
weighted_aux_loss 3675.94921875
loss_r_bn_feature 367.59490966796875
------------iteration 100----------
total loss 1324.3130655123214
main criterion 63.62019441857136
weighted_aux_loss 1260.69287109375
loss_r_bn_feature 126.06929016113281
------------iteration 200----------
total loss 761.7116195947078
main criterion 46.40284273923902
weighted_aux_loss 715.3087768554688
loss_r_bn_feature 71.53087615966797
------------iteration 300----------
total loss 724.9214318856011
main criterion 55.736251221538616
weighted_aux_loss 669.1851806640625
loss_r_bn_feature 66.91851806640625
------------iteration 400----------
total loss 638.3757418996468
main criterion 46.36499971214681
weighted_aux_loss 592.0107421875
loss_r_bn_feature 59.201072692871094
------------iteration 500----------
total loss 557.8394296582901
main criterion 44.22846774422763
weighted_aux_loss 513.6109619140625
loss_r_bn_feature 51.3610954284668
------------iteration 600----------
total loss 612.6852774761331
main criterion 58.426732554258166
weighted_aux_loss 554.258544921875
loss_r_bn_feature 55.42585754394531
------------iteration 700----------
total loss 531.1190594464717
main criterion 46.653910520690474
weighted_aux_loss 484.46514892578125
loss_r_bn_feature 48.44651412963867
------------iteration 800----------
total loss 625.177503632478
main criterion 55.37751583950918
weighted_aux_loss 569.7999877929688
loss_r_bn_feature 56.97999954223633
------------iteration 900----------
total loss 454.4738177644133
main criterion 40.600282608163276
weighted_aux_loss 413.87353515625
loss_r_bn_feature 41.387351989746094
------------iteration 1000----------
total loss 441.8990640199584
main criterion 52.33848662738029
weighted_aux_loss 389.5605773925781
loss_r_bn_feature 38.956058502197266
------------iteration 1100----------
total loss 775.6540148942396
main criterion 69.32210571455208
weighted_aux_loss 706.3319091796875
loss_r_bn_feature 70.63319396972656
------------iteration 1200----------
total loss 395.7282226355266
main criterion 40.953259256620356
weighted_aux_loss 354.77496337890625
loss_r_bn_feature 35.47749710083008
------------iteration 1300----------
total loss 461.4977525388277
main criterion 47.48578964820266
weighted_aux_loss 414.011962890625
loss_r_bn_feature 41.40119552612305
------------iteration 1400----------
total loss 381.2897396311834
main criterion 39.78876306868343
weighted_aux_loss 341.5009765625
loss_r_bn_feature 34.15009689331055
------------iteration 1500----------
total loss 354.6849580814179
main criterion 37.17104206579288
weighted_aux_loss 317.513916015625
loss_r_bn_feature 31.75139045715332
------------iteration 1600----------
total loss 323.3340910734357
main criterion 37.05790699140441
weighted_aux_loss 286.27618408203125
loss_r_bn_feature 28.62761688232422
------------iteration 1700----------
total loss 295.72424827815195
main criterion 38.36194359065193
weighted_aux_loss 257.3623046875
loss_r_bn_feature 25.736230850219727
------------iteration 1800----------
total loss 425.8607187330958
main criterion 47.966431623720766
weighted_aux_loss 377.894287109375
loss_r_bn_feature 37.7894287109375
------------iteration 1900----------
total loss 399.0069109716754
main criterion 50.305555991206624
weighted_aux_loss 348.70135498046875
loss_r_bn_feature 34.87013626098633
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 4377.9151348433115
main criterion 133.2476543745619
weighted_aux_loss 4244.66748046875
loss_r_bn_feature 424.4667663574219
------------iteration 100----------
total loss 1550.6382052733406
main criterion 75.51613496084066
weighted_aux_loss 1475.1220703125
loss_r_bn_feature 147.51220703125
------------iteration 200----------
total loss 1551.002108643172
main criterion 85.41177661192214
weighted_aux_loss 1465.59033203125
loss_r_bn_feature 146.5590362548828
------------iteration 300----------
total loss 654.4123884159476
main criterion 46.65945872844754
weighted_aux_loss 607.7529296875
loss_r_bn_feature 60.77529525756836
------------iteration 400----------
total loss 1030.7519186763539
main criterion 65.07473361776015
weighted_aux_loss 965.6771850585938
loss_r_bn_feature 96.56771850585938
------------iteration 500----------
total loss 728.0995469042525
main criterion 49.77752541987753
weighted_aux_loss 678.322021484375
loss_r_bn_feature 67.83219909667969
------------iteration 600----------
total loss 524.5455610088902
main criterion 45.99450510068706
weighted_aux_loss 478.5510559082031
loss_r_bn_feature 47.855106353759766
------------iteration 700----------
total loss 520.4404690463924
main criterion 43.39179350928304
weighted_aux_loss 477.0486755371094
loss_r_bn_feature 47.70486831665039
------------iteration 800----------
total loss 514.6598067671534
main criterion 43.04939416949721
weighted_aux_loss 471.61041259765625
loss_r_bn_feature 47.161041259765625
------------iteration 900----------
total loss 524.1576683918902
main criterion 41.88343743485895
weighted_aux_loss 482.27423095703125
loss_r_bn_feature 48.22742462158203
------------iteration 1000----------
total loss 904.2725407640562
main criterion 70.52449388905626
weighted_aux_loss 833.748046875
loss_r_bn_feature 83.37480163574219
------------iteration 1100----------
total loss 812.0416184784153
main criterion 66.5148240448216
weighted_aux_loss 745.5267944335938
loss_r_bn_feature 74.55268096923828
------------iteration 1200----------
total loss 524.3545316470025
main criterion 47.439980865752496
weighted_aux_loss 476.91455078125
loss_r_bn_feature 47.69145584106445
------------iteration 1300----------
total loss 627.7933664623009
main criterion 53.800263434957124
weighted_aux_loss 573.9931030273438
loss_r_bn_feature 57.39931106567383
------------iteration 1400----------
total loss 397.93136573084576
main criterion 44.62057471522077
weighted_aux_loss 353.310791015625
loss_r_bn_feature 35.331077575683594
------------iteration 1500----------
total loss 434.53468285035297
main criterion 45.84407005738423
weighted_aux_loss 388.69061279296875
loss_r_bn_feature 38.86906051635742
------------iteration 1600----------
total loss 554.691457892846
main criterion 56.45357337136161
weighted_aux_loss 498.2378845214844
loss_r_bn_feature 49.823787689208984
------------iteration 1700----------
total loss 612.333449748557
main criterion 60.156997111838216
weighted_aux_loss 552.1764526367188
loss_r_bn_feature 55.21764373779297
------------iteration 1800----------
total loss 350.277462313285
main criterion 38.251034090628764
weighted_aux_loss 312.02642822265625
loss_r_bn_feature 31.20264434814453
------------iteration 1900----------
total loss 267.5835744372114
main criterion 34.671190404008236
weighted_aux_loss 232.91238403320312
loss_r_bn_feature 23.29123878479004
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 4181.75165372799
main criterion 136.23139005611498
weighted_aux_loss 4045.520263671875
loss_r_bn_feature 404.5520324707031
------------iteration 100----------
total loss 1524.0506424685739
main criterion 54.83872840607395
weighted_aux_loss 1469.2119140625
loss_r_bn_feature 146.9211883544922
------------iteration 200----------
total loss 852.2335698470532
main criterion 51.83696340174076
weighted_aux_loss 800.3966064453125
loss_r_bn_feature 80.03965759277344
------------iteration 300----------
total loss 749.12337259481
main criterion 53.05434183309121
weighted_aux_loss 696.0690307617188
loss_r_bn_feature 69.60690307617188
------------iteration 400----------
total loss 596.4178632123767
main criterion 48.299882255345494
weighted_aux_loss 548.1179809570312
loss_r_bn_feature 54.811798095703125
------------iteration 500----------
total loss 663.4961392324883
main criterion 49.99424714264455
weighted_aux_loss 613.5018920898438
loss_r_bn_feature 61.350189208984375
------------iteration 600----------
total loss 535.9353971026756
main criterion 47.05444617494113
weighted_aux_loss 488.8809509277344
loss_r_bn_feature 48.88809585571289
------------iteration 700----------
total loss 511.91606501951264
main criterion 46.823962968731415
weighted_aux_loss 465.09210205078125
loss_r_bn_feature 46.50920867919922
------------iteration 800----------
total loss 450.4568806905914
main criterion 43.82220661832577
weighted_aux_loss 406.6346740722656
loss_r_bn_feature 40.66346740722656
------------iteration 900----------
total loss 533.427810819944
main criterion 43.00117507775658
weighted_aux_loss 490.4266357421875
loss_r_bn_feature 49.04266357421875
------------iteration 1000----------
total loss 464.6964394526437
main criterion 49.15029687451868
weighted_aux_loss 415.546142578125
loss_r_bn_feature 41.55461502075195
------------iteration 1100----------
total loss 560.2573845120171
main criterion 59.97949144561081
weighted_aux_loss 500.27789306640625
loss_r_bn_feature 50.02779006958008
------------iteration 1200----------
total loss 353.0426615761846
main criterion 39.69470015040335
weighted_aux_loss 313.34796142578125
loss_r_bn_feature 31.334796905517578
------------iteration 1300----------
total loss 381.37298896300064
main criterion 45.824130320422526
weighted_aux_loss 335.5488586425781
loss_r_bn_feature 33.55488586425781
------------iteration 1400----------
total loss 309.1664003929068
main criterion 39.07750268782867
weighted_aux_loss 270.0888977050781
loss_r_bn_feature 27.00889015197754
------------iteration 1500----------
total loss 355.63516367278817
main criterion 52.97897470794444
weighted_aux_loss 302.65618896484375
loss_r_bn_feature 30.2656192779541
------------iteration 1600----------
total loss 289.41094367111924
main criterion 37.80518500412707
weighted_aux_loss 251.6057586669922
loss_r_bn_feature 25.16057586669922
------------iteration 1700----------
total loss 252.68508806456907
main criterion 37.191618826287815
weighted_aux_loss 215.49346923828125
loss_r_bn_feature 21.549346923828125
------------iteration 1800----------
total loss 252.60731427593666
main criterion 36.00634686871009
weighted_aux_loss 216.60096740722656
loss_r_bn_feature 21.660097122192383
------------iteration 1900----------
total loss 217.30945552048863
main criterion 33.495490676738626
weighted_aux_loss 183.81396484375
loss_r_bn_feature 18.381397247314453
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 4323.0383158192235
main criterion 138.80149941297344
weighted_aux_loss 4184.23681640625
loss_r_bn_feature 418.4236755371094
------------iteration 100----------
total loss 1671.0074757664477
main criterion 104.51943865707264
weighted_aux_loss 1566.488037109375
loss_r_bn_feature 156.6488037109375
------------iteration 200----------
total loss 1167.8488619984955
main criterion 55.030502623495536
weighted_aux_loss 1112.818359375
loss_r_bn_feature 111.2818374633789
------------iteration 300----------
total loss 715.0007070307458
main criterion 60.61105859324585
weighted_aux_loss 654.3896484375
loss_r_bn_feature 65.43896484375
------------iteration 400----------
total loss 833.8038341252326
main criterion 64.45446889085761
weighted_aux_loss 769.349365234375
loss_r_bn_feature 76.9349365234375
------------iteration 500----------
total loss 1011.8370431221435
main criterion 72.30451138386233
weighted_aux_loss 939.5325317382812
loss_r_bn_feature 93.95325469970703
------------iteration 600----------
total loss 584.7696834336376
main criterion 44.90927083598137
weighted_aux_loss 539.8604125976562
loss_r_bn_feature 53.98604202270508
------------iteration 700----------
total loss 490.99285298966834
main criterion 45.48562032365274
weighted_aux_loss 445.5072326660156
loss_r_bn_feature 44.550724029541016
------------iteration 800----------
total loss 498.20624069151023
main criterion 50.88717941221337
weighted_aux_loss 447.3190612792969
loss_r_bn_feature 44.73190689086914
------------iteration 900----------
total loss 397.854149817437
main criterion 41.53090762993699
weighted_aux_loss 356.3232421875
loss_r_bn_feature 35.63232421875
------------iteration 1000----------
total loss 466.1333354999953
main criterion 41.774387746089076
weighted_aux_loss 424.35894775390625
loss_r_bn_feature 42.43589401245117
------------iteration 1100----------
total loss 409.4211583074858
main criterion 43.637711041860854
weighted_aux_loss 365.783447265625
loss_r_bn_feature 36.578346252441406
------------iteration 1200----------
total loss 352.0622051005539
main criterion 40.938273215788314
weighted_aux_loss 311.1239318847656
loss_r_bn_feature 31.11239242553711
------------iteration 1300----------
total loss 1616.1994366074196
main criterion 93.55392879491949
weighted_aux_loss 1522.6455078125
loss_r_bn_feature 152.26455688476562
------------iteration 1400----------
total loss 380.41724704418215
main criterion 48.90830539379151
weighted_aux_loss 331.5089416503906
loss_r_bn_feature 33.15089416503906
------------iteration 1500----------
total loss 294.26630777711233
main criterion 38.28112406129204
weighted_aux_loss 255.9851837158203
loss_r_bn_feature 25.59851837158203
------------iteration 1600----------
total loss 287.4916778081578
main criterion 37.09153132378283
weighted_aux_loss 250.400146484375
loss_r_bn_feature 25.040014266967773
------------iteration 1700----------
total loss 383.4865813239654
main criterion 48.85782766185605
weighted_aux_loss 334.6287536621094
loss_r_bn_feature 33.46287536621094
------------iteration 1800----------
total loss 257.3453329942922
main criterion 35.08156956655782
weighted_aux_loss 222.26376342773438
loss_r_bn_feature 22.226375579833984
------------iteration 1900----------
total loss 271.53326787648655
main criterion 36.91836919484593
weighted_aux_loss 234.61489868164062
loss_r_bn_feature 23.461490631103516
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 4325.955249163917
main criterion 131.85075697641693
weighted_aux_loss 4194.1044921875
loss_r_bn_feature 419.41046142578125
------------iteration 100----------
total loss 1432.293740869031
main criterion 56.120523095593335
weighted_aux_loss 1376.1732177734375
loss_r_bn_feature 137.61732482910156
------------iteration 200----------
total loss 1876.3164113871226
main criterion 72.30591334024754
weighted_aux_loss 1804.010498046875
loss_r_bn_feature 180.4010467529297
------------iteration 300----------
total loss 822.5321008871492
main criterion 49.18200323089923
weighted_aux_loss 773.35009765625
loss_r_bn_feature 77.33500671386719
------------iteration 400----------
total loss 904.4295839760301
main criterion 77.33815331196763
weighted_aux_loss 827.0914306640625
loss_r_bn_feature 82.70914459228516
------------iteration 500----------
total loss 536.0748601332185
main criterion 51.166748560952854
weighted_aux_loss 484.9081115722656
loss_r_bn_feature 48.49081039428711
------------iteration 600----------
total loss 519.6064135147772
main criterion 48.725401551886485
weighted_aux_loss 470.8810119628906
loss_r_bn_feature 47.08810043334961
------------iteration 700----------
total loss 1239.0129462967773
main criterion 84.66541211708987
weighted_aux_loss 1154.3475341796875
loss_r_bn_feature 115.43475341796875
------------iteration 800----------
total loss 1010.677672578538
main criterion 89.5363761918193
weighted_aux_loss 921.1412963867188
loss_r_bn_feature 92.11412811279297
------------iteration 900----------
total loss 433.40212593988804
main criterion 40.39449654535681
weighted_aux_loss 393.00762939453125
loss_r_bn_feature 39.30076217651367
------------iteration 1000----------
total loss 503.11559854170116
main criterion 44.26333413740429
weighted_aux_loss 458.8522644042969
loss_r_bn_feature 45.88522720336914
------------iteration 1100----------
total loss 402.491074916254
main criterion 41.42747628344147
weighted_aux_loss 361.0635986328125
loss_r_bn_feature 36.106361389160156
------------iteration 1200----------
total loss 352.87170662205585
main criterion 38.633791582993375
weighted_aux_loss 314.2379150390625
loss_r_bn_feature 31.423789978027344
------------iteration 1300----------
total loss 449.36985961470293
main criterion 48.8930224564998
weighted_aux_loss 400.4768371582031
loss_r_bn_feature 40.04768371582031
------------iteration 1400----------
total loss 302.17034327777844
main criterion 39.43935572895031
weighted_aux_loss 262.7309875488281
loss_r_bn_feature 26.27309799194336
------------iteration 1500----------
total loss 327.649299149651
main criterion 37.02741194261974
weighted_aux_loss 290.62188720703125
loss_r_bn_feature 29.06218719482422
------------iteration 1600----------
total loss 369.07779909422504
main criterion 39.17450929930315
weighted_aux_loss 329.9032897949219
loss_r_bn_feature 32.99032974243164
------------iteration 1700----------
total loss 288.94936775464276
main criterion 39.86303352612716
weighted_aux_loss 249.08633422851562
loss_r_bn_feature 24.908634185791016
------------iteration 1800----------
total loss 296.6172300173691
main criterion 36.74241312283781
weighted_aux_loss 259.87481689453125
loss_r_bn_feature 25.98748207092285
------------iteration 1900----------
total loss 364.99678125803496
main criterion 44.0556496662381
weighted_aux_loss 320.9411315917969
loss_r_bn_feature 32.094112396240234
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 3730.620307819449
main criterion 135.339057819449
weighted_aux_loss 3595.28125
loss_r_bn_feature 359.52813720703125
------------iteration 100----------
total loss 1344.2687554181814
main criterion 51.89937065255647
weighted_aux_loss 1292.369384765625
loss_r_bn_feature 129.2369384765625
------------iteration 200----------
total loss 783.0890944429332
main criterion 59.38737325152691
weighted_aux_loss 723.7017211914062
loss_r_bn_feature 72.37017059326172
------------iteration 300----------
total loss 645.7862812215061
main criterion 56.61709176838111
weighted_aux_loss 589.169189453125
loss_r_bn_feature 58.91691589355469
------------iteration 400----------
total loss 572.8524646527903
main criterion 54.38920781685287
weighted_aux_loss 518.4632568359375
loss_r_bn_feature 51.84632873535156
------------iteration 500----------
total loss 662.9476809861209
main criterion 50.06096223612086
weighted_aux_loss 612.88671875
loss_r_bn_feature 61.288673400878906
------------iteration 600----------
total loss 496.9855687336424
main criterion 44.385776253173695
weighted_aux_loss 452.59979248046875
loss_r_bn_feature 45.259979248046875
------------iteration 700----------
total loss 548.487049202167
main criterion 48.385425667010765
weighted_aux_loss 500.10162353515625
loss_r_bn_feature 50.010162353515625
------------iteration 800----------
total loss 458.66319827384655
main criterion 45.5166833812684
weighted_aux_loss 413.1465148925781
loss_r_bn_feature 41.31465148925781
------------iteration 900----------
total loss 422.41947877763613
main criterion 40.86335695146427
weighted_aux_loss 381.5561218261719
loss_r_bn_feature 38.15561294555664
------------iteration 1000----------
total loss 445.5668979588763
main criterion 40.153323740126325
weighted_aux_loss 405.41357421875
loss_r_bn_feature 40.541358947753906
------------iteration 1100----------
total loss 341.5320587997388
main criterion 40.01100167083257
weighted_aux_loss 301.52105712890625
loss_r_bn_feature 30.1521053314209
------------iteration 1200----------
total loss 387.3504915636483
main criterion 42.91415123161704
weighted_aux_loss 344.43634033203125
loss_r_bn_feature 34.443634033203125
------------iteration 1300----------
total loss 354.3067276827618
main criterion 44.08700112026181
weighted_aux_loss 310.2197265625
loss_r_bn_feature 31.02197265625
------------iteration 1400----------
total loss 330.93936220888565
main criterion 36.45687929872941
weighted_aux_loss 294.48248291015625
loss_r_bn_feature 29.44824981689453
------------iteration 1500----------
total loss 303.12005522105466
main criterion 36.02703764292966
weighted_aux_loss 267.093017578125
loss_r_bn_feature 26.709300994873047
------------iteration 1600----------
total loss 297.73728777066646
main criterion 36.938947926916455
weighted_aux_loss 260.79833984375
loss_r_bn_feature 26.079833984375
------------iteration 1700----------
total loss 1029.0918570815613
main criterion 82.80425942531134
weighted_aux_loss 946.28759765625
loss_r_bn_feature 94.6287612915039
------------iteration 1800----------
total loss 308.9006532374857
main criterion 35.75679337420445
weighted_aux_loss 273.14385986328125
loss_r_bn_feature 27.31438636779785
------------iteration 1900----------
total loss 414.60273942363665
main criterion 49.69600114238662
weighted_aux_loss 364.90673828125
loss_r_bn_feature 36.49067306518555
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 4306.097904798165
main criterion 129.98511182941485
weighted_aux_loss 4176.11279296875
loss_r_bn_feature 417.6112976074219
------------iteration 100----------
total loss 1693.3475190208912
main criterion 57.980697731828826
weighted_aux_loss 1635.3668212890625
loss_r_bn_feature 163.53668212890625
------------iteration 200----------
total loss 943.1249393168565
main criterion 61.640320176231505
weighted_aux_loss 881.484619140625
loss_r_bn_feature 88.1484603881836
------------iteration 300----------
total loss 673.6183446133157
main criterion 48.10760242581564
weighted_aux_loss 625.5107421875
loss_r_bn_feature 62.55107498168945
------------iteration 400----------
total loss 717.4626392937153
main criterion 47.792717418715206
weighted_aux_loss 669.669921875
loss_r_bn_feature 66.96699523925781
------------iteration 500----------
total loss 542.2970617008582
main criterion 46.58606316570194
weighted_aux_loss 495.71099853515625
loss_r_bn_feature 49.57109832763672
------------iteration 600----------
total loss 583.8042272847628
main criterion 50.884793691012746
weighted_aux_loss 532.91943359375
loss_r_bn_feature 53.29194641113281
------------iteration 700----------
total loss 508.00307311133935
main criterion 46.584432974620576
weighted_aux_loss 461.41864013671875
loss_r_bn_feature 46.14186477661133
------------iteration 800----------
total loss 581.3529554833182
main criterion 43.22710099113072
weighted_aux_loss 538.1258544921875
loss_r_bn_feature 53.81258773803711
------------iteration 900----------
total loss 498.1399048315836
main criterion 42.89326176517732
weighted_aux_loss 455.24664306640625
loss_r_bn_feature 45.52466583251953
------------iteration 1000----------
total loss 437.5027529810081
main criterion 45.48920317632057
weighted_aux_loss 392.0135498046875
loss_r_bn_feature 39.20135498046875
------------iteration 1100----------
total loss 557.9133367907888
main criterion 55.52930358766381
weighted_aux_loss 502.384033203125
loss_r_bn_feature 50.2384033203125
------------iteration 1200----------
total loss 374.56394135632036
main criterion 38.17172944225784
weighted_aux_loss 336.3922119140625
loss_r_bn_feature 33.63922119140625
------------iteration 1300----------
total loss 339.0291418970017
main criterion 36.42391728762667
weighted_aux_loss 302.605224609375
loss_r_bn_feature 30.260520935058594
------------iteration 1400----------
total loss 531.2453705902395
main criterion 48.46814891055208
weighted_aux_loss 482.7772216796875
loss_r_bn_feature 48.2777214050293
------------iteration 1500----------
total loss 626.3911550871384
main criterion 60.40696319260708
weighted_aux_loss 565.9841918945312
loss_r_bn_feature 56.598419189453125
------------iteration 1600----------
total loss 976.2127063366704
main criterion 67.69000125854535
weighted_aux_loss 908.522705078125
loss_r_bn_feature 90.8522720336914
------------iteration 1700----------
total loss 333.30589872075336
main criterion 37.672201210987716
weighted_aux_loss 295.6336975097656
loss_r_bn_feature 29.563369750976562
------------iteration 1800----------
total loss 291.35295689388124
main criterion 37.7492886809906
weighted_aux_loss 253.60366821289062
loss_r_bn_feature 25.360366821289062
------------iteration 1900----------
total loss 303.57664381905977
main criterion 38.96250807687228
weighted_aux_loss 264.6141357421875
loss_r_bn_feature 26.46141242980957
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 4525.152022397722
main criterion 138.8263388039727
weighted_aux_loss 4386.32568359375
loss_r_bn_feature 438.632568359375
------------iteration 100----------
total loss 1686.9169761901078
main criterion 55.22874376823268
weighted_aux_loss 1631.688232421875
loss_r_bn_feature 163.1688232421875
------------iteration 200----------
total loss 982.512310057216
main criterion 51.31327441268484
weighted_aux_loss 931.1990356445312
loss_r_bn_feature 93.11990356445312
------------iteration 300----------
total loss 848.0866003075279
main criterion 58.73790645987165
weighted_aux_loss 789.3486938476562
loss_r_bn_feature 78.93486785888672
------------iteration 400----------
total loss 677.6974626770142
main criterion 51.5500627746704
weighted_aux_loss 626.1473999023438
loss_r_bn_feature 62.614742279052734
------------iteration 500----------
total loss 684.4971033117172
main criterion 53.66525516718588
weighted_aux_loss 630.8318481445312
loss_r_bn_feature 63.08318328857422
------------iteration 600----------
total loss 1161.8885125502418
main criterion 81.08260434711671
weighted_aux_loss 1080.805908203125
loss_r_bn_feature 108.08059692382812
------------iteration 700----------
total loss 511.96032553681545
main criterion 46.99072104462797
weighted_aux_loss 464.9696044921875
loss_r_bn_feature 46.4969596862793
------------iteration 800----------
total loss 1118.4616100960764
main criterion 79.66204954920143
weighted_aux_loss 1038.799560546875
loss_r_bn_feature 103.87995147705078
------------iteration 900----------
total loss 578.9635181998075
main criterion 50.576433238869974
weighted_aux_loss 528.3870849609375
loss_r_bn_feature 52.83871078491211
------------iteration 1000----------
total loss 494.64455785264454
main criterion 42.882503409285185
weighted_aux_loss 451.7620544433594
loss_r_bn_feature 45.176204681396484
------------iteration 1100----------
total loss 439.85954311927367
main criterion 47.40333584388301
weighted_aux_loss 392.4562072753906
loss_r_bn_feature 39.24562072753906
------------iteration 1200----------
total loss 530.9043103376439
main criterion 54.12162601147203
weighted_aux_loss 476.7826843261719
loss_r_bn_feature 47.67826843261719
------------iteration 1300----------
total loss 379.80476961054615
main criterion 40.996725176952424
weighted_aux_loss 338.80804443359375
loss_r_bn_feature 33.88080596923828
------------iteration 1400----------
total loss 577.012260384598
main criterion 61.80773157600427
weighted_aux_loss 515.2045288085938
loss_r_bn_feature 51.520450592041016
------------iteration 1500----------
total loss 552.3754441449069
main criterion 58.74568340271932
weighted_aux_loss 493.6297607421875
loss_r_bn_feature 49.36297607421875
------------iteration 1600----------
total loss 467.4613372190805
main criterion 53.01431573470555
weighted_aux_loss 414.447021484375
loss_r_bn_feature 41.4447021484375
------------iteration 1700----------
total loss 904.7687388088707
main criterion 72.00860697293325
weighted_aux_loss 832.7601318359375
loss_r_bn_feature 83.27601623535156
------------iteration 1800----------
total loss 293.0019697410606
main criterion 36.85832350082621
weighted_aux_loss 256.1436462402344
loss_r_bn_feature 25.614364624023438
------------iteration 1900----------
total loss 278.7400214924765
main criterion 40.39256360673431
weighted_aux_loss 238.3474578857422
loss_r_bn_feature 23.834745407104492
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 4578.779313596574
main criterion 131.82325890907376
weighted_aux_loss 4446.9560546875
loss_r_bn_feature 444.69561767578125
------------iteration 100----------
total loss 1662.2661010780857
main criterion 75.97801514058571
weighted_aux_loss 1586.2880859375
loss_r_bn_feature 158.62881469726562
------------iteration 200----------
total loss 853.2517793317454
main criterion 53.8113496442454
weighted_aux_loss 799.4404296875
loss_r_bn_feature 79.94404602050781
------------iteration 300----------
total loss 760.3872309363836
main criterion 49.62984568247737
weighted_aux_loss 710.7573852539062
loss_r_bn_feature 71.07573699951172
------------iteration 400----------
total loss 1115.6155810707005
main criterion 78.81931642226301
weighted_aux_loss 1036.7962646484375
loss_r_bn_feature 103.67962646484375
------------iteration 500----------
total loss 704.0806293025578
main criterion 53.81097598224532
weighted_aux_loss 650.2696533203125
loss_r_bn_feature 65.02696228027344
------------iteration 600----------
total loss 1060.3269242394358
main criterion 72.20747843865449
weighted_aux_loss 988.1194458007812
loss_r_bn_feature 98.81194305419922
------------iteration 700----------
total loss 744.3608969228751
main criterion 60.56426606350008
weighted_aux_loss 683.796630859375
loss_r_bn_feature 68.3796615600586
------------iteration 800----------
total loss 750.1268509134374
main criterion 68.08107454624987
weighted_aux_loss 682.0457763671875
loss_r_bn_feature 68.20457458496094
------------iteration 900----------
total loss 526.0803547197493
main criterion 51.45053294240549
weighted_aux_loss 474.62982177734375
loss_r_bn_feature 47.462982177734375
------------iteration 1000----------
total loss 497.79721445000206
main criterion 49.397617282033316
weighted_aux_loss 448.39959716796875
loss_r_bn_feature 44.83995819091797
------------iteration 1100----------
total loss 487.0765086710005
main criterion 53.118683963969254
weighted_aux_loss 433.95782470703125
loss_r_bn_feature 43.395782470703125
------------iteration 1200----------
total loss 389.27790756282013
main criterion 44.921462250320104
weighted_aux_loss 344.3564453125
loss_r_bn_feature 34.435646057128906
------------iteration 1300----------
total loss 372.76236286068803
main criterion 42.75290241146927
weighted_aux_loss 330.00946044921875
loss_r_bn_feature 33.000946044921875
------------iteration 1400----------
total loss 314.7751133592516
main criterion 37.256467119017216
weighted_aux_loss 277.5186462402344
loss_r_bn_feature 27.751863479614258
------------iteration 1500----------
total loss 637.047966269103
main criterion 64.4570238862905
weighted_aux_loss 572.5909423828125
loss_r_bn_feature 57.25909423828125
------------iteration 1600----------
total loss 294.04683734531187
main criterion 36.60439349765561
weighted_aux_loss 257.44244384765625
loss_r_bn_feature 25.744245529174805
------------iteration 1700----------
total loss 352.8332150633509
main criterion 43.19341403796026
weighted_aux_loss 309.6398010253906
loss_r_bn_feature 30.963979721069336
------------iteration 1800----------
total loss 873.6009508887308
main criterion 66.39880245123084
weighted_aux_loss 807.2021484375
loss_r_bn_feature 80.72021484375
------------iteration 1900----------
total loss 274.0672612048594
main criterion 36.68034409060157
weighted_aux_loss 237.3869171142578
loss_r_bn_feature 23.738691329956055
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 4442.697678459065
main criterion 133.3695534590648
weighted_aux_loss 4309.328125
loss_r_bn_feature 430.93280029296875
------------iteration 100----------
total loss 1813.2769162080535
main criterion 58.12384003617854
weighted_aux_loss 1755.153076171875
loss_r_bn_feature 175.5153045654297
------------iteration 200----------
total loss 1379.980215965272
main criterion 68.77062123870955
weighted_aux_loss 1311.2095947265625
loss_r_bn_feature 131.12095642089844
------------iteration 300----------
total loss 1667.3116518581623
main criterion 100.09876123316229
weighted_aux_loss 1567.212890625
loss_r_bn_feature 156.72128295898438
------------iteration 400----------
total loss 941.4216336674763
main criterion 61.51904577685123
weighted_aux_loss 879.902587890625
loss_r_bn_feature 87.9902572631836
------------iteration 500----------
total loss 980.684062161192
main criterion 66.43876186822318
weighted_aux_loss 914.2453002929688
loss_r_bn_feature 91.42453002929688
------------iteration 600----------
total loss 594.7087330108462
main criterion 46.19670908506493
weighted_aux_loss 548.5120239257812
loss_r_bn_feature 54.85120391845703
------------iteration 700----------
total loss 579.4919685018629
main criterion 51.85335766201911
weighted_aux_loss 527.6386108398438
loss_r_bn_feature 52.763858795166016
------------iteration 800----------
total loss 501.81334182746195
main criterion 42.08693191535257
weighted_aux_loss 459.7264099121094
loss_r_bn_feature 45.97264099121094
------------iteration 900----------
total loss 487.46390248984795
main criterion 43.142277733988564
weighted_aux_loss 444.3216247558594
loss_r_bn_feature 44.43216323852539
------------iteration 1000----------
total loss 692.5967952255991
main criterion 59.78295245216153
weighted_aux_loss 632.8138427734375
loss_r_bn_feature 63.28138732910156
------------iteration 1100----------
total loss 405.91252533431776
main criterion 40.205402531583374
weighted_aux_loss 365.7071228027344
loss_r_bn_feature 36.57071304321289
------------iteration 1200----------
total loss 578.0248771788588
main criterion 57.71304856557758
weighted_aux_loss 520.3118286132812
loss_r_bn_feature 52.031185150146484
------------iteration 1300----------
total loss 1020.2043239266403
main criterion 81.54148212976531
weighted_aux_loss 938.662841796875
loss_r_bn_feature 93.86628723144531
------------iteration 1400----------
total loss 742.2239170187501
main criterion 63.806802760937586
weighted_aux_loss 678.4171142578125
loss_r_bn_feature 67.84171295166016
------------iteration 1500----------
total loss 423.8559062164902
main criterion 49.59583541570894
weighted_aux_loss 374.26007080078125
loss_r_bn_feature 37.42600631713867
------------iteration 1600----------
total loss 624.6649547506313
main criterion 57.59122428188133
weighted_aux_loss 567.07373046875
loss_r_bn_feature 56.70737075805664
------------iteration 1700----------
total loss 276.8656977787633
main criterion 35.62622634321643
weighted_aux_loss 241.23947143554688
loss_r_bn_feature 24.123947143554688
------------iteration 1800----------
total loss 292.1527939576227
main criterion 36.5235977906305
weighted_aux_loss 255.6291961669922
loss_r_bn_feature 25.56291961669922
------------iteration 1900----------
total loss 287.0916922449911
main criterion 35.88336399792082
weighted_aux_loss 251.2083282470703
loss_r_bn_feature 25.120832443237305
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 4652.560824879572
main criterion 130.48611784832178
weighted_aux_loss 4522.07470703125
loss_r_bn_feature 452.2074890136719
------------iteration 100----------
total loss 1634.1246912644704
main criterion 56.62212778790776
weighted_aux_loss 1577.5025634765625
loss_r_bn_feature 157.75025939941406
------------iteration 200----------
total loss 1069.143967540961
main criterion 80.80510035346101
weighted_aux_loss 988.3388671875
loss_r_bn_feature 98.8338851928711
------------iteration 300----------
total loss 681.3104024976964
main criterion 50.45810757582139
weighted_aux_loss 630.852294921875
loss_r_bn_feature 63.085227966308594
------------iteration 400----------
total loss 929.5959225123274
main criterion 59.6936397974837
weighted_aux_loss 869.9022827148438
loss_r_bn_feature 86.99022674560547
------------iteration 500----------
total loss 1056.9741340564385
main criterion 75.92640456425107
weighted_aux_loss 981.0477294921875
loss_r_bn_feature 98.10477447509766
------------iteration 600----------
total loss 799.7650331186437
main criterion 61.461505286612514
weighted_aux_loss 738.3035278320312
loss_r_bn_feature 73.83035278320312
------------iteration 700----------
total loss 628.4907857177046
main criterion 48.38617145989212
weighted_aux_loss 580.1046142578125
loss_r_bn_feature 58.010459899902344
------------iteration 800----------
total loss 618.8918584194142
main criterion 48.07880910300798
weighted_aux_loss 570.8130493164062
loss_r_bn_feature 57.08130645751953
------------iteration 900----------
total loss 648.3864674484686
main criterion 63.78759049534361
weighted_aux_loss 584.598876953125
loss_r_bn_feature 58.45988845825195
------------iteration 1000----------
total loss 422.87711189080784
main criterion 44.35257575799535
weighted_aux_loss 378.5245361328125
loss_r_bn_feature 37.852455139160156
------------iteration 1100----------
total loss 383.08883131857056
main criterion 39.6771186720862
weighted_aux_loss 343.4117126464844
loss_r_bn_feature 34.34117126464844
------------iteration 1200----------
total loss 413.83832842874295
main criterion 43.01542193460233
weighted_aux_loss 370.8229064941406
loss_r_bn_feature 37.08229064941406
------------iteration 1300----------
total loss 423.10927123316884
main criterion 42.69118041285632
weighted_aux_loss 380.4180908203125
loss_r_bn_feature 38.04180908203125
------------iteration 1400----------
total loss 395.14515087631867
main criterion 46.56119701889678
weighted_aux_loss 348.5839538574219
loss_r_bn_feature 34.858394622802734
------------iteration 1500----------
total loss 312.11115807091204
main criterion 37.61305016075578
weighted_aux_loss 274.49810791015625
loss_r_bn_feature 27.449810028076172
------------iteration 1600----------
total loss 290.5251796625073
main criterion 36.87378195742918
weighted_aux_loss 253.65139770507812
loss_r_bn_feature 25.36513900756836
------------iteration 1700----------
total loss 784.7948130155432
main criterion 67.4806650663245
weighted_aux_loss 717.3141479492188
loss_r_bn_feature 71.73141479492188
------------iteration 1800----------
total loss 771.8958695708826
main criterion 70.0814164458826
weighted_aux_loss 701.814453125
loss_r_bn_feature 70.18144226074219
------------iteration 1900----------
total loss 361.63528859007755
main criterion 44.7646220861713
weighted_aux_loss 316.87066650390625
loss_r_bn_feature 31.68706703186035
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 4476.107683115814
main criterion 136.85280030331432
weighted_aux_loss 4339.2548828125
loss_r_bn_feature 433.92547607421875
------------iteration 100----------
total loss 1810.8593676247526
main criterion 58.78759028100273
weighted_aux_loss 1752.07177734375
loss_r_bn_feature 175.20718383789062
------------iteration 200----------
total loss 1107.4518041882795
main criterion 69.05336668827944
weighted_aux_loss 1038.3984375
loss_r_bn_feature 103.83984375
------------iteration 300----------
total loss 756.6783702249786
main criterion 50.65102647497856
weighted_aux_loss 706.02734375
loss_r_bn_feature 70.60273742675781
------------iteration 400----------
total loss 992.3461028099553
main criterion 77.0085783958927
weighted_aux_loss 915.3375244140625
loss_r_bn_feature 91.53375244140625
------------iteration 500----------
total loss 2114.3560097607888
main criterion 105.22820214360125
weighted_aux_loss 2009.1278076171875
loss_r_bn_feature 200.91278076171875
------------iteration 600----------
total loss 600.5636531648087
main criterion 48.98363607496495
weighted_aux_loss 551.5800170898438
loss_r_bn_feature 55.15800094604492
------------iteration 700----------
total loss 681.263378834931
main criterion 52.45454094430607
weighted_aux_loss 628.808837890625
loss_r_bn_feature 62.880882263183594
------------iteration 800----------
total loss 524.9125178581511
main criterion 42.11796219408864
weighted_aux_loss 482.7945556640625
loss_r_bn_feature 48.279457092285156
------------iteration 900----------
total loss 524.6936208272892
main criterion 44.568742897601695
weighted_aux_loss 480.1248779296875
loss_r_bn_feature 48.012489318847656
------------iteration 1000----------
total loss 470.58877616814925
main criterion 40.793976363461745
weighted_aux_loss 429.7947998046875
loss_r_bn_feature 42.9794807434082
------------iteration 1100----------
total loss 445.92174078532537
main criterion 41.179858461106626
weighted_aux_loss 404.74188232421875
loss_r_bn_feature 40.47418975830078
------------iteration 1200----------
total loss 392.2465230306347
main criterion 42.67553914391594
weighted_aux_loss 349.57098388671875
loss_r_bn_feature 34.95709991455078
------------iteration 1300----------
total loss 499.29923697704
main criterion 46.03037711375876
weighted_aux_loss 453.26885986328125
loss_r_bn_feature 45.32688522338867
------------iteration 1400----------
total loss 368.45549173697685
main criterion 40.59947367057061
weighted_aux_loss 327.85601806640625
loss_r_bn_feature 32.78560256958008
------------iteration 1500----------
total loss 378.75981781600456
main criterion 40.79552338241083
weighted_aux_loss 337.96429443359375
loss_r_bn_feature 33.79642868041992
------------iteration 1600----------
total loss 347.3727262300982
main criterion 38.73692300744193
weighted_aux_loss 308.63580322265625
loss_r_bn_feature 30.86358070373535
------------iteration 1700----------
total loss 320.08369091966847
main criterion 38.58369091966849
weighted_aux_loss 281.5
loss_r_bn_feature 28.150001525878906
------------iteration 1800----------
total loss 377.64137798767183
main criterion 48.90114361267182
weighted_aux_loss 328.740234375
loss_r_bn_feature 32.8740234375
------------iteration 1900----------
total loss 520.9098126595532
main criterion 57.475639075568814
weighted_aux_loss 463.4341735839844
loss_r_bn_feature 46.34341812133789
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 4597.059818326136
main criterion 127.39624410738674
weighted_aux_loss 4469.66357421875
loss_r_bn_feature 446.96636962890625
------------iteration 100----------
total loss 1648.4671527892397
main criterion 62.12987251580213
weighted_aux_loss 1586.3372802734375
loss_r_bn_feature 158.63372802734375
------------iteration 200----------
total loss 1534.9543655098132
main criterion 78.66762234575074
weighted_aux_loss 1456.2867431640625
loss_r_bn_feature 145.62867736816406
------------iteration 300----------
total loss 838.1191857399286
main criterion 53.12065058367862
weighted_aux_loss 784.99853515625
loss_r_bn_feature 78.4998550415039
------------iteration 400----------
total loss 699.9812755703484
main criterion 47.67512322659845
weighted_aux_loss 652.30615234375
loss_r_bn_feature 65.2306137084961
------------iteration 500----------
total loss 563.9113268170969
main criterion 49.31220572334685
weighted_aux_loss 514.59912109375
loss_r_bn_feature 51.45991134643555
------------iteration 600----------
total loss 576.000543863318
main criterion 42.960993082067915
weighted_aux_loss 533.03955078125
loss_r_bn_feature 53.303955078125
------------iteration 700----------
total loss 554.6929067600784
main criterion 49.432439230781455
weighted_aux_loss 505.2604675292969
loss_r_bn_feature 50.52604675292969
------------iteration 800----------
total loss 507.08162898927844
main criterion 42.58602352052844
weighted_aux_loss 464.49560546875
loss_r_bn_feature 46.449562072753906
------------iteration 900----------
total loss 440.67518844705745
main criterion 41.41862716776058
weighted_aux_loss 399.2565612792969
loss_r_bn_feature 39.925655364990234
------------iteration 1000----------
total loss 1326.5232832858437
main criterion 83.22860555146872
weighted_aux_loss 1243.294677734375
loss_r_bn_feature 124.3294677734375
------------iteration 1100----------
total loss 426.3756810326045
main criterion 39.411691774792004
weighted_aux_loss 386.9639892578125
loss_r_bn_feature 38.6963996887207
------------iteration 1200----------
total loss 535.2792359958704
main criterion 50.1568605075892
weighted_aux_loss 485.12237548828125
loss_r_bn_feature 48.512237548828125
------------iteration 1300----------
total loss 812.5466048880974
main criterion 66.79776455606614
weighted_aux_loss 745.7488403320312
loss_r_bn_feature 74.57488250732422
------------iteration 1400----------
total loss 468.6209313925205
main criterion 50.95006347259865
weighted_aux_loss 417.6708679199219
loss_r_bn_feature 41.767086029052734
------------iteration 1500----------
total loss 531.1742088798283
main criterion 55.011946916937596
weighted_aux_loss 476.1622619628906
loss_r_bn_feature 47.61622619628906
------------iteration 1600----------
total loss 391.7566438056113
main criterion 45.55315259467378
weighted_aux_loss 346.2034912109375
loss_r_bn_feature 34.6203498840332
------------iteration 1700----------
total loss 423.70981294672094
main criterion 48.235295124455334
weighted_aux_loss 375.4745178222656
loss_r_bn_feature 37.54745101928711
------------iteration 1800----------
total loss 369.26495835072586
main criterion 40.223179786272716
weighted_aux_loss 329.0417785644531
loss_r_bn_feature 32.904178619384766
------------iteration 1900----------
total loss 333.0115396999193
main criterion 40.202152492888054
weighted_aux_loss 292.80938720703125
loss_r_bn_feature 29.28093910217285
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 4967.151029716135
main criterion 131.15102971613473
weighted_aux_loss 4836.0
loss_r_bn_feature 483.6000061035156
------------iteration 100----------
total loss 1475.701218075017
main criterion 62.69511455939214
weighted_aux_loss 1413.006103515625
loss_r_bn_feature 141.3006134033203
------------iteration 200----------
total loss 1198.1917589342822
main criterion 53.18431264521973
weighted_aux_loss 1145.0074462890625
loss_r_bn_feature 114.50074005126953
------------iteration 300----------
total loss 822.673053176742
main criterion 51.903277786116945
weighted_aux_loss 770.769775390625
loss_r_bn_feature 77.07698059082031
------------iteration 400----------
total loss 750.6754637413397
main criterion 49.77885729602728
weighted_aux_loss 700.8966064453125
loss_r_bn_feature 70.08966064453125
------------iteration 500----------
total loss 627.9636038617356
main criterion 48.843608744548064
weighted_aux_loss 579.1199951171875
loss_r_bn_feature 57.91200256347656
------------iteration 600----------
total loss 595.1537479881458
main criterion 54.73638958970832
weighted_aux_loss 540.4173583984375
loss_r_bn_feature 54.04173278808594
------------iteration 700----------
total loss 557.8850783374786
main criterion 50.27674093513483
weighted_aux_loss 507.60833740234375
loss_r_bn_feature 50.760833740234375
------------iteration 800----------
total loss 806.9972401121951
main criterion 66.67344860828887
weighted_aux_loss 740.3237915039062
loss_r_bn_feature 74.03237915039062
------------iteration 900----------
total loss 486.5257627792994
main criterion 43.964483482424406
weighted_aux_loss 442.561279296875
loss_r_bn_feature 44.256126403808594
------------iteration 1000----------
total loss 1146.2556969639404
main criterion 72.2774254795655
weighted_aux_loss 1073.978271484375
loss_r_bn_feature 107.3978271484375
------------iteration 1100----------
total loss 406.7494881889262
main criterion 41.196509673301165
weighted_aux_loss 365.552978515625
loss_r_bn_feature 36.5552978515625
------------iteration 1200----------
total loss 472.4405828610579
main criterion 55.18838559543293
weighted_aux_loss 417.252197265625
loss_r_bn_feature 41.7252197265625
------------iteration 1300----------
total loss 330.79828332567695
main criterion 40.044590698723844
weighted_aux_loss 290.7536926269531
loss_r_bn_feature 29.075368881225586
------------iteration 1400----------
total loss 791.6598064558318
main criterion 69.71608086989434
weighted_aux_loss 721.9437255859375
loss_r_bn_feature 72.19437408447266
------------iteration 1500----------
total loss 360.53563106481107
main criterion 42.14311397496734
weighted_aux_loss 318.39251708984375
loss_r_bn_feature 31.839250564575195
------------iteration 1600----------
total loss 827.9512943129535
main criterion 67.88073767232844
weighted_aux_loss 760.070556640625
loss_r_bn_feature 76.0070571899414
------------iteration 1700----------
total loss 462.3691694973555
main criterion 55.29580523954299
weighted_aux_loss 407.0733642578125
loss_r_bn_feature 40.70733642578125
------------iteration 1800----------
total loss 322.8043833244785
main criterion 43.10485939869724
weighted_aux_loss 279.69952392578125
loss_r_bn_feature 27.969951629638672
------------iteration 1900----------
total loss 355.1581081451539
main criterion 44.89001122132576
weighted_aux_loss 310.2680969238281
loss_r_bn_feature 31.026809692382812
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 4469.17543250157
main criterion 147.59437781407001
weighted_aux_loss 4321.5810546875
loss_r_bn_feature 432.1580810546875
------------iteration 100----------
total loss 1615.0055275336558
main criterion 60.65274433053076
weighted_aux_loss 1554.352783203125
loss_r_bn_feature 155.43527221679688
------------iteration 200----------
total loss 986.372750188848
main criterion 58.22492304041059
weighted_aux_loss 928.1478271484375
loss_r_bn_feature 92.81478118896484
------------iteration 300----------
total loss 911.5483270646555
main criterion 65.49412784590555
weighted_aux_loss 846.05419921875
loss_r_bn_feature 84.60542297363281
------------iteration 400----------
total loss 756.7996314501796
main criterion 50.39020762205461
weighted_aux_loss 706.409423828125
loss_r_bn_feature 70.64094543457031
------------iteration 500----------
total loss 649.876109910525
main criterion 47.11500151208748
weighted_aux_loss 602.7611083984375
loss_r_bn_feature 60.2761116027832
------------iteration 600----------
total loss 633.0952908640627
main criterion 45.40174838359392
weighted_aux_loss 587.6935424804688
loss_r_bn_feature 58.769351959228516
------------iteration 700----------
total loss 615.6761693093435
main criterion 55.04048815699986
weighted_aux_loss 560.6356811523438
loss_r_bn_feature 56.063568115234375
------------iteration 800----------
total loss 560.8603180081855
main criterion 45.145596328497966
weighted_aux_loss 515.7147216796875
loss_r_bn_feature 51.57147216796875
------------iteration 900----------
total loss 475.27310545280966
main criterion 44.37656004265341
weighted_aux_loss 430.89654541015625
loss_r_bn_feature 43.08965301513672
------------iteration 1000----------
total loss 1211.1584949147805
main criterion 84.55876346946799
weighted_aux_loss 1126.5997314453125
loss_r_bn_feature 112.65997314453125
------------iteration 1100----------
total loss 519.8600628543879
main criterion 48.084092395403566
weighted_aux_loss 471.7759704589844
loss_r_bn_feature 47.17759704589844
------------iteration 1200----------
total loss 515.6351144361587
main criterion 44.72080779553369
weighted_aux_loss 470.914306640625
loss_r_bn_feature 47.0914306640625
------------iteration 1300----------
total loss 375.4177162808007
main criterion 42.55855490384754
weighted_aux_loss 332.8591613769531
loss_r_bn_feature 33.28591537475586
------------iteration 1400----------
total loss 376.5665339532937
main criterion 41.86292067204369
weighted_aux_loss 334.70361328125
loss_r_bn_feature 33.470359802246094
------------iteration 1500----------
total loss 344.1250037333477
main criterion 40.78509894819145
weighted_aux_loss 303.33990478515625
loss_r_bn_feature 30.3339900970459
------------iteration 1600----------
total loss 318.36912413101084
main criterion 35.45960875015143
weighted_aux_loss 282.9095153808594
loss_r_bn_feature 28.290952682495117
------------iteration 1700----------
total loss 793.9802579490979
main criterion 61.53641029284791
weighted_aux_loss 732.44384765625
loss_r_bn_feature 73.244384765625
------------iteration 1800----------
total loss 272.0829164063206
main criterion 36.904358056711196
weighted_aux_loss 235.17855834960938
loss_r_bn_feature 23.51785659790039
------------iteration 1900----------
total loss 371.3643015142624
main criterion 44.50456030332492
weighted_aux_loss 326.8597412109375
loss_r_bn_feature 32.68597412109375
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 4873.284750268082
main criterion 130.55867604933127
weighted_aux_loss 4742.72607421875
loss_r_bn_feature 474.2726135253906
------------iteration 100----------
total loss 1765.796666464669
main criterion 60.89005025373151
weighted_aux_loss 1704.9066162109375
loss_r_bn_feature 170.49066162109375
------------iteration 200----------
total loss 1165.5228035876344
main criterion 53.71152429075928
weighted_aux_loss 1111.811279296875
loss_r_bn_feature 111.1811294555664
------------iteration 300----------
total loss 1080.2451868778007
main criterion 65.42572886998835
weighted_aux_loss 1014.8194580078125
loss_r_bn_feature 101.48194885253906
------------iteration 400----------
total loss 833.0442473292673
main criterion 54.1690031886423
weighted_aux_loss 778.875244140625
loss_r_bn_feature 77.88752746582031
------------iteration 500----------
total loss 706.7854004580416
main criterion 46.918823309604164
weighted_aux_loss 659.8665771484375
loss_r_bn_feature 65.98665618896484
------------iteration 600----------
total loss 659.107045083558
main criterion 53.133229165589256
weighted_aux_loss 605.9738159179688
loss_r_bn_feature 60.597381591796875
------------iteration 700----------
total loss 754.2599501796948
main criterion 63.888246078132234
weighted_aux_loss 690.3717041015625
loss_r_bn_feature 69.03717041015625
------------iteration 800----------
total loss 621.0450480260121
main criterion 57.34442546741828
weighted_aux_loss 563.7006225585938
loss_r_bn_feature 56.37006378173828
------------iteration 900----------
total loss 481.260469857292
main criterion 47.196993294791966
weighted_aux_loss 434.0634765625
loss_r_bn_feature 43.406349182128906
------------iteration 1000----------
total loss 595.6493000606694
main criterion 57.31128736535683
weighted_aux_loss 538.3380126953125
loss_r_bn_feature 53.83380126953125
------------iteration 1100----------
total loss 451.2676660940093
main criterion 47.20696663111872
weighted_aux_loss 404.0606994628906
loss_r_bn_feature 40.406070709228516
------------iteration 1200----------
total loss 445.18489529139515
main criterion 43.29924465662952
weighted_aux_loss 401.8856506347656
loss_r_bn_feature 40.18856430053711
------------iteration 1300----------
total loss 508.94507517725947
main criterion 51.75162424952511
weighted_aux_loss 457.1934509277344
loss_r_bn_feature 45.71934509277344
------------iteration 1400----------
total loss 332.05897080797195
main criterion 39.953807233753224
weighted_aux_loss 292.10516357421875
loss_r_bn_feature 29.21051597595215
------------iteration 1500----------
total loss 341.94180850805543
main criterion 38.959295080321056
weighted_aux_loss 302.9825134277344
loss_r_bn_feature 30.298250198364258
------------iteration 1600----------
total loss 685.8937490466167
main criterion 62.99549465208537
weighted_aux_loss 622.8982543945312
loss_r_bn_feature 62.289825439453125
------------iteration 1700----------
total loss 340.113274864838
main criterion 42.49849825350985
weighted_aux_loss 297.6147766113281
loss_r_bn_feature 29.761476516723633
------------iteration 1800----------
total loss 351.25035280296333
main criterion 47.72783083030709
weighted_aux_loss 303.52252197265625
loss_r_bn_feature 30.352252960205078
------------iteration 1900----------
total loss 381.9522835738061
main criterion 44.572522831618606
weighted_aux_loss 337.3797607421875
loss_r_bn_feature 33.73797607421875
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 4573.813866494602
main criterion 135.91884696335188
weighted_aux_loss 4437.89501953125
loss_r_bn_feature 443.7895202636719
------------iteration 100----------
total loss 1776.6414690592753
main criterion 60.77745538740026
weighted_aux_loss 1715.864013671875
loss_r_bn_feature 171.58639526367188
------------iteration 200----------
total loss 894.5021877985961
main criterion 57.55406768140858
weighted_aux_loss 836.9481201171875
loss_r_bn_feature 83.69480895996094
------------iteration 300----------
total loss 921.4195018578561
main criterion 71.40924795160609
weighted_aux_loss 850.01025390625
loss_r_bn_feature 85.00102233886719
------------iteration 400----------
total loss 568.7682765765761
main criterion 48.23397481876364
weighted_aux_loss 520.5343017578125
loss_r_bn_feature 52.05343246459961
------------iteration 500----------
total loss 646.7237641066246
main criterion 47.41523139178081
weighted_aux_loss 599.3085327148438
loss_r_bn_feature 59.930850982666016
------------iteration 600----------
total loss 494.35379525641673
main criterion 46.560521330635474
weighted_aux_loss 447.79327392578125
loss_r_bn_feature 44.779327392578125
------------iteration 700----------
total loss 1299.6702237998704
main criterion 91.5015226279955
weighted_aux_loss 1208.168701171875
loss_r_bn_feature 120.8168716430664
------------iteration 800----------
total loss 624.5942586234506
main criterion 48.234822588294364
weighted_aux_loss 576.3594360351562
loss_r_bn_feature 57.63594436645508
------------iteration 900----------
total loss 510.73845238078815
main criterion 43.79387230266315
weighted_aux_loss 466.944580078125
loss_r_bn_feature 46.6944580078125
------------iteration 1000----------
total loss 491.23018630531055
main criterion 46.92836745765432
weighted_aux_loss 444.30181884765625
loss_r_bn_feature 44.43018341064453
------------iteration 1100----------
total loss 525.8605471514139
main criterion 42.817639436570225
weighted_aux_loss 483.04290771484375
loss_r_bn_feature 48.304290771484375
------------iteration 1200----------
total loss 388.1779961616263
main criterion 39.739305976079436
weighted_aux_loss 348.4386901855469
loss_r_bn_feature 34.843868255615234
------------iteration 1300----------
total loss 519.2185008143839
main criterion 48.194788656180755
weighted_aux_loss 471.0237121582031
loss_r_bn_feature 47.10237121582031
------------iteration 1400----------
total loss 561.7725945322411
main criterion 60.17408379005357
weighted_aux_loss 501.5985107421875
loss_r_bn_feature 50.15985107421875
------------iteration 1500----------
total loss 485.82044584119126
main criterion 53.452647989628744
weighted_aux_loss 432.3677978515625
loss_r_bn_feature 43.236778259277344
------------iteration 1600----------
total loss 311.0499904824429
main criterion 37.231051273458554
weighted_aux_loss 273.8189392089844
loss_r_bn_feature 27.381895065307617
------------iteration 1700----------
total loss 287.0250703361186
main criterion 35.662399437681096
weighted_aux_loss 251.3626708984375
loss_r_bn_feature 25.136266708374023
------------iteration 1800----------
total loss 271.423889721081
main criterion 36.29855402772163
weighted_aux_loss 235.12533569335938
loss_r_bn_feature 23.51253318786621
------------iteration 1900----------
total loss 446.987990832751
main criterion 50.282546496813474
weighted_aux_loss 396.7054443359375
loss_r_bn_feature 39.6705436706543
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 4814.464904140001
main criterion 137.13238460875056
weighted_aux_loss 4677.33251953125
loss_r_bn_feature 467.7332458496094
------------iteration 100----------
total loss 1562.3778582470136
main criterion 57.384694184513656
weighted_aux_loss 1504.9931640625
loss_r_bn_feature 150.4993133544922
------------iteration 200----------
total loss 815.8604828015513
main criterion 59.305307020301285
weighted_aux_loss 756.55517578125
loss_r_bn_feature 75.655517578125
------------iteration 300----------
total loss 1246.011049090801
main criterion 82.74163991111348
weighted_aux_loss 1163.2694091796875
loss_r_bn_feature 116.32694244384766
------------iteration 400----------
total loss 704.2987197384631
main criterion 50.604566906431856
weighted_aux_loss 653.6941528320312
loss_r_bn_feature 65.36941528320312
------------iteration 500----------
total loss 683.6453978738276
main criterion 57.08234855742137
weighted_aux_loss 626.5630493164062
loss_r_bn_feature 62.65630340576172
------------iteration 600----------
total loss 587.5290647809471
main criterion 46.36402571844711
weighted_aux_loss 541.1650390625
loss_r_bn_feature 54.11650466918945
------------iteration 700----------
total loss 595.1459791545121
main criterion 42.251936185762176
weighted_aux_loss 552.89404296875
loss_r_bn_feature 55.289405822753906
------------iteration 800----------
total loss 574.8504035435972
main criterion 55.455750223284646
weighted_aux_loss 519.3946533203125
loss_r_bn_feature 51.9394645690918
------------iteration 900----------
total loss 421.5463859653719
main criterion 42.94833298685624
weighted_aux_loss 378.5980529785156
loss_r_bn_feature 37.859806060791016
------------iteration 1000----------
total loss 593.0091190625064
main criterion 53.29085734375642
weighted_aux_loss 539.71826171875
loss_r_bn_feature 53.971824645996094
------------iteration 1100----------
total loss 436.3438400434354
main criterion 49.079924027810414
weighted_aux_loss 387.263916015625
loss_r_bn_feature 38.72639083862305
------------iteration 1200----------
total loss 393.6002517273108
main criterion 45.34887843629517
weighted_aux_loss 348.2513732910156
loss_r_bn_feature 34.825138092041016
------------iteration 1300----------
total loss 332.5903148517997
main criterion 40.85612295726842
weighted_aux_loss 291.73419189453125
loss_r_bn_feature 29.173418045043945
------------iteration 1400----------
total loss 705.8991955321632
main criterion 62.76674924310069
weighted_aux_loss 643.1324462890625
loss_r_bn_feature 64.31324768066406
------------iteration 1500----------
total loss 663.4906949630966
main criterion 66.26181312715916
weighted_aux_loss 597.2288818359375
loss_r_bn_feature 59.7228889465332
------------iteration 1600----------
total loss 1542.5494194174878
main criterion 88.80491258155025
weighted_aux_loss 1453.7445068359375
loss_r_bn_feature 145.37445068359375
------------iteration 1700----------
total loss 365.92723099610555
main criterion 43.21971146485553
weighted_aux_loss 322.70751953125
loss_r_bn_feature 32.270751953125
------------iteration 1800----------
total loss 548.7466512879702
main criterion 59.73325407117326
weighted_aux_loss 489.0133972167969
loss_r_bn_feature 48.90134048461914
------------iteration 1900----------
total loss 297.73039270741583
main criterion 39.57554651600958
weighted_aux_loss 258.15484619140625
loss_r_bn_feature 25.81548309326172
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 5082.168052073217
main criterion 147.32381379196707
weighted_aux_loss 4934.84423828125
loss_r_bn_feature 493.48443603515625
------------iteration 100----------
total loss 1434.689729392206
main criterion 53.10110634533087
weighted_aux_loss 1381.588623046875
loss_r_bn_feature 138.1588592529297
------------iteration 200----------
total loss 1303.5898905430386
main criterion 86.52702433210106
weighted_aux_loss 1217.0628662109375
loss_r_bn_feature 121.70629119873047
------------iteration 300----------
total loss 971.2462079358522
main criterion 71.64592717413353
weighted_aux_loss 899.6002807617188
loss_r_bn_feature 89.96002960205078
------------iteration 400----------
total loss 645.1706846763116
main criterion 48.3782652427178
weighted_aux_loss 596.7924194335938
loss_r_bn_feature 59.67924118041992
------------iteration 500----------
total loss 830.8267205554316
main criterion 58.00500424683784
weighted_aux_loss 772.8217163085938
loss_r_bn_feature 77.28217315673828
------------iteration 600----------
total loss 730.04812322399
main criterion 63.19692693492755
weighted_aux_loss 666.8511962890625
loss_r_bn_feature 66.68511962890625
------------iteration 700----------
total loss 528.4260895193339
main criterion 50.39294742949017
weighted_aux_loss 478.03314208984375
loss_r_bn_feature 47.803314208984375
------------iteration 800----------
total loss 704.8584548714948
main criterion 54.83660428555727
weighted_aux_loss 650.0218505859375
loss_r_bn_feature 65.00218200683594
------------iteration 900----------
total loss 524.2078009090168
main criterion 43.12876038167298
weighted_aux_loss 481.07904052734375
loss_r_bn_feature 48.10790252685547
------------iteration 1000----------
total loss 923.4535398455398
main criterion 67.0802488299148
weighted_aux_loss 856.373291015625
loss_r_bn_feature 85.6373291015625
------------iteration 1100----------
total loss 397.96314031105993
main criterion 41.07642156105993
weighted_aux_loss 356.88671875
loss_r_bn_feature 35.68867111206055
------------iteration 1200----------
total loss 405.3397298011404
main criterion 40.435921207390365
weighted_aux_loss 364.90380859375
loss_r_bn_feature 36.490379333496094
------------iteration 1300----------
total loss 541.6811963515808
main criterion 52.85413946681514
weighted_aux_loss 488.8270568847656
loss_r_bn_feature 48.88270568847656
------------iteration 1400----------
total loss 409.35037063474135
main criterion 45.42892288083513
weighted_aux_loss 363.92144775390625
loss_r_bn_feature 36.39214324951172
------------iteration 1500----------
total loss 374.2929420808184
main criterion 42.70969012769345
weighted_aux_loss 331.583251953125
loss_r_bn_feature 33.1583251953125
------------iteration 1600----------
total loss 255.7070243257487
main criterion 35.375130404850275
weighted_aux_loss 220.33189392089844
loss_r_bn_feature 22.03318977355957
------------iteration 1700----------
total loss 236.57948021692854
main criterion 35.83167748255355
weighted_aux_loss 200.747802734375
loss_r_bn_feature 20.074779510498047
------------iteration 1800----------
total loss 319.3783587692681
main criterion 40.988496708721236
weighted_aux_loss 278.3898620605469
loss_r_bn_feature 27.838985443115234
------------iteration 1900----------
total loss 250.55177383936552
main criterion 36.42256241358427
weighted_aux_loss 214.12921142578125
loss_r_bn_feature 21.412921905517578
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 4463.781030666888
main criterion 126.83230019813787
weighted_aux_loss 4336.94873046875
loss_r_bn_feature 433.69488525390625
------------iteration 100----------
total loss 1368.4020963577275
main criterion 58.663570967102565
weighted_aux_loss 1309.738525390625
loss_r_bn_feature 130.97384643554688
------------iteration 200----------
total loss 1016.3832802190293
main criterion 67.47147601981054
weighted_aux_loss 948.9118041992188
loss_r_bn_feature 94.89118194580078
------------iteration 300----------
total loss 878.7317655973893
main criterion 59.876540988014305
weighted_aux_loss 818.855224609375
loss_r_bn_feature 81.8855209350586
------------iteration 400----------
total loss 525.9278625040463
main criterion 42.348394730608824
weighted_aux_loss 483.5794677734375
loss_r_bn_feature 48.357948303222656
------------iteration 500----------
total loss 667.3065258472898
main criterion 56.29749264416479
weighted_aux_loss 611.009033203125
loss_r_bn_feature 61.10090255737305
------------iteration 600----------
total loss 531.5342804924708
main criterion 47.70664377372081
weighted_aux_loss 483.82763671875
loss_r_bn_feature 48.38276290893555
------------iteration 700----------
total loss 582.097279103404
main criterion 46.74370244324781
weighted_aux_loss 535.3535766601562
loss_r_bn_feature 53.53535842895508
------------iteration 800----------
total loss 514.9695681524921
main criterion 40.794122595851455
weighted_aux_loss 474.1754455566406
loss_r_bn_feature 47.417545318603516
------------iteration 900----------
total loss 515.9860665881467
main criterion 44.51615692017799
weighted_aux_loss 471.46990966796875
loss_r_bn_feature 47.14699172973633
------------iteration 1000----------
total loss 602.8103511631796
main criterion 53.91942098739833
weighted_aux_loss 548.8909301757812
loss_r_bn_feature 54.889095306396484
------------iteration 1100----------
total loss 423.3327992829525
main criterion 42.69025167553065
weighted_aux_loss 380.6425476074219
loss_r_bn_feature 38.06425476074219
------------iteration 1200----------
total loss 363.24380042484677
main criterion 36.746333383831114
weighted_aux_loss 326.4974670410156
loss_r_bn_feature 32.64974594116211
------------iteration 1300----------
total loss 348.9248205120778
main criterion 39.612351029655954
weighted_aux_loss 309.3124694824219
loss_r_bn_feature 30.931245803833008
------------iteration 1400----------
total loss 1311.4250083767242
main criterion 83.8221031032868
weighted_aux_loss 1227.6029052734375
loss_r_bn_feature 122.76029205322266
------------iteration 1500----------
total loss 339.7903187819158
main criterion 44.2428029127752
weighted_aux_loss 295.5475158691406
loss_r_bn_feature 29.554752349853516
------------iteration 1600----------
total loss 344.5324384501423
main criterion 39.01571481732984
weighted_aux_loss 305.5167236328125
loss_r_bn_feature 30.551673889160156
------------iteration 1700----------
total loss 299.01691054510945
main criterion 35.5773963849532
weighted_aux_loss 263.43951416015625
loss_r_bn_feature 26.343952178955078
------------iteration 1800----------
total loss 304.88359241914657
main criterion 38.00709705781844
weighted_aux_loss 266.8764953613281
loss_r_bn_feature 26.68764877319336
------------iteration 1900----------
total loss 285.1792707357702
main criterion 33.87714671233274
weighted_aux_loss 251.3021240234375
loss_r_bn_feature 25.130212783813477
=> using pytorch pre-trained model 'resnet18'
process data from ../recover/syn_data/tiny-imagenet/237
  0%|          | 0/300 [00:00<?, ?it/s]  0%|          | 1/300 [00:03<19:51,  3.99s/it]  1%|          | 2/300 [00:06<14:23,  2.90s/it]  1%|          | 3/300 [00:08<12:31,  2.53s/it]  1%|▏         | 4/300 [00:10<11:54,  2.41s/it]  2%|▏         | 5/300 [00:12<11:19,  2.30s/it]  2%|▏         | 6/300 [00:14<10:57,  2.24s/it]  2%|▏         | 7/300 [00:16<10:44,  2.20s/it]  3%|▎         | 8/300 [00:18<10:34,  2.17s/it]  3%|▎         | 9/300 [00:21<10:33,  2.18s/it]  3%|▎         | 10/300 [00:23<10:26,  2.16s/it]  4%|▎         | 11/300 [00:25<10:18,  2.14s/it]  4%|▍         | 12/300 [00:27<10:19,  2.15s/it]  4%|▍         | 13/300 [00:29<10:16,  2.15s/it]  5%|▍         | 14/300 [00:31<10:09,  2.13s/it]  5%|▌         | 15/300 [00:33<10:10,  2.14s/it]  5%|▌         | 16/300 [00:35<10:05,  2.13s/it]  6%|▌         | 17/300 [00:38<09:58,  2.12s/it]  6%|▌         | 18/300 [00:40<09:57,  2.12s/it]  6%|▋         | 19/300 [00:42<09:54,  2.11s/it]  7%|▋         | 20/300 [00:44<09:53,  2.12s/it]  7%|▋         | 21/300 [00:46<09:56,  2.14s/it]  7%|▋         | 22/300 [00:48<09:49,  2.12s/it]  8%|▊         | 23/300 [00:50<09:45,  2.11s/it]  8%|▊         | 24/300 [00:52<09:41,  2.11s/it]  8%|▊         | 25/300 [00:54<09:37,  2.10s/it]  9%|▊         | 26/300 [00:57<09:42,  2.13s/it]  9%|▉         | 27/300 [00:59<09:43,  2.14s/it]  9%|▉         | 28/300 [01:01<09:38,  2.13s/it] 10%|▉         | 29/300 [01:03<09:34,  2.12s/it] 10%|█         | 30/300 [01:05<09:29,  2.11s/it] 10%|█         | 31/300 [01:07<09:23,  2.10s/it] 11%|█         | 32/300 [01:09<09:34,  2.14s/it] 11%|█         | 33/300 [01:12<09:36,  2.16s/it] 11%|█▏        | 34/300 [01:14<09:31,  2.15s/it] 12%|█▏        | 35/300 [01:16<09:23,  2.13s/it] 12%|█▏        | 36/300 [01:18<09:24,  2.14s/it] 12%|█▏        | 37/300 [01:20<09:25,  2.15s/it] 13%|█▎        | 38/300 [01:22<09:17,  2.13s/it] 13%|█▎        | 39/300 [01:24<09:16,  2.13s/it] 13%|█▎        | 40/300 [01:27<09:16,  2.14s/it] 14%|█▎        | 41/300 [01:29<09:10,  2.13s/it] 14%|█▍        | 42/300 [01:31<09:12,  2.14s/it] 14%|█▍        | 43/300 [01:33<09:06,  2.13s/it] 15%|█▍        | 44/300 [01:35<09:06,  2.14s/it] 15%|█▌        | 45/300 [01:37<09:08,  2.15s/it] 15%|█▌        | 46/300 [01:39<09:02,  2.14s/it] 16%|█▌        | 47/300 [01:42<09:02,  2.15s/it] 16%|█▌        | 48/300 [01:44<08:54,  2.12s/it] 16%|█▋        | 49/300 [01:46<08:53,  2.13s/it] 17%|█▋        | 50/300 [01:48<08:53,  2.13s/it] 17%|█▋        | 51/300 [01:50<08:57,  2.16s/it] 17%|█▋        | 52/300 [01:52<08:51,  2.14s/it] 18%|█▊        | 53/300 [01:54<08:44,  2.13s/it] 18%|█▊        | 54/300 [01:56<08:39,  2.11s/it] 18%|█▊        | 55/300 [01:58<08:39,  2.12s/it] 19%|█▊        | 56/300 [02:01<08:34,  2.11s/it] 19%|█▉        | 57/300 [02:03<08:34,  2.12s/it] 19%|█▉        | 58/300 [02:05<08:34,  2.13s/it] 20%|█▉        | 59/300 [02:07<08:28,  2.11s/it] 20%|██        | 60/300 [02:09<08:28,  2.12s/it] 20%|██        | 61/300 [02:11<08:31,  2.14s/it] 21%|██        | 62/300 [02:13<08:30,  2.14s/it] 21%|██        | 63/300 [02:15<08:23,  2.12s/it] 21%|██▏       | 64/300 [02:18<08:23,  2.13s/it] 22%|██▏       | 65/300 [02:20<08:23,  2.14s/it] 22%|██▏       | 66/300 [02:22<08:24,  2.16s/it] 22%|██▏       | 67/300 [02:24<08:17,  2.14s/it] 23%|██▎       | 68/300 [02:26<08:13,  2.13s/it] 23%|██▎       | 69/300 [02:28<08:12,  2.13s/it] 23%|██▎       | 70/300 [02:30<08:07,  2.12s/it] 24%|██▎       | 71/300 [02:33<08:12,  2.15s/it] 24%|██▍       | 72/300 [02:35<08:09,  2.15s/it] 24%|██▍       | 73/300 [02:37<08:02,  2.13s/it] 25%|██▍       | 74/300 [02:39<08:03,  2.14s/it] 25%|██▌       | 75/300 [02:41<08:02,  2.15s/it] 25%|██▌       | 76/300 [02:43<08:02,  2.15s/it] 26%|██▌       | 77/300 [02:46<07:58,  2.15s/it] 26%|██▌       | 78/300 [02:48<07:56,  2.15s/it] 26%|██▋       | 79/300 [02:50<07:48,  2.12s/it] 27%|██▋       | 80/300 [02:52<07:46,  2.12s/it] 27%|██▋       | 81/300 [02:54<07:48,  2.14s/it] 27%|██▋       | 82/300 [02:56<07:45,  2.13s/it] 28%|██▊       | 83/300 [02:58<07:42,  2.13s/it] 28%|██▊       | 84/300 [03:00<07:43,  2.15s/it] 28%|██▊       | 85/300 [03:03<07:37,  2.13s/it] 29%|██▊       | 86/300 [03:05<07:37,  2.14s/it] 29%|██▉       | 87/300 [03:07<07:32,  2.13s/it] 29%|██▉       | 88/300 [03:09<07:36,  2.15s/it] 30%|██▉       | 89/300 [03:11<07:38,  2.17s/it] 30%|███       | 90/300 [03:13<07:30,  2.14s/it] 30%|███       | 91/300 [03:15<07:24,  2.13s/it] 31%|███       | 92/300 [03:17<07:19,  2.11s/it] 31%|███       | 93/300 [03:20<07:15,  2.10s/it] 31%|███▏      | 94/300 [03:22<07:17,  2.12s/it] 32%|███▏      | 95/300 [03:24<07:15,  2.13s/it] 32%|███▏      | 96/300 [03:26<07:18,  2.15s/it] 32%|███▏      | 97/300 [03:28<07:15,  2.15s/it] 33%|███▎      | 98/300 [03:30<07:08,  2.12s/it] 33%|███▎      | 99/300 [03:32<07:06,  2.12s/it] 33%|███▎      | 100/300 [03:35<07:16,  2.18s/it] 34%|███▎      | 101/300 [03:37<07:14,  2.18s/it] 34%|███▍      | 102/300 [03:39<07:10,  2.17s/it] 34%|███▍      | 103/300 [03:41<07:06,  2.17s/it] 35%|███▍      | 104/300 [03:43<07:01,  2.15s/it] 35%|███▌      | 105/300 [03:45<07:00,  2.16s/it] 35%|███▌      | 106/300 [03:48<06:58,  2.16s/it] 36%|███▌      | 107/300 [03:50<06:56,  2.16s/it] 36%|███▌      | 108/300 [03:52<06:50,  2.14s/it] 36%|███▋      | 109/300 [03:54<06:50,  2.15s/it] 37%|███▋      | 110/300 [03:56<06:48,  2.15s/it] 37%|███▋      | 111/300 [03:58<06:52,  2.18s/it] 37%|███▋      | 112/300 [04:01<06:44,  2.15s/it] 38%|███▊      | 113/300 [04:03<06:40,  2.14s/it] 38%|███▊      | 114/300 [04:05<06:36,  2.13s/it] 38%|███▊      | 115/300 [04:07<06:35,  2.14s/it] 39%|███▊      | 116/300 [04:09<06:38,  2.17s/it] 39%|███▉      | 117/300 [04:11<06:35,  2.16s/it] 39%|███▉      | 118/300 [04:13<06:30,  2.15s/it] 40%|███▉      | 119/300 [04:16<06:26,  2.13s/it] 40%|████      | 120/300 [04:18<06:22,  2.13s/it] 40%|████      | 121/300 [04:20<06:23,  2.14s/it] 41%|████      | 122/300 [04:22<06:22,  2.15s/it] 41%|████      | 123/300 [04:24<06:16,  2.13s/it] 41%|████▏     | 124/300 [04:26<06:14,  2.13s/it] 42%|████▏     | 125/300 [04:28<06:13,  2.14s/it] 42%|████▏     | 126/300 [04:31<06:15,  2.16s/it] 42%|████▏     | 127/300 [04:33<06:13,  2.16s/it] 43%|████▎     | 128/300 [04:35<06:15,  2.18s/it] 43%|████▎     | 129/300 [04:37<06:08,  2.16s/it] 43%|████▎     | 130/300 [04:39<06:03,  2.14s/it] 44%|████▎     | 131/300 [04:41<05:59,  2.13s/it] 44%|████▍     | 132/300 [04:43<05:59,  2.14s/it] 44%|████▍     | 133/300 [04:46<05:58,  2.15s/it] 45%|████▍     | 134/300 [04:48<05:54,  2.14s/it] 45%|████▌     | 135/300 [04:50<05:51,  2.13s/it] 45%|████▌     | 136/300 [04:52<05:48,  2.12s/it] 46%|████▌     | 137/300 [04:54<05:45,  2.12s/it] 46%|████▌     | 138/300 [04:56<05:48,  2.15s/it] 46%|████▋     | 139/300 [04:58<05:45,  2.14s/it] 47%|████▋     | 140/300 [05:00<05:39,  2.12s/it] 47%|████▋     | 141/300 [05:03<05:40,  2.14s/it] 47%|████▋     | 142/300 [05:05<05:38,  2.15s/it] 48%|████▊     | 143/300 [05:07<05:38,  2.16s/it] 48%|████▊     | 144/300 [05:09<05:34,  2.15s/it] 48%|████▊     | 145/300 [05:11<05:33,  2.15s/it] 49%|████▊     | 146/300 [05:13<05:29,  2.14s/it] 49%|████▉     | 147/300 [05:16<05:32,  2.17s/it] 49%|████▉     | 148/300 [05:18<05:29,  2.17s/it] 50%|████▉     | 149/300 [05:20<05:25,  2.15s/it] 50%|█████     | 150/300 [05:22<05:20,  2.14s/it] 50%|█████     | 151/300 [05:24<05:16,  2.12s/it] 51%|█████     | 152/300 [05:26<05:12,  2.11s/it] 51%|█████     | 153/300 [05:28<05:11,  2.12s/it] 51%|█████▏    | 154/300 [05:30<05:10,  2.13s/it] 52%|█████▏    | 155/300 [05:33<05:09,  2.13s/it] 52%|█████▏    | 156/300 [05:35<05:06,  2.13s/it] 52%|█████▏    | 157/300 [05:37<05:08,  2.16s/it] 53%|█████▎    | 158/300 [05:39<05:13,  2.21s/it] 53%|█████▎    | 159/300 [05:41<05:09,  2.20s/it] 53%|█████▎    | 160/300 [05:44<05:05,  2.18s/it] 54%|█████▎    | 161/300 [05:46<05:02,  2.18s/it] 54%|█████▍    | 162/300 [05:48<04:57,  2.15s/it] 54%|█████▍    | 163/300 [05:50<04:51,  2.13s/it] 55%|█████▍    | 164/300 [05:52<04:50,  2.13s/it] 55%|█████▌    | 165/300 [05:54<04:46,  2.12s/it] 55%|█████▌    | 166/300 [05:56<04:42,  2.11s/it] 56%|█████▌    | 167/300 [05:58<04:44,  2.14s/it] 56%|█████▌    | 168/300 [06:01<04:41,  2.13s/it] 56%|█████▋    | 169/300 [06:03<04:41,  2.15s/it] 57%|█████▋    | 170/300 [06:05<04:39,  2.15s/it] 57%|█████▋    | 171/300 [06:07<04:38,  2.16s/it] 57%|█████▋    | 172/300 [06:09<04:38,  2.17s/it] 58%|█████▊    | 173/300 [06:11<04:34,  2.16s/it] 58%|█████▊    | 174/300 [06:14<04:35,  2.18s/it] 58%|█████▊    | 175/300 [06:16<04:32,  2.18s/it] 59%|█████▊    | 176/300 [06:18<04:27,  2.16s/it] 59%|█████▉    | 177/300 [06:20<04:22,  2.13s/it] 59%|█████▉    | 178/300 [06:22<04:17,  2.11s/it] 60%|█████▉    | 179/300 [06:24<04:16,  2.12s/it] 60%|██████    | 180/300 [06:26<04:15,  2.13s/it] 60%|██████    | 181/300 [06:28<04:13,  2.13s/it] 61%|██████    | 182/300 [06:31<04:13,  2.15s/it] 61%|██████    | 183/300 [06:33<04:09,  2.13s/it] 61%|██████▏   | 184/300 [06:35<04:07,  2.13s/it] 62%|██████▏   | 185/300 [06:37<04:03,  2.12s/it] 62%|██████▏   | 186/300 [06:39<04:04,  2.15s/it] 62%|██████▏   | 187/300 [06:41<04:05,  2.17s/it] 63%|██████▎   | 188/300 [06:44<04:05,  2.19s/it] 63%|██████▎   | 189/300 [06:46<04:02,  2.18s/it] 63%|██████▎   | 190/300 [06:48<03:58,  2.17s/it] 64%|██████▎   | 191/300 [06:50<03:54,  2.15s/it] 64%|██████▍   | 192/300 [06:52<03:53,  2.16s/it] 64%|██████▍   | 193/300 [06:54<03:48,  2.14s/it] 65%|██████▍   | 194/300 [06:56<03:45,  2.13s/it] 65%|██████▌   | 195/300 [06:59<03:44,  2.14s/it] 65%|██████▌   | 196/300 [07:01<03:44,  2.15s/it] 66%|██████▌   | 197/300 [07:03<03:39,  2.13s/it] 66%|██████▌   | 198/300 [07:05<03:35,  2.12s/it] 66%|██████▋   | 199/300 [07:07<03:34,  2.13s/it] 67%|██████▋   | 200/300 [07:09<03:33,  2.13s/it] 67%|██████▋   | 201/300 [07:11<03:32,  2.15s/it] 67%|██████▋   | 202/300 [07:14<03:30,  2.15s/it] 68%|██████▊   | 203/300 [07:16<03:27,  2.14s/it] 68%|██████▊   | 204/300 [07:18<03:27,  2.16s/it] 68%|██████▊   | 205/300 [07:20<03:25,  2.16s/it] 69%|██████▊   | 206/300 [07:22<03:20,  2.14s/it] 69%|██████▉   | 207/300 [07:24<03:17,  2.12s/it] 69%|██████▉   | 208/300 [07:26<03:15,  2.13s/it] 70%|██████▉   | 209/300 [07:29<03:13,  2.13s/it] 70%|███████   | 210/300 [07:31<03:14,  2.16s/it] 70%|███████   | 211/300 [07:33<03:12,  2.16s/it] 71%|███████   | 212/300 [07:35<03:08,  2.14s/it] 71%|███████   | 213/300 [07:37<03:06,  2.14s/it] 71%|███████▏  | 214/300 [07:39<03:04,  2.15s/it] 72%|███████▏  | 215/300 [07:42<03:03,  2.16s/it] 72%|███████▏  | 216/300 [07:44<02:59,  2.14s/it] 72%|███████▏  | 217/300 [07:46<02:57,  2.14s/it] 73%|███████▎  | 218/300 [07:48<02:55,  2.14s/it] 73%|███████▎  | 219/300 [07:50<02:53,  2.14s/it] 73%|███████▎  | 220/300 [07:52<02:50,  2.13s/it] 74%|███████▎  | 221/300 [07:54<02:49,  2.14s/it] 74%|███████▍  | 222/300 [07:56<02:45,  2.13s/it] 74%|███████▍  | 223/300 [07:59<02:45,  2.15s/it] 75%|███████▍  | 224/300 [08:01<02:42,  2.14s/it] 75%|███████▌  | 225/300 [08:03<02:40,  2.13s/it] 75%|███████▌  | 226/300 [08:05<02:38,  2.14s/it] 76%|███████▌  | 227/300 [08:07<02:35,  2.12s/it] 76%|███████▌  | 228/300 [08:09<02:32,  2.12s/it] 76%|███████▋  | 229/300 [08:11<02:31,  2.14s/it] 77%|███████▋  | 230/300 [08:13<02:28,  2.13s/it] 77%|███████▋  | 231/300 [08:16<02:28,  2.16s/it] 77%|███████▋  | 232/300 [08:18<02:25,  2.15s/it] 78%|███████▊  | 233/300 [08:20<02:23,  2.15s/it] 78%|███████▊  | 234/300 [08:22<02:22,  2.16s/it] 78%|███████▊  | 235/300 [08:24<02:22,  2.19s/it] 79%|███████▊  | 236/300 [08:27<02:19,  2.17s/it] 79%|███████▉  | 237/300 [08:29<02:16,  2.16s/it] 79%|███████▉  | 238/300 [08:31<02:13,  2.15s/it] 80%|███████▉  | 239/300 [08:33<02:11,  2.16s/it] 80%|████████  | 240/300 [08:35<02:09,  2.16s/it] 80%|████████  | 241/300 [08:37<02:06,  2.15s/it] 81%|████████  | 242/300 [08:39<02:04,  2.15s/it] 81%|████████  | 243/300 [08:42<02:01,  2.14s/it] 81%|████████▏ | 244/300 [08:44<02:00,  2.15s/it] 82%|████████▏ | 245/300 [08:46<01:58,  2.16s/it] 82%|████████▏ | 246/300 [08:48<01:55,  2.14s/it] 82%|████████▏ | 247/300 [08:50<01:53,  2.15s/it] 83%|████████▎ | 248/300 [08:52<01:52,  2.16s/it] 83%|████████▎ | 249/300 [08:54<01:49,  2.15s/it] 83%|████████▎ | 250/300 [08:57<01:47,  2.15s/it] 84%|████████▎ | 251/300 [08:59<01:45,  2.15s/it] 84%|████████▍ | 252/300 [09:01<01:43,  2.16s/it] 84%|████████▍ | 253/300 [09:03<01:42,  2.17s/it] 85%|████████▍ | 254/300 [09:05<01:39,  2.17s/it] 85%|████████▌ | 255/300 [09:07<01:36,  2.15s/it] 85%|████████▌ | 256/300 [09:10<01:34,  2.15s/it] 86%|████████▌ | 257/300 [09:12<01:32,  2.16s/it] 86%|████████▌ | 258/300 [09:14<01:29,  2.14s/it] 86%|████████▋ | 259/300 [09:16<01:27,  2.14s/it] 87%|████████▋ | 260/300 [09:18<01:26,  2.16s/it] 87%|████████▋ | 261/300 [09:20<01:23,  2.14s/it] 87%|████████▋ | 262/300 [09:22<01:22,  2.16s/it] 88%|████████▊ | 263/300 [09:25<01:19,  2.15s/it] 88%|████████▊ | 264/300 [09:27<01:16,  2.13s/it] 88%|████████▊ | 265/300 [09:29<01:15,  2.15s/it] 89%|████████▊ | 266/300 [09:31<01:13,  2.15s/it] 89%|████████▉ | 267/300 [09:33<01:10,  2.15s/it] 89%|████████▉ | 268/300 [09:35<01:08,  2.15s/it] 90%|████████▉ | 269/300 [09:37<01:06,  2.13s/it] 90%|█████████ | 270/300 [09:40<01:04,  2.15s/it] 90%|█████████ | 271/300 [09:42<01:02,  2.17s/it] 91%|█████████ | 272/300 [09:44<01:00,  2.15s/it] 91%|█████████ | 273/300 [09:46<00:57,  2.14s/it] 91%|█████████▏| 274/300 [09:48<00:55,  2.13s/it] 92%|█████████▏| 275/300 [09:50<00:53,  2.15s/it] 92%|█████████▏| 276/300 [09:52<00:51,  2.14s/it] 92%|█████████▏| 277/300 [09:55<00:49,  2.14s/it] 93%|█████████▎| 278/300 [09:57<00:47,  2.18s/it] 93%|█████████▎| 279/300 [09:59<00:45,  2.16s/it] 93%|█████████▎| 280/300 [10:01<00:43,  2.16s/it] 94%|█████████▎| 281/300 [10:03<00:41,  2.16s/it] 94%|█████████▍| 282/300 [10:05<00:38,  2.14s/it] 94%|█████████▍| 283/300 [10:08<00:36,  2.13s/it] 95%|█████████▍| 284/300 [10:10<00:34,  2.15s/it] 95%|█████████▌| 285/300 [10:12<00:31,  2.13s/it] 95%|█████████▌| 286/300 [10:14<00:29,  2.12s/it] 96%|█████████▌| 287/300 [10:16<00:27,  2.11s/it] 96%|█████████▌| 288/300 [10:18<00:25,  2.15s/it] 96%|█████████▋| 289/300 [10:20<00:23,  2.14s/it] 97%|█████████▋| 290/300 [10:23<00:21,  2.14s/it] 97%|█████████▋| 291/300 [10:25<00:19,  2.13s/it] 97%|█████████▋| 292/300 [10:27<00:17,  2.15s/it] 98%|█████████▊| 293/300 [10:29<00:14,  2.13s/it] 98%|█████████▊| 294/300 [10:31<00:12,  2.12s/it] 98%|█████████▊| 295/300 [10:33<00:10,  2.14s/it] 99%|█████████▊| 296/300 [10:35<00:08,  2.13s/it] 99%|█████████▉| 297/300 [10:37<00:06,  2.12s/it] 99%|█████████▉| 298/300 [10:40<00:04,  2.16s/it]100%|█████████▉| 299/300 [10:42<00:02,  2.19s/it]100%|██████████| 300/300 [10:44<00:00,  2.16s/it]100%|██████████| 300/300 [10:44<00:00,  2.15s/it]
W&B disabled.
W&B online. Running your script from this directory will now sync to the cloud.
wandb: Currently logged in as: hl57. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.12 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.10
wandb: Run data is saved locally in /media/techt/One Touch/DD/SRe2L/train/wandb/run-20231007_102316-wzy4pmy8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run earthy-music-404
wandb: ⭐️ View project at https://wandb.ai/hl57/final_rn18_fkd
wandb: 🚀 View run at https://wandb.ai/hl57/final_rn18_fkd/runs/wzy4pmy8
======= FKD: dataset info ======
path: ../relabel/FKD_cutmix_fp16/tiny-imagenet/237/
num img: 2000
batch size: 100
max epoch: 300
================================
load data successfully
=> loading student model 'resnet18'

Epoch: 0
TRAIN Iter 0: lr = 0.001000,	loss = 0.009046,	Top-1 err = 99.000000,	Top-5 err = 95.850000,	train_time = 15.784136
TEST Iter 0: loss = 10.707317,	Top-1 err = 99.240000,	Top-5 err = 96.270000,	val_time = 17.955293

Epoch: 1

Epoch: 2

Epoch: 3

Epoch: 4

Epoch: 5

Epoch: 6

Epoch: 7

Epoch: 8

Epoch: 9

Epoch: 10
TRAIN Iter 10: lr = 0.000997,	loss = 0.006974,	Top-1 err = 97.750000,	Top-5 err = 91.500000,	train_time = 14.917237
TEST Iter 10: loss = 5.295267,	Top-1 err = 97.090000,	Top-5 err = 88.690000,	val_time = 18.065386

Epoch: 11

Epoch: 12

Epoch: 13

Epoch: 14

Epoch: 15

Epoch: 16

Epoch: 17

Epoch: 18

Epoch: 19

Epoch: 20
TRAIN Iter 20: lr = 0.000989,	loss = 0.006159,	Top-1 err = 96.150000,	Top-5 err = 84.500000,	train_time = 14.791523
TEST Iter 20: loss = 5.063441,	Top-1 err = 95.220000,	Top-5 err = 83.260000,	val_time = 18.040169

Epoch: 21

Epoch: 22

Epoch: 23

Epoch: 24

Epoch: 25

Epoch: 26

Epoch: 27

Epoch: 28

Epoch: 29

Epoch: 30
TRAIN Iter 30: lr = 0.000976,	loss = 0.005623,	Top-1 err = 94.200000,	Top-5 err = 81.300000,	train_time = 14.778284
TEST Iter 30: loss = 4.936121,	Top-1 err = 94.340000,	Top-5 err = 79.750000,	val_time = 18.054168

Epoch: 31

Epoch: 32

Epoch: 33

Epoch: 34

Epoch: 35

Epoch: 36

Epoch: 37

Epoch: 38

Epoch: 39

Epoch: 40
TRAIN Iter 40: lr = 0.000957,	loss = 0.005366,	Top-1 err = 90.400000,	Top-5 err = 72.200000,	train_time = 14.818276
TEST Iter 40: loss = 4.948733,	Top-1 err = 93.350000,	Top-5 err = 78.710000,	val_time = 17.686386

Epoch: 41

Epoch: 42

Epoch: 43

Epoch: 44

Epoch: 45

Epoch: 46

Epoch: 47

Epoch: 48

Epoch: 49

Epoch: 50
TRAIN Iter 50: lr = 0.000933,	loss = 0.004785,	Top-1 err = 88.800000,	Top-5 err = 71.000000,	train_time = 15.168281
TEST Iter 50: loss = 4.270422,	Top-1 err = 89.520000,	Top-5 err = 69.320000,	val_time = 17.731916

Epoch: 51

Epoch: 52

Epoch: 53

Epoch: 54

Epoch: 55

Epoch: 56

Epoch: 57

Epoch: 58

Epoch: 59

Epoch: 60
TRAIN Iter 60: lr = 0.000905,	loss = 0.004811,	Top-1 err = 85.950000,	Top-5 err = 65.050000,	train_time = 15.026019
TEST Iter 60: loss = 5.123844,	Top-1 err = 90.050000,	Top-5 err = 72.380000,	val_time = 17.998787

Epoch: 61

Epoch: 62

Epoch: 63

Epoch: 64

Epoch: 65

Epoch: 66

Epoch: 67

Epoch: 68

Epoch: 69

Epoch: 70
TRAIN Iter 70: lr = 0.000872,	loss = 0.004416,	Top-1 err = 87.500000,	Top-5 err = 68.350000,	train_time = 18.688356
TEST Iter 70: loss = 5.194252,	Top-1 err = 89.400000,	Top-5 err = 70.430000,	val_time = 25.833268

Epoch: 71

Epoch: 72

Epoch: 73

Epoch: 74

Epoch: 75

Epoch: 76

Epoch: 77

Epoch: 78

Epoch: 79

Epoch: 80
TRAIN Iter 80: lr = 0.000835,	loss = 0.004270,	Top-1 err = 82.650000,	Top-5 err = 58.450000,	train_time = 15.098034
TEST Iter 80: loss = 4.685121,	Top-1 err = 87.340000,	Top-5 err = 66.810000,	val_time = 18.546967

Epoch: 81

Epoch: 82

Epoch: 83

Epoch: 84

Epoch: 85

Epoch: 86

Epoch: 87

Epoch: 88

Epoch: 89

Epoch: 90
TRAIN Iter 90: lr = 0.000794,	loss = 0.003944,	Top-1 err = 83.650000,	Top-5 err = 64.450000,	train_time = 15.210045
TEST Iter 90: loss = 4.599945,	Top-1 err = 86.640000,	Top-5 err = 64.270000,	val_time = 17.882610

Epoch: 91

Epoch: 92

Epoch: 93

Epoch: 94

Epoch: 95

Epoch: 96

Epoch: 97

Epoch: 98

Epoch: 99

Epoch: 100
TRAIN Iter 100: lr = 0.000750,	loss = 0.003895,	Top-1 err = 77.100000,	Top-5 err = 52.150000,	train_time = 15.426784
TEST Iter 100: loss = 3.809839,	Top-1 err = 81.560000,	Top-5 err = 54.820000,	val_time = 17.971170

Epoch: 101

Epoch: 102

Epoch: 103

Epoch: 104

Epoch: 105

Epoch: 106

Epoch: 107

Epoch: 108

Epoch: 109

Epoch: 110
TRAIN Iter 110: lr = 0.000703,	loss = 0.003800,	Top-1 err = 75.050000,	Top-5 err = 51.550000,	train_time = 15.033139
TEST Iter 110: loss = 3.801695,	Top-1 err = 81.020000,	Top-5 err = 55.610000,	val_time = 17.788601

Epoch: 111

Epoch: 112

Epoch: 113

Epoch: 114

Epoch: 115

Epoch: 116

Epoch: 117

Epoch: 118

Epoch: 119

Epoch: 120
TRAIN Iter 120: lr = 0.000655,	loss = 0.003560,	Top-1 err = 71.750000,	Top-5 err = 44.600000,	train_time = 15.015410
TEST Iter 120: loss = 3.770666,	Top-1 err = 80.180000,	Top-5 err = 53.180000,	val_time = 17.884776

Epoch: 121

Epoch: 122

Epoch: 123

Epoch: 124

Epoch: 125

Epoch: 126

Epoch: 127

Epoch: 128

Epoch: 129

Epoch: 130
TRAIN Iter 130: lr = 0.000604,	loss = 0.003290,	Top-1 err = 70.700000,	Top-5 err = 45.600000,	train_time = 15.090045
TEST Iter 130: loss = 3.854507,	Top-1 err = 79.580000,	Top-5 err = 53.220000,	val_time = 18.040233

Epoch: 131

Epoch: 132

Epoch: 133

Epoch: 134

Epoch: 135

Epoch: 136

Epoch: 137

Epoch: 138

Epoch: 139

Epoch: 140
TRAIN Iter 140: lr = 0.000552,	loss = 0.003205,	Top-1 err = 70.000000,	Top-5 err = 44.300000,	train_time = 15.116434
TEST Iter 140: loss = 3.554635,	Top-1 err = 76.440000,	Top-5 err = 49.450000,	val_time = 17.774909

Epoch: 141

Epoch: 142

Epoch: 143

Epoch: 144

Epoch: 145

Epoch: 146

Epoch: 147

Epoch: 148

Epoch: 149

Epoch: 150
TRAIN Iter 150: lr = 0.000500,	loss = 0.003128,	Top-1 err = 68.500000,	Top-5 err = 46.850000,	train_time = 15.356399
TEST Iter 150: loss = 3.638570,	Top-1 err = 75.650000,	Top-5 err = 48.260000,	val_time = 17.850147

Epoch: 151

Epoch: 152

Epoch: 153

Epoch: 154

Epoch: 155

Epoch: 156

Epoch: 157

Epoch: 158

Epoch: 159

Epoch: 160
TRAIN Iter 160: lr = 0.000448,	loss = 0.002951,	Top-1 err = 64.000000,	Top-5 err = 39.050000,	train_time = 15.047313
TEST Iter 160: loss = 3.148321,	Top-1 err = 71.260000,	Top-5 err = 42.590000,	val_time = 17.944142

Epoch: 161

Epoch: 162

Epoch: 163

Epoch: 164

Epoch: 165

Epoch: 166

Epoch: 167

Epoch: 168

Epoch: 169

Epoch: 170
TRAIN Iter 170: lr = 0.000396,	loss = 0.002949,	Top-1 err = 67.150000,	Top-5 err = 43.800000,	train_time = 15.400409
TEST Iter 170: loss = 3.191600,	Top-1 err = 70.860000,	Top-5 err = 41.640000,	val_time = 18.171941

Epoch: 171

Epoch: 172

Epoch: 173

Epoch: 174

Epoch: 175

Epoch: 176

Epoch: 177

Epoch: 178

Epoch: 179

Epoch: 180
TRAIN Iter 180: lr = 0.000345,	loss = 0.002761,	Top-1 err = 59.350000,	Top-5 err = 36.750000,	train_time = 14.975597
TEST Iter 180: loss = 3.005802,	Top-1 err = 68.950000,	Top-5 err = 39.470000,	val_time = 17.781568

Epoch: 181

Epoch: 182

Epoch: 183

Epoch: 184

Epoch: 185

Epoch: 186

Epoch: 187

Epoch: 188

Epoch: 189

Epoch: 190
TRAIN Iter 190: lr = 0.000297,	loss = 0.002733,	Top-1 err = 63.750000,	Top-5 err = 40.100000,	train_time = 15.026478
TEST Iter 190: loss = 2.926152,	Top-1 err = 67.960000,	Top-5 err = 38.350000,	val_time = 17.937305

Epoch: 191

Epoch: 192

Epoch: 193

Epoch: 194

Epoch: 195

Epoch: 196

Epoch: 197

Epoch: 198

Epoch: 199

Epoch: 200
TRAIN Iter 200: lr = 0.000250,	loss = 0.002659,	Top-1 err = 66.650000,	Top-5 err = 43.850000,	train_time = 15.096869
TEST Iter 200: loss = 2.958918,	Top-1 err = 68.330000,	Top-5 err = 38.170000,	val_time = 17.815053

Epoch: 201

Epoch: 202

Epoch: 203

Epoch: 204

Epoch: 205

Epoch: 206

Epoch: 207

Epoch: 208

Epoch: 209

Epoch: 210
TRAIN Iter 210: lr = 0.000206,	loss = 0.002618,	Top-1 err = 58.300000,	Top-5 err = 34.300000,	train_time = 15.012795
TEST Iter 210: loss = 2.885486,	Top-1 err = 66.620000,	Top-5 err = 36.950000,	val_time = 17.744690

Epoch: 211

Epoch: 212

Epoch: 213

Epoch: 214

Epoch: 215

Epoch: 216

Epoch: 217

Epoch: 218

Epoch: 219

Epoch: 220
TRAIN Iter 220: lr = 0.000165,	loss = 0.002546,	Top-1 err = 67.950000,	Top-5 err = 48.100000,	train_time = 15.032217
TEST Iter 220: loss = 2.871221,	Top-1 err = 66.420000,	Top-5 err = 36.620000,	val_time = 17.837987

Epoch: 221

Epoch: 222

Epoch: 223

Epoch: 224

Epoch: 225

Epoch: 226

Epoch: 227

Epoch: 228

Epoch: 229

Epoch: 230
TRAIN Iter 230: lr = 0.000128,	loss = 0.002545,	Top-1 err = 67.400000,	Top-5 err = 45.350000,	train_time = 15.049503
TEST Iter 230: loss = 2.743189,	Top-1 err = 64.270000,	Top-5 err = 34.330000,	val_time = 17.809443

Epoch: 231

Epoch: 232

Epoch: 233

Epoch: 234

Epoch: 235

Epoch: 236

Epoch: 237

Epoch: 238

Epoch: 239

Epoch: 240
TRAIN Iter 240: lr = 0.000095,	loss = 0.002431,	Top-1 err = 57.400000,	Top-5 err = 37.250000,	train_time = 14.881740
TEST Iter 240: loss = 2.702382,	Top-1 err = 63.970000,	Top-5 err = 34.430000,	val_time = 17.744518

Epoch: 241

Epoch: 242

Epoch: 243

Epoch: 244

Epoch: 245

Epoch: 246

Epoch: 247

Epoch: 248

Epoch: 249

Epoch: 250
TRAIN Iter 250: lr = 0.000067,	loss = 0.002429,	Top-1 err = 55.050000,	Top-5 err = 33.350000,	train_time = 15.046666
TEST Iter 250: loss = 2.703792,	Top-1 err = 63.830000,	Top-5 err = 33.830000,	val_time = 17.731281

Epoch: 251

Epoch: 252

Epoch: 253

Epoch: 254

Epoch: 255

Epoch: 256

Epoch: 257

Epoch: 258

Epoch: 259

Epoch: 260
TRAIN Iter 260: lr = 0.000043,	loss = 0.002421,	Top-1 err = 52.500000,	Top-5 err = 31.250000,	train_time = 15.046179
TEST Iter 260: loss = 2.651926,	Top-1 err = 63.160000,	Top-5 err = 33.300000,	val_time = 17.725647

Epoch: 261

Epoch: 262

Epoch: 263

Epoch: 264

Epoch: 265

Epoch: 266

Epoch: 267

Epoch: 268

Epoch: 269

Epoch: 270
TRAIN Iter 270: lr = 0.000024,	loss = 0.002427,	Top-1 err = 62.300000,	Top-5 err = 39.100000,	train_time = 14.975166
TEST Iter 270: loss = 2.640618,	Top-1 err = 62.490000,	Top-5 err = 32.970000,	val_time = 17.981766

Epoch: 271

Epoch: 272

Epoch: 273

Epoch: 274

Epoch: 275

Epoch: 276

Epoch: 277

Epoch: 278

Epoch: 279

Epoch: 280
TRAIN Iter 280: lr = 0.000011,	loss = 0.002409,	Top-1 err = 55.900000,	Top-5 err = 38.550000,	train_time = 15.350155
TEST Iter 280: loss = 2.620885,	Top-1 err = 62.370000,	Top-5 err = 32.550000,	val_time = 17.890403

Epoch: 281

Epoch: 282

Epoch: 283

Epoch: 284

Epoch: 285

Epoch: 286

Epoch: 287

Epoch: 288

Epoch: 289

Epoch: 290
TRAIN Iter 290: lr = 0.000003,	loss = 0.002388,	Top-1 err = 52.950000,	Top-5 err = 31.250000,	train_time = 16.050617
TEST Iter 290: loss = 2.628274,	Top-1 err = 62.390000,	Top-5 err = 32.730000,	val_time = 17.993743

Epoch: 291

Epoch: 292

Epoch: 293

Epoch: 294

Epoch: 295

Epoch: 296

Epoch: 297

Epoch: 298

Epoch: 299
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.029 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.029 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:  train/Top1 ▁▁▁▂▂▂▂▃▄▃▄▃▄▅▆▄▅▆▆▇▇▆▄▇▆▅▆▇▇▇▆▇▇█▇███▇▆
wandb:  train/Top5 ▁▁▂▂▃▃▃▄▅▄▅▅▅▆▇▆▆▇▆▇▇▇▅▇▆▆▇▇▆▆▆▇▇█▇███▇▆
wandb: train/epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  train/loss ██▆▆▆▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    train/lr ███████▇▇▇▇▇▇▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁
wandb:   val/epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███
wandb:    val/loss █▃▃▃▃▂▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    val/top1 ▁▁▂▂▂▃▃▃▃▃▄▄▅▅▅▅▆▆▇▇▇▇▇████████
wandb:    val/top5 ▁▂▂▃▃▄▄▄▄▅▆▅▆▆▆▆▇▇▇▇▇██████████
wandb: 
wandb: Run summary:
wandb:  train/Top1 31.5
wandb:  train/Top5 54.4
wandb: train/epoch 299
wandb:  train/loss 0.00237
wandb:    train/lr 0.0
wandb:   val/epoch 299
wandb:    val/loss 2.61713
wandb:    val/top1 37.92
wandb:    val/top5 67.53
wandb: 
wandb: 🚀 View run earthy-music-404 at: https://wandb.ai/hl57/final_rn18_fkd/runs/wzy4pmy8
wandb: ️⚡ View job at https://wandb.ai/hl57/final_rn18_fkd/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjk3MzkyNjU3/version_details/v37
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231007_102316-wzy4pmy8/logs
TEST Iter 299: loss = 2.617133,	Top-1 err = 62.080000,	Top-5 err = 32.470000,	val_time = 18.228581
