r_bn:  3.0
lr:  0.25
Computing sample features for class 0
Computing sample features for class 1
Computing sample features for class 2
Computing sample features for class 3
Computing sample features for class 4
Computing sample features for class 5
Computing sample features for class 6
Computing sample features for class 7
Computing sample features for class 8
Computing sample features for class 9
bc shape (10, 50, 512)
Computing BatchNorm statistics for class 0
Computing BatchNorm statistics for class 1
Computing BatchNorm statistics for class 2
Computing BatchNorm statistics for class 3
Computing BatchNorm statistics for class 4
Computing BatchNorm statistics for class 5
Computing BatchNorm statistics for class 6
Computing BatchNorm statistics for class 7
Computing BatchNorm statistics for class 8
Computing BatchNorm statistics for class 9
/home/techt/anaconda3/envs/hl/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/techt/anaconda3/envs/hl/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
/home/techt/anaconda3/envs/hl/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
get_images call
class_per_batch  2 batch_size  100 args.ipc  50
------------iteration 0----------
total loss 2886.766978811185
main criterion 105.31678349868501
weighted_aux_loss 2781.4501953125
loss_r_bn_feature 927.1500244140625
------------iteration 100----------
total loss 908.0099235561062
main criterion 44.88846359516867
weighted_aux_loss 863.1214599609375
loss_r_bn_feature 287.7071533203125
------------iteration 200----------
total loss 695.7551281363001
main criterion 38.12317012848762
weighted_aux_loss 657.6319580078125
loss_r_bn_feature 219.21066284179688
------------iteration 300----------
total loss 647.1297812766928
main criterion 39.2670493430991
weighted_aux_loss 607.8627319335938
loss_r_bn_feature 202.62091064453125
------------iteration 400----------
total loss 592.5706111993828
main criterion 38.97765465641411
weighted_aux_loss 553.5929565429688
loss_r_bn_feature 184.53099060058594
------------iteration 500----------
total loss 512.6048930290689
main criterion 35.18127852711577
weighted_aux_loss 477.4236145019531
loss_r_bn_feature 159.14120483398438
------------iteration 600----------
total loss 533.7456853563111
main criterion 37.99007012193608
weighted_aux_loss 495.755615234375
loss_r_bn_feature 165.2518768310547
------------iteration 700----------
total loss 437.0043123513003
main criterion 33.528573825909675
weighted_aux_loss 403.4757385253906
loss_r_bn_feature 134.49191284179688
------------iteration 800----------
total loss 396.60432232330925
main criterion 33.86829937409052
weighted_aux_loss 362.73602294921875
loss_r_bn_feature 120.9120101928711
------------iteration 900----------
total loss 349.7232138992356
main criterion 31.12696145782935
weighted_aux_loss 318.59625244140625
loss_r_bn_feature 106.1987533569336
------------iteration 1000----------
total loss 411.9678000466996
main criterion 35.15951147248087
weighted_aux_loss 376.80828857421875
loss_r_bn_feature 125.6027603149414
------------iteration 1100----------
total loss 375.3009851162741
main criterion 37.46178223541469
weighted_aux_loss 337.8392028808594
loss_r_bn_feature 112.61306762695312
------------iteration 1200----------
total loss 231.87064418805568
main criterion 28.949089622626005
weighted_aux_loss 202.9215545654297
loss_r_bn_feature 67.64051818847656
------------iteration 1300----------
total loss 218.04510685162393
main criterion 27.2599963779911
weighted_aux_loss 190.7851104736328
loss_r_bn_feature 63.595035552978516
------------iteration 1400----------
total loss 222.4483279704897
main criterion 26.059061002716252
weighted_aux_loss 196.38926696777344
loss_r_bn_feature 65.46308898925781
------------iteration 1500----------
total loss 211.40160185130824
main criterion 29.10814482005824
weighted_aux_loss 182.29345703125
loss_r_bn_feature 60.764488220214844
------------iteration 1600----------
total loss 251.9807397534132
main criterion 32.95846192138194
weighted_aux_loss 219.02227783203125
loss_r_bn_feature 73.0074234008789
------------iteration 1700----------
total loss 138.0463193480877
main criterion 23.455781323185352
weighted_aux_loss 114.59053802490234
loss_r_bn_feature 38.19684600830078
------------iteration 1800----------
total loss 181.73599870613026
main criterion 28.864019946364643
weighted_aux_loss 152.87197875976562
loss_r_bn_feature 50.95732498168945
------------iteration 1900----------
total loss 162.74199710711213
main criterion 27.426796301448057
weighted_aux_loss 135.31520080566406
loss_r_bn_feature 45.10506820678711
------------iteration 0----------
total loss 3290.804856073874
main criterion 83.53093029262409
weighted_aux_loss 3207.27392578125
loss_r_bn_feature 1069.09130859375
------------iteration 100----------
total loss 1001.6359271602903
main criterion 51.65893741419652
weighted_aux_loss 949.9769897460938
loss_r_bn_feature 316.65899658203125
------------iteration 200----------
total loss 588.506906735754
main criterion 31.995798337316458
weighted_aux_loss 556.5111083984375
loss_r_bn_feature 185.50369262695312
------------iteration 300----------
total loss 549.585376864739
main criterion 32.32359707958277
weighted_aux_loss 517.2617797851562
loss_r_bn_feature 172.42059326171875
------------iteration 400----------
total loss 546.2931760293304
main criterion 33.449120853549225
weighted_aux_loss 512.8440551757812
loss_r_bn_feature 170.94801330566406
------------iteration 500----------
total loss 566.1299550182711
main criterion 36.07502337764608
weighted_aux_loss 530.054931640625
loss_r_bn_feature 176.6849822998047
------------iteration 600----------
total loss 496.6562811973773
main criterion 31.2980048301898
weighted_aux_loss 465.3582763671875
loss_r_bn_feature 155.1194305419922
------------iteration 700----------
total loss 557.0320852141903
main criterion 34.14536646419022
weighted_aux_loss 522.88671875
loss_r_bn_feature 174.29556274414062
------------iteration 800----------
total loss 423.94949827760183
main criterion 32.50745115846124
weighted_aux_loss 391.4420471191406
loss_r_bn_feature 130.48068237304688
------------iteration 900----------
total loss 428.94365928656873
main criterion 29.078546981881207
weighted_aux_loss 399.8651123046875
loss_r_bn_feature 133.2883758544922
------------iteration 1000----------
total loss 421.97478536148645
main criterion 37.5732350685177
weighted_aux_loss 384.40155029296875
loss_r_bn_feature 128.13385009765625
------------iteration 1100----------
total loss 322.606671190336
main criterion 29.030804491117266
weighted_aux_loss 293.57586669921875
loss_r_bn_feature 97.8586196899414
------------iteration 1200----------
total loss 320.12276774221846
main criterion 28.193660076202814
weighted_aux_loss 291.9291076660156
loss_r_bn_feature 97.30970001220703
------------iteration 1300----------
total loss 248.57190940053607
main criterion 25.725550147606395
weighted_aux_loss 222.8463592529297
loss_r_bn_feature 74.28211975097656
------------iteration 1400----------
total loss 284.67023618230115
main criterion 31.942849707691764
weighted_aux_loss 252.72738647460938
loss_r_bn_feature 84.24246215820312
------------iteration 1500----------
total loss 232.54095412862745
main criterion 33.85984756124463
weighted_aux_loss 198.6811065673828
loss_r_bn_feature 66.22703552246094
------------iteration 1600----------
total loss 150.9543290257806
main criterion 23.701490891014966
weighted_aux_loss 127.25283813476562
loss_r_bn_feature 42.4176139831543
------------iteration 1700----------
total loss 152.15516974553816
main criterion 24.340434332940514
weighted_aux_loss 127.81473541259766
loss_r_bn_feature 42.60491180419922
------------iteration 1800----------
total loss 189.6760957814309
main criterion 27.07250386248558
weighted_aux_loss 162.6035919189453
loss_r_bn_feature 54.20119857788086
------------iteration 1900----------
total loss 556.0151099510626
main criterion 47.59415047840636
weighted_aux_loss 508.42095947265625
loss_r_bn_feature 169.47364807128906
------------iteration 0----------
total loss 3354.3071851626455
main criterion 92.92266367827047
weighted_aux_loss 3261.384521484375
loss_r_bn_feature 1087.128173828125
------------iteration 100----------
total loss 915.6281034805945
main criterion 42.650259242313304
weighted_aux_loss 872.9778442382812
loss_r_bn_feature 290.99261474609375
------------iteration 200----------
total loss 977.1124239810017
main criterion 51.809994781783004
weighted_aux_loss 925.3024291992188
loss_r_bn_feature 308.43414306640625
------------iteration 300----------
total loss 606.4708438567841
main criterion 34.535052841159114
weighted_aux_loss 571.935791015625
loss_r_bn_feature 190.645263671875
------------iteration 400----------
total loss 588.1312614841028
main criterion 35.777867929415216
weighted_aux_loss 552.3533935546875
loss_r_bn_feature 184.1177978515625
------------iteration 500----------
total loss 628.775704379288
main criterion 35.20685672303795
weighted_aux_loss 593.56884765625
loss_r_bn_feature 197.8562774658203
------------iteration 600----------
total loss 725.9368552341081
main criterion 49.60519019504558
weighted_aux_loss 676.3316650390625
loss_r_bn_feature 225.44387817382812
------------iteration 700----------
total loss 598.761287232324
main criterion 41.19720031826147
weighted_aux_loss 557.5640869140625
loss_r_bn_feature 185.8546905517578
------------iteration 800----------
total loss 463.0781110181304
main criterion 38.81715520758352
weighted_aux_loss 424.2609558105469
loss_r_bn_feature 141.42031860351562
------------iteration 900----------
total loss 513.9852201113528
main criterion 36.46163002346214
weighted_aux_loss 477.5235900878906
loss_r_bn_feature 159.17453002929688
------------iteration 1000----------
total loss 400.7989113842171
main criterion 30.28279810296709
weighted_aux_loss 370.51611328125
loss_r_bn_feature 123.50537109375
------------iteration 1100----------
total loss 340.2051836805317
main criterion 28.176314051625475
weighted_aux_loss 312.02886962890625
loss_r_bn_feature 104.0096206665039
------------iteration 1200----------
total loss 342.0904397639954
main criterion 30.08100983235475
weighted_aux_loss 312.0094299316406
loss_r_bn_feature 104.00314331054688
------------iteration 1300----------
total loss 407.184987033271
main criterion 37.28795334186474
weighted_aux_loss 369.89703369140625
loss_r_bn_feature 123.29901123046875
------------iteration 1400----------
total loss 619.5399175139547
main criterion 52.7972417327047
weighted_aux_loss 566.74267578125
loss_r_bn_feature 188.9142303466797
------------iteration 1500----------
total loss 270.3380375359327
main criterion 26.489648864057674
weighted_aux_loss 243.848388671875
loss_r_bn_feature 81.28279876708984
------------iteration 1600----------
total loss 290.10359249148917
main criterion 35.770889854770395
weighted_aux_loss 254.33270263671875
loss_r_bn_feature 84.7775650024414
------------iteration 1700----------
total loss 201.05344674920372
main criterion 25.598582247250587
weighted_aux_loss 175.45486450195312
loss_r_bn_feature 58.484954833984375
------------iteration 1800----------
total loss 270.0983628469524
main criterion 29.66765300808521
weighted_aux_loss 240.4307098388672
loss_r_bn_feature 80.14356994628906
------------iteration 1900----------
total loss 157.85724668139704
main criterion 23.513649269287658
weighted_aux_loss 134.34359741210938
loss_r_bn_feature 44.78119659423828
------------iteration 0----------
total loss 3346.7202938635564
main criterion 98.18074308230662
weighted_aux_loss 3248.53955078125
loss_r_bn_feature 1082.8465576171875
------------iteration 100----------
total loss 879.1170464105506
main criterion 36.18333059023819
weighted_aux_loss 842.9337158203125
loss_r_bn_feature 280.9779052734375
------------iteration 200----------
total loss 690.284240494301
main criterion 32.87835670523852
weighted_aux_loss 657.4058837890625
loss_r_bn_feature 219.1352996826172
------------iteration 300----------
total loss 664.2321327104701
main criterion 43.605912007345125
weighted_aux_loss 620.626220703125
loss_r_bn_feature 206.8754119873047
------------iteration 400----------
total loss 554.6902243318332
main criterion 34.975075406051985
weighted_aux_loss 519.7151489257812
loss_r_bn_feature 173.23838806152344
------------iteration 500----------
total loss 920.538608896647
main criterion 60.42972217789705
weighted_aux_loss 860.10888671875
loss_r_bn_feature 286.7029724121094
------------iteration 600----------
total loss 499.8691799255914
main criterion 37.43842431035705
weighted_aux_loss 462.4307556152344
loss_r_bn_feature 154.14358520507812
------------iteration 700----------
total loss 472.39929401900383
main criterion 34.99759113814444
weighted_aux_loss 437.4017028808594
loss_r_bn_feature 145.80056762695312
------------iteration 800----------
total loss 386.6860805329557
main criterion 30.59199483959632
weighted_aux_loss 356.0940856933594
loss_r_bn_feature 118.69802856445312
------------iteration 900----------
total loss 439.05988224964636
main criterion 30.01114567738072
weighted_aux_loss 409.0487365722656
loss_r_bn_feature 136.34957885742188
------------iteration 1000----------
total loss 546.620683329692
main criterion 48.021470683207696
weighted_aux_loss 498.5992126464844
loss_r_bn_feature 166.19973754882812
------------iteration 1100----------
total loss 263.3035035008352
main criterion 27.067205893413288
weighted_aux_loss 236.23629760742188
loss_r_bn_feature 78.74542999267578
------------iteration 1200----------
total loss 296.16987661647397
main criterion 25.524673979755235
weighted_aux_loss 270.64520263671875
loss_r_bn_feature 90.21507263183594
------------iteration 1300----------
total loss 237.92380338408026
main criterion 26.194189126267762
weighted_aux_loss 211.7296142578125
loss_r_bn_feature 70.5765380859375
------------iteration 1400----------
total loss 229.26947314188223
main criterion 28.876833981725976
weighted_aux_loss 200.39263916015625
loss_r_bn_feature 66.79754638671875
------------iteration 1500----------
total loss 188.7340357835782
main criterion 25.656887346078207
weighted_aux_loss 163.0771484375
loss_r_bn_feature 54.359046936035156
------------iteration 1600----------
total loss 320.6441238066982
main criterion 33.53792263482317
weighted_aux_loss 287.106201171875
loss_r_bn_feature 95.70207214355469
------------iteration 1700----------
total loss 175.16763126164037
main criterion 25.626005285077866
weighted_aux_loss 149.5416259765625
loss_r_bn_feature 49.847206115722656
------------iteration 1800----------
total loss 416.2636475231343
main criterion 43.72083136102492
weighted_aux_loss 372.5428161621094
loss_r_bn_feature 124.18093872070312
------------iteration 1900----------
total loss 128.21194745564665
main criterion 22.622134223224776
weighted_aux_loss 105.58981323242188
loss_r_bn_feature 35.19660568237305
------------iteration 0----------
total loss 3453.1468295002487
main criterion 92.06797207837349
weighted_aux_loss 3361.078857421875
loss_r_bn_feature 1120.359619140625
------------iteration 100----------
total loss 1176.3013409409891
main criterion 44.4759014878641
weighted_aux_loss 1131.825439453125
loss_r_bn_feature 377.275146484375
------------iteration 200----------
total loss 1287.7875000565562
main criterion 54.144677790931105
weighted_aux_loss 1233.642822265625
loss_r_bn_feature 411.21429443359375
------------iteration 300----------
total loss 952.0966665153522
main criterion 42.074937999727226
weighted_aux_loss 910.021728515625
loss_r_bn_feature 303.340576171875
------------iteration 400----------
total loss 955.8861742613187
main criterion 43.324406683193715
weighted_aux_loss 912.561767578125
loss_r_bn_feature 304.187255859375
------------iteration 500----------
total loss 886.2001113180776
main criterion 42.377723622765025
weighted_aux_loss 843.8223876953125
loss_r_bn_feature 281.2741394042969
------------iteration 600----------
total loss 1308.2768409696878
main criterion 67.55491714156275
weighted_aux_loss 1240.721923828125
loss_r_bn_feature 413.573974609375
------------iteration 700----------
total loss 805.8152400091633
main criterion 41.53875075135077
weighted_aux_loss 764.2764892578125
loss_r_bn_feature 254.75881958007812
------------iteration 800----------
total loss 972.212563458062
main criterion 44.776223126030786
weighted_aux_loss 927.4363403320312
loss_r_bn_feature 309.14544677734375
------------iteration 900----------
total loss 754.8407388860678
main criterion 40.800638788411575
weighted_aux_loss 714.0401000976562
loss_r_bn_feature 238.01336669921875
------------iteration 1000----------
total loss 769.9761190573674
main criterion 44.14982511205489
weighted_aux_loss 725.8262939453125
loss_r_bn_feature 241.9420928955078
------------iteration 1100----------
total loss 546.3282005596963
main criterion 37.61149523743064
weighted_aux_loss 508.7167053222656
loss_r_bn_feature 169.57223510742188
------------iteration 1200----------
total loss 553.1154150136813
main criterion 37.99236813868128
weighted_aux_loss 515.123046875
loss_r_bn_feature 171.7076873779297
------------iteration 1300----------
total loss 490.2573586865762
main criterion 37.677585737357454
weighted_aux_loss 452.57977294921875
loss_r_bn_feature 150.85992431640625
------------iteration 1400----------
total loss 464.84511200898777
main criterion 40.02641694062841
weighted_aux_loss 424.8186950683594
loss_r_bn_feature 141.60623168945312
------------iteration 1500----------
total loss 429.78892553231947
main criterion 35.09877050302257
weighted_aux_loss 394.6901550292969
loss_r_bn_feature 131.56338500976562
------------iteration 1600----------
total loss 318.09196004299645
main criterion 34.16038045315272
weighted_aux_loss 283.93157958984375
loss_r_bn_feature 94.64385986328125
------------iteration 1700----------
total loss 267.3169531177068
main criterion 33.12785094485527
weighted_aux_loss 234.18910217285156
loss_r_bn_feature 78.06303405761719
------------iteration 1800----------
total loss 257.90189350477647
main criterion 33.259193309463946
weighted_aux_loss 224.6427001953125
loss_r_bn_feature 74.88089752197266
------------iteration 1900----------
total loss 375.73917224147357
main criterion 41.26010730006733
weighted_aux_loss 334.47906494140625
loss_r_bn_feature 111.49302673339844
=> using pytorch pre-trained model 'resnet18'
process data from ../recover/syn_data/imagenette/341
  0%|          | 0/300 [00:00<?, ?it/s]  0%|          | 1/300 [00:02<11:33,  2.32s/it]  1%|          | 2/300 [00:03<07:06,  1.43s/it]  1%|          | 3/300 [00:03<05:41,  1.15s/it]  1%|▏         | 4/300 [00:04<04:55,  1.00it/s]  2%|▏         | 5/300 [00:05<04:36,  1.07it/s]  2%|▏         | 6/300 [00:06<04:18,  1.14it/s]  2%|▏         | 7/300 [00:07<04:09,  1.17it/s]  3%|▎         | 8/300 [00:07<04:03,  1.20it/s]  3%|▎         | 9/300 [00:08<04:00,  1.21it/s]  3%|▎         | 10/300 [00:09<03:54,  1.24it/s]  4%|▎         | 11/300 [00:10<03:56,  1.22it/s]  4%|▍         | 12/300 [00:11<03:55,  1.22it/s]  4%|▍         | 13/300 [00:11<03:52,  1.24it/s]  5%|▍         | 14/300 [00:12<03:47,  1.26it/s]  5%|▌         | 15/300 [00:13<03:48,  1.25it/s]  5%|▌         | 16/300 [00:14<03:50,  1.23it/s]  6%|▌         | 17/300 [00:15<03:44,  1.26it/s]  6%|▌         | 18/300 [00:15<03:43,  1.26it/s]  6%|▋         | 19/300 [00:16<03:41,  1.27it/s]  7%|▋         | 20/300 [00:17<03:46,  1.24it/s]  7%|▋         | 21/300 [00:18<03:47,  1.23it/s]  7%|▋         | 22/300 [00:19<03:44,  1.24it/s]  8%|▊         | 23/300 [00:19<03:48,  1.21it/s]  8%|▊         | 24/300 [00:20<03:43,  1.24it/s]  8%|▊         | 25/300 [00:21<03:43,  1.23it/s]  9%|▊         | 26/300 [00:22<03:44,  1.22it/s]  9%|▉         | 27/300 [00:23<03:40,  1.24it/s]  9%|▉         | 28/300 [00:23<03:38,  1.24it/s] 10%|▉         | 29/300 [00:24<03:37,  1.25it/s] 10%|█         | 30/300 [00:25<03:37,  1.24it/s] 10%|█         | 31/300 [00:26<03:37,  1.24it/s] 11%|█         | 32/300 [00:27<03:36,  1.24it/s] 11%|█         | 33/300 [00:28<03:35,  1.24it/s] 11%|█▏        | 34/300 [00:28<03:37,  1.22it/s] 12%|█▏        | 35/300 [00:29<03:38,  1.21it/s] 12%|█▏        | 36/300 [00:30<03:37,  1.22it/s] 12%|█▏        | 37/300 [00:31<03:38,  1.21it/s] 13%|█▎        | 38/300 [00:32<03:33,  1.22it/s] 13%|█▎        | 39/300 [00:32<03:30,  1.24it/s] 13%|█▎        | 40/300 [00:33<03:28,  1.25it/s] 14%|█▎        | 41/300 [00:34<03:26,  1.26it/s] 14%|█▍        | 42/300 [00:35<03:26,  1.25it/s] 14%|█▍        | 43/300 [00:36<03:22,  1.27it/s] 15%|█▍        | 44/300 [00:37<03:31,  1.21it/s] 15%|█▌        | 45/300 [00:37<03:37,  1.17it/s] 15%|█▌        | 46/300 [00:38<03:30,  1.21it/s] 16%|█▌        | 47/300 [00:39<03:23,  1.24it/s] 16%|█▌        | 48/300 [00:40<03:22,  1.24it/s] 16%|█▋        | 49/300 [00:41<03:19,  1.26it/s] 17%|█▋        | 50/300 [00:41<03:17,  1.26it/s] 17%|█▋        | 51/300 [00:42<03:16,  1.27it/s] 17%|█▋        | 52/300 [00:43<03:18,  1.25it/s] 18%|█▊        | 53/300 [00:44<03:17,  1.25it/s] 18%|█▊        | 54/300 [00:44<03:15,  1.26it/s] 18%|█▊        | 55/300 [00:45<03:16,  1.25it/s] 19%|█▊        | 56/300 [00:46<03:18,  1.23it/s] 19%|█▉        | 57/300 [00:47<03:19,  1.22it/s] 19%|█▉        | 58/300 [00:48<03:14,  1.24it/s] 20%|█▉        | 59/300 [00:49<03:11,  1.26it/s] 20%|██        | 60/300 [00:49<03:08,  1.27it/s] 20%|██        | 61/300 [00:50<03:12,  1.24it/s] 21%|██        | 62/300 [00:51<03:10,  1.25it/s] 21%|██        | 63/300 [00:52<03:10,  1.24it/s] 21%|██▏       | 64/300 [00:53<03:08,  1.25it/s] 22%|██▏       | 65/300 [00:53<03:07,  1.25it/s] 22%|██▏       | 66/300 [00:54<03:06,  1.26it/s] 22%|██▏       | 67/300 [00:55<03:07,  1.24it/s] 23%|██▎       | 68/300 [00:56<03:08,  1.23it/s] 23%|██▎       | 69/300 [00:57<03:07,  1.23it/s] 23%|██▎       | 70/300 [00:57<03:05,  1.24it/s] 24%|██▎       | 71/300 [00:58<03:07,  1.22it/s] 24%|██▍       | 72/300 [00:59<03:07,  1.22it/s] 24%|██▍       | 73/300 [01:00<03:06,  1.21it/s] 25%|██▍       | 74/300 [01:01<03:03,  1.23it/s] 25%|██▌       | 75/300 [01:01<03:02,  1.24it/s] 25%|██▌       | 76/300 [01:02<02:58,  1.25it/s] 26%|██▌       | 77/300 [01:03<02:57,  1.25it/s] 26%|██▌       | 78/300 [01:04<02:56,  1.26it/s] 26%|██▋       | 79/300 [01:05<02:54,  1.27it/s] 27%|██▋       | 80/300 [01:05<02:54,  1.26it/s] 27%|██▋       | 81/300 [01:06<02:53,  1.26it/s] 27%|██▋       | 82/300 [01:07<02:54,  1.25it/s] 28%|██▊       | 83/300 [01:08<02:55,  1.23it/s] 28%|██▊       | 84/300 [01:09<02:58,  1.21it/s] 28%|██▊       | 85/300 [01:10<02:57,  1.21it/s] 29%|██▊       | 86/300 [01:10<02:54,  1.23it/s] 29%|██▉       | 87/300 [01:11<02:54,  1.22it/s] 29%|██▉       | 88/300 [01:12<02:50,  1.24it/s] 30%|██▉       | 89/300 [01:13<02:53,  1.22it/s] 30%|███       | 90/300 [01:14<02:47,  1.25it/s] 30%|███       | 91/300 [01:14<02:46,  1.26it/s] 31%|███       | 92/300 [01:15<02:46,  1.25it/s] 31%|███       | 93/300 [01:16<02:44,  1.26it/s] 31%|███▏      | 94/300 [01:17<02:45,  1.25it/s] 32%|███▏      | 95/300 [01:18<02:46,  1.23it/s] 32%|███▏      | 96/300 [01:18<02:47,  1.22it/s] 32%|███▏      | 97/300 [01:19<02:46,  1.22it/s] 33%|███▎      | 98/300 [01:20<02:44,  1.23it/s] 33%|███▎      | 99/300 [01:21<02:43,  1.23it/s] 33%|███▎      | 100/300 [01:22<02:39,  1.25it/s] 34%|███▎      | 101/300 [01:22<02:40,  1.24it/s] 34%|███▍      | 102/300 [01:23<02:40,  1.24it/s] 34%|███▍      | 103/300 [01:24<02:37,  1.25it/s] 35%|███▍      | 104/300 [01:25<02:38,  1.24it/s] 35%|███▌      | 105/300 [01:26<02:38,  1.23it/s] 35%|███▌      | 106/300 [01:26<02:36,  1.24it/s] 36%|███▌      | 107/300 [01:27<02:34,  1.25it/s] 36%|███▌      | 108/300 [01:28<02:33,  1.25it/s] 36%|███▋      | 109/300 [01:29<02:33,  1.24it/s] 37%|███▋      | 110/300 [01:30<02:34,  1.23it/s] 37%|███▋      | 111/300 [01:30<02:32,  1.24it/s] 37%|███▋      | 112/300 [01:31<02:30,  1.25it/s] 38%|███▊      | 113/300 [01:32<02:28,  1.26it/s] 38%|███▊      | 114/300 [01:33<02:28,  1.25it/s] 38%|███▊      | 115/300 [01:34<02:29,  1.24it/s] 39%|███▊      | 116/300 [01:34<02:26,  1.26it/s] 39%|███▉      | 117/300 [01:35<02:23,  1.28it/s] 39%|███▉      | 118/300 [01:36<02:22,  1.28it/s] 40%|███▉      | 119/300 [01:37<02:21,  1.28it/s] 40%|████      | 120/300 [01:38<02:22,  1.27it/s] 40%|████      | 121/300 [01:38<02:18,  1.29it/s] 41%|████      | 122/300 [01:39<02:17,  1.30it/s] 41%|████      | 123/300 [01:40<02:16,  1.30it/s] 41%|████▏     | 124/300 [01:41<02:16,  1.29it/s] 42%|████▏     | 125/300 [01:41<02:17,  1.27it/s] 42%|████▏     | 126/300 [01:42<02:17,  1.27it/s] 42%|████▏     | 127/300 [01:43<02:17,  1.26it/s] 43%|████▎     | 128/300 [01:44<02:18,  1.24it/s] 43%|████▎     | 129/300 [01:45<02:18,  1.24it/s] 43%|████▎     | 130/300 [01:46<02:17,  1.24it/s] 44%|████▎     | 131/300 [01:46<02:14,  1.26it/s] 44%|████▍     | 132/300 [01:47<02:14,  1.25it/s] 44%|████▍     | 133/300 [01:48<02:13,  1.25it/s] 45%|████▍     | 134/300 [01:49<02:12,  1.26it/s] 45%|████▌     | 135/300 [01:49<02:09,  1.28it/s] 45%|████▌     | 136/300 [01:50<02:08,  1.28it/s] 46%|████▌     | 137/300 [01:51<02:08,  1.27it/s] 46%|████▌     | 138/300 [01:52<02:09,  1.25it/s] 46%|████▋     | 139/300 [01:53<02:09,  1.24it/s] 47%|████▋     | 140/300 [01:54<02:14,  1.19it/s] 47%|████▋     | 141/300 [01:54<02:13,  1.19it/s] 47%|████▋     | 142/300 [01:55<02:10,  1.21it/s] 48%|████▊     | 143/300 [01:56<02:07,  1.23it/s] 48%|████▊     | 144/300 [01:57<02:08,  1.22it/s] 48%|████▊     | 145/300 [01:58<02:06,  1.22it/s] 49%|████▊     | 146/300 [01:59<02:11,  1.17it/s] 49%|████▉     | 147/300 [01:59<02:08,  1.19it/s] 49%|████▉     | 148/300 [02:00<02:04,  1.22it/s] 50%|████▉     | 149/300 [02:01<02:06,  1.19it/s] 50%|█████     | 150/300 [02:02<02:05,  1.20it/s] 50%|█████     | 151/300 [02:03<02:01,  1.23it/s] 51%|█████     | 152/300 [02:03<02:01,  1.22it/s] 51%|█████     | 153/300 [02:04<02:01,  1.21it/s] 51%|█████▏    | 154/300 [02:05<01:59,  1.23it/s] 52%|█████▏    | 155/300 [02:06<01:56,  1.25it/s] 52%|█████▏    | 156/300 [02:07<01:55,  1.25it/s] 52%|█████▏    | 157/300 [02:07<01:55,  1.24it/s] 53%|█████▎    | 158/300 [02:08<01:58,  1.20it/s] 53%|█████▎    | 159/300 [02:09<01:57,  1.20it/s] 53%|█████▎    | 160/300 [02:10<01:54,  1.23it/s] 54%|█████▎    | 161/300 [02:11<01:55,  1.20it/s] 54%|█████▍    | 162/300 [02:12<01:53,  1.21it/s] 54%|█████▍    | 163/300 [02:12<01:50,  1.24it/s] 55%|█████▍    | 164/300 [02:13<01:49,  1.25it/s] 55%|█████▌    | 165/300 [02:14<01:45,  1.28it/s] 55%|█████▌    | 166/300 [02:15<01:50,  1.22it/s] 56%|█████▌    | 167/300 [02:16<01:46,  1.25it/s] 56%|█████▌    | 168/300 [02:16<01:45,  1.25it/s] 56%|█████▋    | 169/300 [02:17<01:45,  1.24it/s] 57%|█████▋    | 170/300 [02:18<01:45,  1.23it/s] 57%|█████▋    | 171/300 [02:19<01:45,  1.22it/s] 57%|█████▋    | 172/300 [02:20<01:43,  1.24it/s] 58%|█████▊    | 173/300 [02:21<01:43,  1.23it/s] 58%|█████▊    | 174/300 [02:21<01:41,  1.24it/s] 58%|█████▊    | 175/300 [02:22<01:41,  1.23it/s] 59%|█████▊    | 176/300 [02:23<01:39,  1.25it/s] 59%|█████▉    | 177/300 [02:24<01:38,  1.25it/s] 59%|█████▉    | 178/300 [02:25<01:39,  1.23it/s] 60%|█████▉    | 179/300 [02:25<01:39,  1.22it/s] 60%|██████    | 180/300 [02:26<01:37,  1.23it/s] 60%|██████    | 181/300 [02:27<01:36,  1.24it/s] 61%|██████    | 182/300 [02:28<01:34,  1.24it/s] 61%|██████    | 183/300 [02:29<01:34,  1.24it/s] 61%|██████▏   | 184/300 [02:29<01:32,  1.25it/s] 62%|██████▏   | 185/300 [02:30<01:31,  1.26it/s] 62%|██████▏   | 186/300 [02:31<01:31,  1.25it/s] 62%|██████▏   | 187/300 [02:32<01:30,  1.24it/s] 63%|██████▎   | 188/300 [02:33<01:30,  1.23it/s] 63%|██████▎   | 189/300 [02:33<01:30,  1.22it/s] 63%|██████▎   | 190/300 [02:34<01:30,  1.22it/s] 64%|██████▎   | 191/300 [02:35<01:29,  1.22it/s] 64%|██████▍   | 192/300 [02:36<01:27,  1.24it/s] 64%|██████▍   | 193/300 [02:37<01:25,  1.25it/s] 65%|██████▍   | 194/300 [02:37<01:24,  1.25it/s] 65%|██████▌   | 195/300 [02:38<01:24,  1.24it/s] 65%|██████▌   | 196/300 [02:39<01:24,  1.24it/s] 66%|██████▌   | 197/300 [02:40<01:22,  1.26it/s] 66%|██████▌   | 198/300 [02:41<01:22,  1.24it/s] 66%|██████▋   | 199/300 [02:42<01:21,  1.23it/s] 67%|██████▋   | 200/300 [02:42<01:21,  1.23it/s] 67%|██████▋   | 201/300 [02:43<01:21,  1.21it/s] 67%|██████▋   | 202/300 [02:44<01:21,  1.20it/s] 68%|██████▊   | 203/300 [02:45<01:21,  1.19it/s] 68%|██████▊   | 204/300 [02:46<01:19,  1.20it/s] 68%|██████▊   | 205/300 [02:47<01:18,  1.22it/s] 69%|██████▊   | 206/300 [02:47<01:17,  1.22it/s] 69%|██████▉   | 207/300 [02:48<01:16,  1.22it/s] 69%|██████▉   | 208/300 [02:49<01:14,  1.23it/s] 70%|██████▉   | 209/300 [02:50<01:12,  1.25it/s] 70%|███████   | 210/300 [02:50<01:11,  1.26it/s] 70%|███████   | 211/300 [02:51<01:10,  1.26it/s] 71%|███████   | 212/300 [02:52<01:11,  1.23it/s] 71%|███████   | 213/300 [02:53<01:12,  1.20it/s] 71%|███████▏  | 214/300 [02:54<01:11,  1.21it/s] 72%|███████▏  | 215/300 [02:55<01:09,  1.23it/s] 72%|███████▏  | 216/300 [02:56<01:10,  1.20it/s] 72%|███████▏  | 217/300 [02:56<01:08,  1.22it/s] 73%|███████▎  | 218/300 [02:57<01:05,  1.25it/s] 73%|███████▎  | 219/300 [02:58<01:04,  1.26it/s] 73%|███████▎  | 220/300 [02:59<01:03,  1.26it/s] 74%|███████▎  | 221/300 [02:59<01:02,  1.27it/s] 74%|███████▍  | 222/300 [03:00<01:01,  1.26it/s] 74%|███████▍  | 223/300 [03:01<01:01,  1.26it/s] 75%|███████▍  | 224/300 [03:02<01:01,  1.24it/s] 75%|███████▌  | 225/300 [03:03<00:59,  1.25it/s] 75%|███████▌  | 226/300 [03:03<00:59,  1.24it/s] 76%|███████▌  | 227/300 [03:04<00:58,  1.26it/s] 76%|███████▌  | 228/300 [03:05<00:57,  1.26it/s] 76%|███████▋  | 229/300 [03:06<00:57,  1.25it/s] 77%|███████▋  | 230/300 [03:07<00:56,  1.23it/s] 77%|███████▋  | 231/300 [03:07<00:56,  1.23it/s] 77%|███████▋  | 232/300 [03:08<00:55,  1.23it/s] 78%|███████▊  | 233/300 [03:09<00:53,  1.24it/s] 78%|███████▊  | 234/300 [03:10<00:52,  1.26it/s] 78%|███████▊  | 235/300 [03:11<00:51,  1.26it/s] 79%|███████▊  | 236/300 [03:11<00:50,  1.28it/s] 79%|███████▉  | 237/300 [03:12<00:50,  1.25it/s] 79%|███████▉  | 238/300 [03:13<00:49,  1.26it/s] 80%|███████▉  | 239/300 [03:14<00:47,  1.28it/s] 80%|████████  | 240/300 [03:15<00:46,  1.29it/s] 80%|████████  | 241/300 [03:15<00:46,  1.28it/s] 81%|████████  | 242/300 [03:16<00:44,  1.29it/s] 81%|████████  | 243/300 [03:17<00:43,  1.30it/s] 81%|████████▏ | 244/300 [03:18<00:42,  1.31it/s] 82%|████████▏ | 245/300 [03:18<00:42,  1.28it/s] 82%|████████▏ | 246/300 [03:19<00:42,  1.28it/s] 82%|████████▏ | 247/300 [03:20<00:41,  1.27it/s] 83%|████████▎ | 248/300 [03:21<00:41,  1.26it/s] 83%|████████▎ | 249/300 [03:22<00:40,  1.26it/s] 83%|████████▎ | 250/300 [03:22<00:40,  1.24it/s] 84%|████████▎ | 251/300 [03:23<00:39,  1.24it/s] 84%|████████▍ | 252/300 [03:24<00:38,  1.23it/s] 84%|████████▍ | 253/300 [03:25<00:37,  1.24it/s] 85%|████████▍ | 254/300 [03:26<00:37,  1.24it/s] 85%|████████▌ | 255/300 [03:26<00:36,  1.25it/s] 85%|████████▌ | 256/300 [03:27<00:34,  1.26it/s] 86%|████████▌ | 257/300 [03:28<00:34,  1.24it/s] 86%|████████▌ | 258/300 [03:29<00:34,  1.23it/s] 86%|████████▋ | 259/300 [03:30<00:33,  1.22it/s] 87%|████████▋ | 260/300 [03:30<00:32,  1.24it/s] 87%|████████▋ | 261/300 [03:31<00:30,  1.26it/s] 87%|████████▋ | 262/300 [03:32<00:30,  1.25it/s] 88%|████████▊ | 263/300 [03:33<00:30,  1.23it/s] 88%|████████▊ | 264/300 [03:34<00:29,  1.23it/s] 88%|████████▊ | 265/300 [03:35<00:28,  1.22it/s] 89%|████████▊ | 266/300 [03:35<00:27,  1.22it/s] 89%|████████▉ | 267/300 [03:36<00:26,  1.22it/s] 89%|████████▉ | 268/300 [03:37<00:26,  1.22it/s] 90%|████████▉ | 269/300 [03:38<00:25,  1.23it/s] 90%|█████████ | 270/300 [03:39<00:24,  1.24it/s] 90%|█████████ | 271/300 [03:39<00:22,  1.26it/s] 91%|█████████ | 272/300 [03:40<00:22,  1.26it/s] 91%|█████████ | 273/300 [03:41<00:21,  1.26it/s] 91%|█████████▏| 274/300 [03:42<00:20,  1.27it/s] 92%|█████████▏| 275/300 [03:43<00:19,  1.26it/s] 92%|█████████▏| 276/300 [03:43<00:18,  1.28it/s] 92%|█████████▏| 277/300 [03:44<00:18,  1.26it/s] 93%|█████████▎| 278/300 [03:45<00:17,  1.27it/s] 93%|█████████▎| 279/300 [03:46<00:16,  1.26it/s] 93%|█████████▎| 280/300 [03:46<00:15,  1.25it/s] 94%|█████████▎| 281/300 [03:47<00:15,  1.25it/s] 94%|█████████▍| 282/300 [03:48<00:14,  1.24it/s] 94%|█████████▍| 283/300 [03:49<00:13,  1.26it/s] 95%|█████████▍| 284/300 [03:50<00:12,  1.27it/s] 95%|█████████▌| 285/300 [03:50<00:11,  1.27it/s] 95%|█████████▌| 286/300 [03:51<00:10,  1.29it/s] 96%|█████████▌| 287/300 [03:52<00:10,  1.27it/s] 96%|█████████▌| 288/300 [03:53<00:09,  1.26it/s] 96%|█████████▋| 289/300 [03:54<00:08,  1.24it/s] 97%|█████████▋| 290/300 [03:54<00:08,  1.22it/s] 97%|█████████▋| 291/300 [03:55<00:07,  1.24it/s] 97%|█████████▋| 292/300 [03:56<00:06,  1.25it/s] 98%|█████████▊| 293/300 [03:57<00:05,  1.25it/s] 98%|█████████▊| 294/300 [03:58<00:04,  1.26it/s] 98%|█████████▊| 295/300 [03:58<00:03,  1.26it/s] 99%|█████████▊| 296/300 [03:59<00:03,  1.27it/s] 99%|█████████▉| 297/300 [04:00<00:02,  1.28it/s] 99%|█████████▉| 298/300 [04:01<00:01,  1.27it/s]100%|█████████▉| 299/300 [04:02<00:00,  1.25it/s]100%|██████████| 300/300 [04:02<00:00,  1.27it/s]100%|██████████| 300/300 [04:02<00:00,  1.24it/s]
W&B disabled.
W&B online. Running your script from this directory will now sync to the cloud.
wandb: Currently logged in as: hl57. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.12 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.10
wandb: Run data is saved locally in /media/techt/One Touch/DD/SRe2L/train/wandb/run-20231024_021620-gzqar9e2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pretty-brook-506
wandb: ⭐️ View project at https://wandb.ai/hl57/final_rn18_fkd
wandb: 🚀 View run at https://wandb.ai/hl57/final_rn18_fkd/runs/gzqar9e2
======= FKD: dataset info ======
path: ../relabel/FKD_cutmix_fp16/imagenette/341/
num img: 500
batch size: 100
max epoch: 300
================================
load data successfully
=> loading student model 'resnet18'
TRAIN Iter 0: lr = 0.001000,	loss = 0.023853,	Top-1 err = 89.000000,	Top-5 err = 40.800000,	train_time = 11.535624
TEST Iter 0: loss = 6.740740,	Top-1 err = 87.821656,	Top-5 err = 45.222930,	val_time = 12.884366
TRAIN Iter 10: lr = 0.000997,	loss = 0.011252,	Top-1 err = 51.600000,	Top-5 err = 8.200000,	train_time = 10.937889
TEST Iter 10: loss = 3.631588,	Top-1 err = 58.853503,	Top-5 err = 15.108280,	val_time = 13.055057
TRAIN Iter 20: lr = 0.000989,	loss = 0.007202,	Top-1 err = 50.400000,	Top-5 err = 8.200000,	train_time = 10.925884
TEST Iter 20: loss = 3.824859,	Top-1 err = 59.949045,	Top-5 err = 17.961783,	val_time = 13.110795
TRAIN Iter 30: lr = 0.000976,	loss = 0.006958,	Top-1 err = 32.600000,	Top-5 err = 8.000000,	train_time = 10.899074
TEST Iter 30: loss = 3.073264,	Top-1 err = 48.840764,	Top-5 err = 9.707006,	val_time = 13.279643
TRAIN Iter 40: lr = 0.000957,	loss = 0.005691,	Top-1 err = 38.400000,	Top-5 err = 7.200000,	train_time = 10.780850
TEST Iter 40: loss = 2.173260,	Top-1 err = 43.108280,	Top-5 err = 6.700637,	val_time = 12.926607
TRAIN Iter 50: lr = 0.000933,	loss = 0.005383,	Top-1 err = 44.000000,	Top-5 err = 5.600000,	train_time = 10.863491
TEST Iter 50: loss = 1.742049,	Top-1 err = 37.681529,	Top-5 err = 6.292994,	val_time = 13.274580
TRAIN Iter 60: lr = 0.000905,	loss = 0.004433,	Top-1 err = 39.400000,	Top-5 err = 7.000000,	train_time = 10.772990
TEST Iter 60: loss = 2.651649,	Top-1 err = 44.968153,	Top-5 err = 7.159236,	val_time = 13.223210
TRAIN Iter 70: lr = 0.000872,	loss = 0.004006,	Top-1 err = 45.600000,	Top-5 err = 6.000000,	train_time = 10.791816
TEST Iter 70: loss = 1.399952,	Top-1 err = 33.452229,	Top-5 err = 5.528662,	val_time = 13.093222
TRAIN Iter 80: lr = 0.000835,	loss = 0.003892,	Top-1 err = 27.600000,	Top-5 err = 3.600000,	train_time = 10.725212
TEST Iter 80: loss = 1.226246,	Top-1 err = 30.063694,	Top-5 err = 4.356688,	val_time = 13.051940
TRAIN Iter 90: lr = 0.000794,	loss = 0.003845,	Top-1 err = 54.600000,	Top-5 err = 13.200000,	train_time = 10.674938
TEST Iter 90: loss = 1.284709,	Top-1 err = 31.082803,	Top-5 err = 4.000000,	val_time = 12.943215
TRAIN Iter 100: lr = 0.000750,	loss = 0.003534,	Top-1 err = 34.200000,	Top-5 err = 2.600000,	train_time = 10.999312
TEST Iter 100: loss = 1.875920,	Top-1 err = 35.949045,	Top-5 err = 5.095541,	val_time = 12.966435
TRAIN Iter 110: lr = 0.000703,	loss = 0.003085,	Top-1 err = 31.600000,	Top-5 err = 3.400000,	train_time = 10.850343
TEST Iter 110: loss = 1.202417,	Top-1 err = 29.528662,	Top-5 err = 4.000000,	val_time = 13.007812
TRAIN Iter 120: lr = 0.000655,	loss = 0.002749,	Top-1 err = 41.400000,	Top-5 err = 4.200000,	train_time = 10.688888
TEST Iter 120: loss = 1.207846,	Top-1 err = 30.420382,	Top-5 err = 3.872611,	val_time = 13.159808
TRAIN Iter 130: lr = 0.000604,	loss = 0.002854,	Top-1 err = 28.400000,	Top-5 err = 3.600000,	train_time = 10.828281
TEST Iter 130: loss = 1.045417,	Top-1 err = 26.318471,	Top-5 err = 2.955414,	val_time = 12.909327
TRAIN Iter 140: lr = 0.000552,	loss = 0.002707,	Top-1 err = 31.000000,	Top-5 err = 5.000000,	train_time = 10.682437
TEST Iter 140: loss = 0.864633,	Top-1 err = 24.127389,	Top-5 err = 2.675159,	val_time = 13.234771
TRAIN Iter 150: lr = 0.000500,	loss = 0.002252,	Top-1 err = 32.600000,	Top-5 err = 3.400000,	train_time = 10.710637
TEST Iter 150: loss = 0.975837,	Top-1 err = 24.993631,	Top-5 err = 2.700637,	val_time = 12.759229
TRAIN Iter 160: lr = 0.000448,	loss = 0.002309,	Top-1 err = 21.800000,	Top-5 err = 1.600000,	train_time = 10.672164
TEST Iter 160: loss = 0.823075,	Top-1 err = 21.808917,	Top-5 err = 2.420382,	val_time = 12.871024
TRAIN Iter 170: lr = 0.000396,	loss = 0.002195,	Top-1 err = 26.400000,	Top-5 err = 2.200000,	train_time = 10.614608
TEST Iter 170: loss = 0.754195,	Top-1 err = 21.171975,	Top-5 err = 2.369427,	val_time = 12.798254
TRAIN Iter 180: lr = 0.000345,	loss = 0.001956,	Top-1 err = 17.800000,	Top-5 err = 0.600000,	train_time = 10.735377
TEST Iter 180: loss = 0.759827,	Top-1 err = 21.656051,	Top-5 err = 2.165605,	val_time = 12.898562
TRAIN Iter 190: lr = 0.000297,	loss = 0.002090,	Top-1 err = 29.800000,	Top-5 err = 5.000000,	train_time = 10.617764
TEST Iter 190: loss = 0.674252,	Top-1 err = 19.745223,	Top-5 err = 1.936306,	val_time = 12.903678
TRAIN Iter 200: lr = 0.000250,	loss = 0.001942,	Top-1 err = 21.600000,	Top-5 err = 1.000000,	train_time = 10.633180
TEST Iter 200: loss = 0.614529,	Top-1 err = 18.420382,	Top-5 err = 1.630573,	val_time = 13.047790
TRAIN Iter 210: lr = 0.000206,	loss = 0.001894,	Top-1 err = 27.200000,	Top-5 err = 2.800000,	train_time = 10.719223
TEST Iter 210: loss = 0.686924,	Top-1 err = 19.719745,	Top-5 err = 1.987261,	val_time = 12.831949
TRAIN Iter 220: lr = 0.000165,	loss = 0.001764,	Top-1 err = 33.400000,	Top-5 err = 4.200000,	train_time = 10.650686
TEST Iter 220: loss = 0.612399,	Top-1 err = 18.343949,	Top-5 err = 1.808917,	val_time = 12.841023
TRAIN Iter 230: lr = 0.000128,	loss = 0.001571,	Top-1 err = 37.800000,	Top-5 err = 6.600000,	train_time = 10.630735
TEST Iter 230: loss = 0.617934,	Top-1 err = 18.063694,	Top-5 err = 1.656051,	val_time = 12.772206
TRAIN Iter 240: lr = 0.000095,	loss = 0.001547,	Top-1 err = 44.400000,	Top-5 err = 9.600000,	train_time = 10.602171
TEST Iter 240: loss = 0.571317,	Top-1 err = 17.019108,	Top-5 err = 1.528662,	val_time = 12.796138
TRAIN Iter 250: lr = 0.000067,	loss = 0.001641,	Top-1 err = 38.800000,	Top-5 err = 5.400000,	train_time = 10.553342
TEST Iter 250: loss = 0.563031,	Top-1 err = 17.197452,	Top-5 err = 1.350318,	val_time = 12.726700
TRAIN Iter 260: lr = 0.000043,	loss = 0.001669,	Top-1 err = 34.400000,	Top-5 err = 4.600000,	train_time = 10.670994
TEST Iter 260: loss = 0.560536,	Top-1 err = 16.866242,	Top-5 err = 1.375796,	val_time = 12.840666
TRAIN Iter 270: lr = 0.000024,	loss = 0.001594,	Top-1 err = 61.800000,	Top-5 err = 13.400000,	train_time = 10.643803
TEST Iter 270: loss = 0.559164,	Top-1 err = 16.611465,	Top-5 err = 1.401274,	val_time = 12.751543
TRAIN Iter 280: lr = 0.000011,	loss = 0.001690,	Top-1 err = 17.200000,	Top-5 err = 0.600000,	train_time = 10.622122
TEST Iter 280: loss = 0.556004,	Top-1 err = 16.891720,	Top-5 err = 1.477707,	val_time = 12.866793
TRAIN Iter 290: lr = 0.000003,	loss = 0.001640,	Top-1 err = 31.400000,	Top-5 err = 1.800000,	train_time = 10.593387
TEST Iter 290: loss = 0.551266,	Top-1 err = 16.560510,	Top-5 err = 1.375796,	val_time = 12.798630
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:  train/Top1 ▁▂▃▂▄▃▄▆▃▄▄▆▅▇▅▃█▆▅▅▅▄▆▇▅▇▃▇▇▅▅▆▄▅▅█▅▆▅▅
wandb:  train/Top5 ▁▃▂▁▆▄▅▇▆▆▆▆▇█▇▂█▇▅▄▆▆▇█▅▇▅▅▇▆▆▇▄▆▆██▇▇▆
wandb: train/epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  train/loss █▇▆▅▄▃▃▃▃▃▃▂▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    train/lr ███████▇▇▇▇▇▇▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁
wandb:   val/epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███
wandb:    val/loss █▄▅▄▃▂▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    val/top1 ▁▄▄▅▅▆▅▆▇▇▆▇▇▇▇▇▇█▇████████████
wandb:    val/top5 ▁▆▅▇▇▇▇▇██▇████████████████████
wandb: 
wandb: Run summary:
wandb:  train/Top1 63.0
wandb:  train/Top5 93.2
wandb: train/epoch 299
wandb:  train/loss 0.00151
wandb:    train/lr 0.0
wandb:   val/epoch 299
wandb:    val/loss 0.55033
wandb:    val/top1 83.41401
wandb:    val/top5 98.6242
wandb: 
wandb: 🚀 View run pretty-brook-506 at: https://wandb.ai/hl57/final_rn18_fkd/runs/gzqar9e2
wandb: ️⚡ View job at https://wandb.ai/hl57/final_rn18_fkd/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjk3MzkyNjU3/version_details/v48
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231024_021620-gzqar9e2/logs
TEST Iter 299: loss = 0.550327,	Top-1 err = 16.585987,	Top-5 err = 1.375796,	val_time = 12.787199
