/home/techt/anaconda3/envs/hl/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/techt/anaconda3/envs/hl/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
/home/techt/anaconda3/envs/hl/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
r_bn:  1.0
lr:  0.1
ipc_id =  0
get_images call
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
------------iteration 0----------
total loss 412.8242492675781
main criterion 9.155780792236328
weighted_aux_loss 403.66845703125
loss_r_bn_feature 403.66845703125
------------iteration 100----------
total loss 151.43772888183594
main criterion 0.13740989565849304
weighted_aux_loss 151.30032348632812
loss_r_bn_feature 151.30032348632812
------------iteration 200----------
total loss 103.76368713378906
main criterion 0.06750007718801498
weighted_aux_loss 103.6961898803711
loss_r_bn_feature 103.6961898803711
------------iteration 300----------
total loss 68.93366241455078
main criterion 0.1170165091753006
weighted_aux_loss 68.81664276123047
loss_r_bn_feature 68.81664276123047
------------iteration 400----------
total loss 57.80912780761719
main criterion 0.044179778546094894
weighted_aux_loss 57.764949798583984
loss_r_bn_feature 57.764949798583984
------------iteration 500----------
total loss 74.15750885009766
main criterion 0.13088800013065338
weighted_aux_loss 74.02661895751953
loss_r_bn_feature 74.02661895751953
------------iteration 600----------
total loss 62.65868377685547
main criterion 0.0973508358001709
weighted_aux_loss 62.56133270263672
loss_r_bn_feature 62.56133270263672
------------iteration 700----------
total loss 51.43717956542969
main criterion 0.025025811046361923
weighted_aux_loss 51.41215515136719
loss_r_bn_feature 51.41215515136719
------------iteration 800----------
total loss 56.135894775390625
main criterion 0.05317927896976471
weighted_aux_loss 56.08271408081055
loss_r_bn_feature 56.08271408081055
------------iteration 900----------
total loss 59.76053237915039
main criterion 0.32377558946609497
weighted_aux_loss 59.4367561340332
loss_r_bn_feature 59.4367561340332
------------iteration 1000----------
total loss 35.170806884765625
main criterion 0.028855683282017708
weighted_aux_loss 35.14195251464844
loss_r_bn_feature 35.14195251464844
------------iteration 1100----------
total loss 36.045223236083984
main criterion 0.15739235281944275
weighted_aux_loss 35.88783264160156
loss_r_bn_feature 35.88783264160156
------------iteration 1200----------
total loss 143.2278289794922
main criterion 3.5280027389526367
weighted_aux_loss 139.6998291015625
loss_r_bn_feature 139.6998291015625
------------iteration 1300----------
total loss 24.151477813720703
main criterion 0.060752660036087036
weighted_aux_loss 24.09072494506836
loss_r_bn_feature 24.09072494506836
------------iteration 1400----------
total loss 19.63196563720703
main criterion 0.021790239959955215
weighted_aux_loss 19.61017608642578
loss_r_bn_feature 19.61017608642578
------------iteration 1500----------
total loss 28.332595825195312
main criterion 0.08379149436950684
weighted_aux_loss 28.248804092407227
loss_r_bn_feature 28.248804092407227
------------iteration 1600----------
total loss 13.32873249053955
main criterion 0.0398738756775856
weighted_aux_loss 13.288858413696289
loss_r_bn_feature 13.288858413696289
------------iteration 1700----------
total loss 14.567682266235352
main criterion 0.013449220918118954
weighted_aux_loss 14.554232597351074
loss_r_bn_feature 14.554232597351074
------------iteration 1800----------
total loss 12.932174682617188
main criterion 0.02031009830534458
weighted_aux_loss 12.911864280700684
loss_r_bn_feature 12.911864280700684
------------iteration 1900----------
total loss 11.685032844543457
main criterion 0.04055987671017647
weighted_aux_loss 11.6444730758667
loss_r_bn_feature 11.6444730758667
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
per-class-batchnorm:  False
------------iteration 0----------
total loss 438.1716003417969
main criterion 7.11362361907959
weighted_aux_loss 431.0579833984375
loss_r_bn_feature 431.0579833984375
------------iteration 100----------
total loss 155.3784942626953
main criterion 0.09765958040952682
weighted_aux_loss 155.2808380126953
loss_r_bn_feature 155.2808380126953
------------iteration 200----------
total loss 89.16002655029297
main criterion 0.3143434226512909
weighted_aux_loss 88.8456802368164
loss_r_bn_feature 88.8456802368164
------------iteration 300----------
total loss 86.40592193603516
main criterion 0.09154530614614487
weighted_aux_loss 86.31437683105469
loss_r_bn_feature 86.31437683105469
------------iteration 400----------
total loss 133.16639709472656
main criterion 3.963569402694702
weighted_aux_loss 129.2028350830078
loss_r_bn_feature 129.2028350830078
------------iteration 500----------
total loss 107.50704193115234
main criterion 3.254772424697876
weighted_aux_loss 104.25226593017578
loss_r_bn_feature 104.25226593017578
------------iteration 600----------
total loss 63.68852233886719
main criterion 0.05380669981241226
weighted_aux_loss 63.63471603393555
loss_r_bn_feature 63.63471603393555
------------iteration 700----------
total loss 109.46228790283203
main criterion 2.235205888748169
weighted_aux_loss 107.22708129882812
loss_r_bn_feature 107.22708129882812
------------iteration 800----------
total loss 50.39975357055664
main criterion 0.06768827140331268
weighted_aux_loss 50.33206558227539
loss_r_bn_feature 50.33206558227539
------------iteration 900----------
total loss 45.8090705871582
main criterion 0.0801764577627182
weighted_aux_loss 45.7288932800293
loss_r_bn_feature 45.7288932800293
------------iteration 1000----------
total loss 42.858577728271484
main criterion 0.09430741518735886
weighted_aux_loss 42.7642707824707
loss_r_bn_feature 42.7642707824707
------------iteration 1100----------
total loss 26.630401611328125
main criterion 0.06011801213026047
weighted_aux_loss 26.570283889770508
loss_r_bn_feature 26.570283889770508
------------iteration 1200----------
total loss 41.229888916015625
main criterion 0.06413107365369797
weighted_aux_loss 41.16575622558594
loss_r_bn_feature 41.16575622558594
------------iteration 1300----------
total loss 21.689105987548828
main criterion 0.025931794196367264
weighted_aux_loss 21.66317367553711
loss_r_bn_feature 21.66317367553711
------------iteration 1400----------
total loss 32.06450271606445
main criterion 0.14355802536010742
weighted_aux_loss 31.92094612121582
loss_r_bn_feature 31.92094612121582
------------iteration 1500----------
total loss 27.129465103149414
main criterion 0.027346039190888405
weighted_aux_loss 27.10211944580078
loss_r_bn_feature 27.10211944580078
------------iteration 1600----------
total loss 12.895545959472656
main criterion 0.027400024235248566
weighted_aux_loss 12.868145942687988
loss_r_bn_feature 12.868145942687988
------------iteration 1700----------
total loss 12.19711685180664
main criterion 0.02706238254904747
weighted_aux_loss 12.17005443572998
loss_r_bn_feature 12.17005443572998
------------iteration 1800----------
total loss 12.323039054870605
main criterion 0.022909242659807205
weighted_aux_loss 12.300129890441895
loss_r_bn_feature 12.300129890441895
------------iteration 1900----------
total loss 13.450212478637695
main criterion 0.029334258288145065
weighted_aux_loss 13.420878410339355
loss_r_bn_feature 13.420878410339355
=> using pytorch pre-trained model 'resnet18'
process data from ../recover/syn_data/tiny-imagenet/313
  0%|          | 0/300 [00:00<?, ?it/s]  0%|          | 1/300 [00:02<12:20,  2.48s/it]  1%|          | 2/300 [00:03<07:22,  1.48s/it]  1%|          | 3/300 [00:04<05:48,  1.17s/it]  1%|▏         | 4/300 [00:04<05:01,  1.02s/it]  2%|▏         | 5/300 [00:05<04:34,  1.08it/s]  2%|▏         | 6/300 [00:06<04:17,  1.14it/s]  2%|▏         | 7/300 [00:07<04:06,  1.19it/s]  3%|▎         | 8/300 [00:07<03:59,  1.22it/s]  3%|▎         | 9/300 [00:08<03:53,  1.25it/s]  3%|▎         | 10/300 [00:09<03:53,  1.24it/s]  4%|▎         | 11/300 [00:10<03:53,  1.24it/s]  4%|▍         | 12/300 [00:11<03:50,  1.25it/s]  4%|▍         | 13/300 [00:11<03:49,  1.25it/s]  5%|▍         | 14/300 [00:12<03:46,  1.26it/s]  5%|▌         | 15/300 [00:13<03:44,  1.27it/s]  5%|▌         | 16/300 [00:14<03:45,  1.26it/s]  6%|▌         | 17/300 [00:15<03:42,  1.27it/s]  6%|▌         | 18/300 [00:15<03:41,  1.28it/s]  6%|▋         | 19/300 [00:16<03:40,  1.28it/s]  7%|▋         | 20/300 [00:17<03:40,  1.27it/s]  7%|▋         | 21/300 [00:18<03:39,  1.27it/s]  7%|▋         | 22/300 [00:18<03:38,  1.27it/s]  8%|▊         | 23/300 [00:19<03:38,  1.27it/s]  8%|▊         | 24/300 [00:20<03:37,  1.27it/s]  8%|▊         | 25/300 [00:21<03:35,  1.28it/s]  9%|▊         | 26/300 [00:22<03:36,  1.26it/s]  9%|▉         | 27/300 [00:22<03:35,  1.27it/s]  9%|▉         | 28/300 [00:23<03:36,  1.26it/s] 10%|▉         | 29/300 [00:24<03:32,  1.27it/s] 10%|█         | 30/300 [00:25<03:30,  1.28it/s] 10%|█         | 31/300 [00:26<03:31,  1.27it/s] 11%|█         | 32/300 [00:26<03:30,  1.27it/s] 11%|█         | 33/300 [00:27<03:28,  1.28it/s] 11%|█▏        | 34/300 [00:28<03:30,  1.26it/s] 12%|█▏        | 35/300 [00:29<03:28,  1.27it/s] 12%|█▏        | 36/300 [00:30<03:28,  1.27it/s] 12%|█▏        | 37/300 [00:30<03:28,  1.26it/s] 13%|█▎        | 38/300 [00:31<03:27,  1.26it/s] 13%|█▎        | 39/300 [00:32<03:26,  1.26it/s] 13%|█▎        | 40/300 [00:33<03:26,  1.26it/s] 14%|█▎        | 41/300 [00:33<03:26,  1.26it/s] 14%|█▍        | 42/300 [00:34<03:24,  1.26it/s] 14%|█▍        | 43/300 [00:35<03:23,  1.26it/s] 15%|█▍        | 44/300 [00:36<03:21,  1.27it/s] 15%|█▌        | 45/300 [00:37<03:18,  1.28it/s] 15%|█▌        | 46/300 [00:37<03:17,  1.29it/s] 16%|█▌        | 47/300 [00:38<03:14,  1.30it/s] 16%|█▌        | 48/300 [00:39<03:14,  1.30it/s] 16%|█▋        | 49/300 [00:40<03:12,  1.31it/s] 17%|█▋        | 50/300 [00:40<03:13,  1.29it/s] 17%|█▋        | 51/300 [00:41<03:13,  1.29it/s] 17%|█▋        | 52/300 [00:42<03:15,  1.27it/s] 18%|█▊        | 53/300 [00:43<03:16,  1.26it/s] 18%|█▊        | 54/300 [00:44<03:13,  1.27it/s] 18%|█▊        | 55/300 [00:44<03:13,  1.27it/s] 19%|█▊        | 56/300 [00:45<03:10,  1.28it/s] 19%|█▉        | 57/300 [00:46<03:11,  1.27it/s] 19%|█▉        | 58/300 [00:47<03:12,  1.26it/s] 20%|█▉        | 59/300 [00:48<03:10,  1.26it/s] 20%|██        | 60/300 [00:48<03:09,  1.27it/s] 20%|██        | 61/300 [00:49<03:08,  1.27it/s] 21%|██        | 62/300 [00:50<03:06,  1.27it/s] 21%|██        | 63/300 [00:51<03:06,  1.27it/s] 21%|██▏       | 64/300 [00:51<03:05,  1.27it/s] 22%|██▏       | 65/300 [00:52<03:04,  1.27it/s] 22%|██▏       | 66/300 [00:53<03:03,  1.27it/s] 22%|██▏       | 67/300 [00:54<03:03,  1.27it/s] 23%|██▎       | 68/300 [00:55<03:03,  1.26it/s] 23%|██▎       | 69/300 [00:55<03:00,  1.28it/s] 23%|██▎       | 70/300 [00:56<02:59,  1.28it/s] 24%|██▎       | 71/300 [00:57<02:58,  1.28it/s] 24%|██▍       | 72/300 [00:58<02:56,  1.29it/s] 24%|██▍       | 73/300 [00:59<02:55,  1.29it/s] 25%|██▍       | 74/300 [00:59<02:55,  1.29it/s] 25%|██▌       | 75/300 [01:00<02:54,  1.29it/s] 25%|██▌       | 76/300 [01:01<02:54,  1.29it/s] 26%|██▌       | 77/300 [01:02<02:52,  1.29it/s] 26%|██▌       | 78/300 [01:02<02:51,  1.29it/s] 26%|██▋       | 79/300 [01:03<02:53,  1.28it/s] 27%|██▋       | 80/300 [01:04<02:51,  1.28it/s] 27%|██▋       | 81/300 [01:05<02:52,  1.27it/s] 27%|██▋       | 82/300 [01:06<02:52,  1.26it/s] 28%|██▊       | 83/300 [01:06<02:52,  1.26it/s] 28%|██▊       | 84/300 [01:07<02:50,  1.27it/s] 28%|██▊       | 85/300 [01:08<02:49,  1.27it/s] 29%|██▊       | 86/300 [01:09<02:47,  1.28it/s] 29%|██▉       | 87/300 [01:09<02:46,  1.28it/s] 29%|██▉       | 88/300 [01:10<02:46,  1.27it/s] 30%|██▉       | 89/300 [01:11<02:46,  1.27it/s] 30%|███       | 90/300 [01:12<02:44,  1.28it/s] 30%|███       | 91/300 [01:13<02:44,  1.27it/s] 31%|███       | 92/300 [01:13<02:45,  1.26it/s] 31%|███       | 93/300 [01:14<02:42,  1.27it/s] 31%|███▏      | 94/300 [01:15<02:41,  1.28it/s] 32%|███▏      | 95/300 [01:16<02:41,  1.27it/s] 32%|███▏      | 96/300 [01:17<02:41,  1.27it/s] 32%|███▏      | 97/300 [01:17<02:41,  1.26it/s] 33%|███▎      | 98/300 [01:18<02:41,  1.25it/s] 33%|███▎      | 99/300 [01:19<02:40,  1.25it/s] 33%|███▎      | 100/300 [01:20<02:38,  1.26it/s] 34%|███▎      | 101/300 [01:21<02:38,  1.26it/s] 34%|███▍      | 102/300 [01:21<02:35,  1.27it/s] 34%|███▍      | 103/300 [01:22<02:35,  1.27it/s] 35%|███▍      | 104/300 [01:23<02:34,  1.27it/s] 35%|███▌      | 105/300 [01:24<02:33,  1.27it/s] 35%|███▌      | 106/300 [01:25<02:33,  1.26it/s] 36%|███▌      | 107/300 [01:25<02:32,  1.27it/s] 36%|███▌      | 108/300 [01:26<02:32,  1.26it/s] 36%|███▋      | 109/300 [01:27<02:31,  1.26it/s] 37%|███▋      | 110/300 [01:28<02:31,  1.25it/s] 37%|███▋      | 111/300 [01:28<02:30,  1.26it/s] 37%|███▋      | 112/300 [01:29<02:27,  1.27it/s] 38%|███▊      | 113/300 [01:30<02:26,  1.27it/s] 38%|███▊      | 114/300 [01:31<02:26,  1.27it/s] 38%|███▊      | 115/300 [01:32<02:25,  1.27it/s] 39%|███▊      | 116/300 [01:32<02:22,  1.29it/s] 39%|███▉      | 117/300 [01:33<02:22,  1.28it/s] 39%|███▉      | 118/300 [01:34<02:23,  1.27it/s] 40%|███▉      | 119/300 [01:35<02:22,  1.27it/s] 40%|████      | 120/300 [01:36<02:21,  1.28it/s] 40%|████      | 121/300 [01:36<02:19,  1.28it/s] 41%|████      | 122/300 [01:37<02:18,  1.29it/s] 41%|████      | 123/300 [01:38<02:17,  1.29it/s] 41%|████▏     | 124/300 [01:39<02:17,  1.28it/s] 42%|████▏     | 125/300 [01:39<02:16,  1.29it/s] 42%|████▏     | 126/300 [01:40<02:14,  1.30it/s] 42%|████▏     | 127/300 [01:41<02:13,  1.29it/s] 43%|████▎     | 128/300 [01:42<02:13,  1.29it/s] 43%|████▎     | 129/300 [01:42<02:12,  1.29it/s] 43%|████▎     | 130/300 [01:43<02:12,  1.28it/s] 44%|████▎     | 131/300 [01:44<02:11,  1.29it/s] 44%|████▍     | 132/300 [01:45<02:10,  1.28it/s] 44%|████▍     | 133/300 [01:46<02:09,  1.29it/s] 45%|████▍     | 134/300 [01:46<02:08,  1.30it/s] 45%|████▌     | 135/300 [01:47<02:07,  1.29it/s] 45%|████▌     | 136/300 [01:48<02:06,  1.30it/s] 46%|████▌     | 137/300 [01:49<02:05,  1.30it/s] 46%|████▌     | 138/300 [01:49<02:04,  1.30it/s] 46%|████▋     | 139/300 [01:50<02:04,  1.30it/s] 47%|████▋     | 140/300 [01:51<02:03,  1.30it/s] 47%|████▋     | 141/300 [01:52<02:02,  1.30it/s] 47%|████▋     | 142/300 [01:53<02:01,  1.30it/s] 48%|████▊     | 143/300 [01:53<02:01,  1.30it/s] 48%|████▊     | 144/300 [01:54<02:00,  1.29it/s] 48%|████▊     | 145/300 [01:55<02:00,  1.29it/s] 49%|████▊     | 146/300 [01:56<01:59,  1.29it/s] 49%|████▉     | 147/300 [01:56<01:59,  1.28it/s] 49%|████▉     | 148/300 [01:57<01:58,  1.29it/s] 50%|████▉     | 149/300 [01:58<01:57,  1.28it/s] 50%|█████     | 150/300 [01:59<01:55,  1.29it/s] 50%|█████     | 151/300 [02:00<01:54,  1.30it/s] 51%|█████     | 152/300 [02:00<01:53,  1.31it/s] 51%|█████     | 153/300 [02:01<01:53,  1.30it/s] 51%|█████▏    | 154/300 [02:02<01:51,  1.30it/s] 52%|█████▏    | 155/300 [02:03<01:52,  1.29it/s] 52%|█████▏    | 156/300 [02:03<01:51,  1.29it/s] 52%|█████▏    | 157/300 [02:04<01:49,  1.30it/s] 53%|█████▎    | 158/300 [02:05<01:49,  1.30it/s] 53%|█████▎    | 159/300 [02:06<01:49,  1.29it/s] 53%|█████▎    | 160/300 [02:06<01:49,  1.28it/s] 54%|█████▎    | 161/300 [02:07<01:48,  1.28it/s] 54%|█████▍    | 162/300 [02:08<01:47,  1.29it/s] 54%|█████▍    | 163/300 [02:09<01:46,  1.29it/s] 55%|█████▍    | 164/300 [02:10<01:44,  1.30it/s] 55%|█████▌    | 165/300 [02:10<01:43,  1.31it/s] 55%|█████▌    | 166/300 [02:11<01:44,  1.29it/s] 56%|█████▌    | 167/300 [02:12<01:42,  1.30it/s] 56%|█████▌    | 168/300 [02:13<01:41,  1.30it/s] 56%|█████▋    | 169/300 [02:13<01:40,  1.30it/s] 57%|█████▋    | 170/300 [02:14<01:39,  1.30it/s] 57%|█████▋    | 171/300 [02:15<01:39,  1.29it/s] 57%|█████▋    | 172/300 [02:16<01:38,  1.30it/s] 58%|█████▊    | 173/300 [02:16<01:37,  1.30it/s] 58%|█████▊    | 174/300 [02:17<01:37,  1.29it/s] 58%|█████▊    | 175/300 [02:18<01:36,  1.30it/s] 59%|█████▊    | 176/300 [02:19<01:34,  1.31it/s] 59%|█████▉    | 177/300 [02:20<01:33,  1.31it/s] 59%|█████▉    | 178/300 [02:20<01:33,  1.31it/s] 60%|█████▉    | 179/300 [02:21<01:32,  1.31it/s] 60%|██████    | 180/300 [02:22<01:31,  1.32it/s] 60%|██████    | 181/300 [02:23<01:31,  1.30it/s] 61%|██████    | 182/300 [02:23<01:30,  1.30it/s] 61%|██████    | 183/300 [02:24<01:31,  1.29it/s] 61%|██████▏   | 184/300 [02:25<01:29,  1.30it/s] 62%|██████▏   | 185/300 [02:26<01:28,  1.31it/s] 62%|██████▏   | 186/300 [02:26<01:27,  1.30it/s] 62%|██████▏   | 187/300 [02:27<01:27,  1.30it/s] 63%|██████▎   | 188/300 [02:28<01:26,  1.30it/s] 63%|██████▎   | 189/300 [02:29<01:26,  1.28it/s] 63%|██████▎   | 190/300 [02:30<01:26,  1.27it/s] 64%|██████▎   | 191/300 [02:30<01:26,  1.26it/s] 64%|██████▍   | 192/300 [02:31<01:24,  1.28it/s] 64%|██████▍   | 193/300 [02:32<01:23,  1.28it/s] 65%|██████▍   | 194/300 [02:33<01:22,  1.28it/s] 65%|██████▌   | 195/300 [02:34<01:21,  1.29it/s] 65%|██████▌   | 196/300 [02:34<01:20,  1.30it/s] 66%|██████▌   | 197/300 [02:35<01:19,  1.30it/s] 66%|██████▌   | 198/300 [02:36<01:18,  1.30it/s] 66%|██████▋   | 199/300 [02:37<01:17,  1.30it/s] 67%|██████▋   | 200/300 [02:37<01:16,  1.31it/s] 67%|██████▋   | 201/300 [02:38<01:15,  1.31it/s] 67%|██████▋   | 202/300 [02:39<01:15,  1.30it/s] 68%|██████▊   | 203/300 [02:40<01:15,  1.29it/s] 68%|██████▊   | 204/300 [02:40<01:14,  1.29it/s] 68%|██████▊   | 205/300 [02:41<01:14,  1.27it/s] 69%|██████▊   | 206/300 [02:42<01:13,  1.27it/s] 69%|██████▉   | 207/300 [02:43<01:13,  1.27it/s] 69%|██████▉   | 208/300 [02:44<01:12,  1.27it/s] 70%|██████▉   | 209/300 [02:44<01:11,  1.28it/s] 70%|███████   | 210/300 [02:45<01:10,  1.28it/s] 70%|███████   | 211/300 [02:46<01:09,  1.27it/s] 71%|███████   | 212/300 [02:47<01:09,  1.27it/s] 71%|███████   | 213/300 [02:48<01:08,  1.27it/s] 71%|███████▏  | 214/300 [02:48<01:07,  1.27it/s] 72%|███████▏  | 215/300 [02:49<01:06,  1.27it/s] 72%|███████▏  | 216/300 [02:50<01:06,  1.26it/s] 72%|███████▏  | 217/300 [02:51<01:06,  1.26it/s] 73%|███████▎  | 218/300 [02:51<01:04,  1.27it/s] 73%|███████▎  | 219/300 [02:52<01:03,  1.27it/s] 73%|███████▎  | 220/300 [02:53<01:03,  1.27it/s] 74%|███████▎  | 221/300 [02:54<01:01,  1.27it/s] 74%|███████▍  | 222/300 [02:55<01:00,  1.28it/s] 74%|███████▍  | 223/300 [02:55<01:00,  1.27it/s] 75%|███████▍  | 224/300 [02:56<00:59,  1.28it/s] 75%|███████▌  | 225/300 [02:57<00:58,  1.29it/s] 75%|███████▌  | 226/300 [02:58<00:57,  1.28it/s] 76%|███████▌  | 227/300 [02:59<00:56,  1.28it/s] 76%|███████▌  | 228/300 [02:59<00:56,  1.27it/s] 76%|███████▋  | 229/300 [03:00<00:55,  1.27it/s] 77%|███████▋  | 230/300 [03:01<00:54,  1.28it/s] 77%|███████▋  | 231/300 [03:02<00:54,  1.26it/s] 77%|███████▋  | 232/300 [03:02<00:53,  1.27it/s] 78%|███████▊  | 233/300 [03:03<00:53,  1.26it/s] 78%|███████▊  | 234/300 [03:04<00:51,  1.27it/s] 78%|███████▊  | 235/300 [03:05<00:50,  1.28it/s] 79%|███████▊  | 236/300 [03:06<00:49,  1.28it/s] 79%|███████▉  | 237/300 [03:06<00:49,  1.28it/s] 79%|███████▉  | 238/300 [03:07<00:48,  1.27it/s] 80%|███████▉  | 239/300 [03:08<00:48,  1.27it/s] 80%|████████  | 240/300 [03:09<00:47,  1.27it/s] 80%|████████  | 241/300 [03:10<00:46,  1.27it/s] 81%|████████  | 242/300 [03:10<00:45,  1.27it/s] 81%|████████  | 243/300 [03:11<00:44,  1.27it/s] 81%|████████▏ | 244/300 [03:12<00:44,  1.26it/s] 82%|████████▏ | 245/300 [03:13<00:43,  1.27it/s] 82%|████████▏ | 246/300 [03:13<00:42,  1.27it/s] 82%|████████▏ | 247/300 [03:14<00:41,  1.27it/s] 83%|████████▎ | 248/300 [03:15<00:41,  1.27it/s] 83%|████████▎ | 249/300 [03:16<00:40,  1.27it/s] 83%|████████▎ | 250/300 [03:17<00:39,  1.27it/s] 84%|████████▎ | 251/300 [03:17<00:38,  1.27it/s] 84%|████████▍ | 252/300 [03:18<00:37,  1.27it/s] 84%|████████▍ | 253/300 [03:19<00:36,  1.27it/s] 85%|████████▍ | 254/300 [03:20<00:36,  1.26it/s] 85%|████████▌ | 255/300 [03:21<00:35,  1.28it/s] 85%|████████▌ | 256/300 [03:21<00:34,  1.28it/s] 86%|████████▌ | 257/300 [03:22<00:33,  1.28it/s] 86%|████████▌ | 258/300 [03:23<00:33,  1.27it/s] 86%|████████▋ | 259/300 [03:24<00:32,  1.26it/s] 87%|████████▋ | 260/300 [03:25<00:31,  1.26it/s] 87%|████████▋ | 261/300 [03:25<00:30,  1.27it/s] 87%|████████▋ | 262/300 [03:26<00:29,  1.28it/s] 88%|████████▊ | 263/300 [03:27<00:28,  1.29it/s] 88%|████████▊ | 264/300 [03:28<00:27,  1.29it/s] 88%|████████▊ | 265/300 [03:28<00:26,  1.30it/s] 89%|████████▊ | 266/300 [03:29<00:26,  1.31it/s] 89%|████████▉ | 267/300 [03:30<00:25,  1.30it/s] 89%|████████▉ | 268/300 [03:31<00:24,  1.29it/s] 90%|████████▉ | 269/300 [03:31<00:24,  1.28it/s] 90%|█████████ | 270/300 [03:32<00:23,  1.29it/s] 90%|█████████ | 271/300 [03:33<00:22,  1.27it/s] 91%|█████████ | 272/300 [03:34<00:21,  1.28it/s] 91%|█████████ | 273/300 [03:35<00:21,  1.27it/s] 91%|█████████▏| 274/300 [03:35<00:20,  1.28it/s] 92%|█████████▏| 275/300 [03:36<00:19,  1.29it/s] 92%|█████████▏| 276/300 [03:37<00:18,  1.30it/s] 92%|█████████▏| 277/300 [03:38<00:17,  1.30it/s] 93%|█████████▎| 278/300 [03:38<00:16,  1.31it/s] 93%|█████████▎| 279/300 [03:39<00:16,  1.30it/s] 93%|█████████▎| 280/300 [03:40<00:15,  1.30it/s] 94%|█████████▎| 281/300 [03:41<00:14,  1.29it/s] 94%|█████████▍| 282/300 [03:42<00:14,  1.28it/s] 94%|█████████▍| 283/300 [03:42<00:13,  1.29it/s] 95%|█████████▍| 284/300 [03:43<00:12,  1.29it/s] 95%|█████████▌| 285/300 [03:44<00:11,  1.29it/s] 95%|█████████▌| 286/300 [03:45<00:10,  1.29it/s] 96%|█████████▌| 287/300 [03:45<00:10,  1.29it/s] 96%|█████████▌| 288/300 [03:46<00:09,  1.28it/s] 96%|█████████▋| 289/300 [03:47<00:08,  1.28it/s] 97%|█████████▋| 290/300 [03:48<00:07,  1.27it/s] 97%|█████████▋| 291/300 [03:49<00:07,  1.26it/s] 97%|█████████▋| 292/300 [03:49<00:06,  1.26it/s] 98%|█████████▊| 293/300 [03:50<00:05,  1.25it/s] 98%|█████████▊| 294/300 [03:51<00:04,  1.26it/s] 98%|█████████▊| 295/300 [03:52<00:03,  1.27it/s] 99%|█████████▊| 296/300 [03:53<00:03,  1.26it/s] 99%|█████████▉| 297/300 [03:53<00:02,  1.27it/s] 99%|█████████▉| 298/300 [03:54<00:01,  1.27it/s]100%|█████████▉| 299/300 [03:55<00:00,  1.26it/s]100%|██████████| 300/300 [03:56<00:00,  1.27it/s]100%|██████████| 300/300 [03:56<00:00,  1.27it/s]
W&B disabled.
W&B online. Running your script from this directory will now sync to the cloud.
wandb: Currently logged in as: hl57. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.12 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.10
wandb: Run data is saved locally in /media/techt/One Touch/DD/SRe2L/train/wandb/run-20231023_152049-ro9kgg3b
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ruby-armadillo-491
wandb: ⭐️ View project at https://wandb.ai/hl57/final_rn18_fkd
wandb: 🚀 View run at https://wandb.ai/hl57/final_rn18_fkd/runs/ro9kgg3b
======= FKD: dataset info ======
path: ../relabel/FKD_cutmix_fp16/tiny-imagenet/313/
num img: 200
batch size: 100
max epoch: 300
================================
load data successfully
=> loading student model 'resnet18'
TRAIN Iter 0: lr = 0.001000,	loss = 0.008964,	Top-1 err = 99.500000,	Top-5 err = 97.000000,	train_time = 3.658158
TEST Iter 0: loss = 56.692791,	Top-1 err = 99.500000,	Top-5 err = 97.500000,	val_time = 19.405090
TRAIN Iter 10: lr = 0.000997,	loss = 0.007179,	Top-1 err = 99.500000,	Top-5 err = 96.500000,	train_time = 2.680245
TEST Iter 10: loss = 6.565467,	Top-1 err = 99.170000,	Top-5 err = 95.720000,	val_time = 18.950337
TRAIN Iter 20: lr = 0.000989,	loss = 0.006430,	Top-1 err = 100.000000,	Top-5 err = 93.000000,	train_time = 2.717433
TEST Iter 20: loss = 5.743300,	Top-1 err = 98.920000,	Top-5 err = 95.130000,	val_time = 19.305338
TRAIN Iter 30: lr = 0.000976,	loss = 0.007739,	Top-1 err = 97.500000,	Top-5 err = 89.500000,	train_time = 2.701285
TEST Iter 30: loss = 5.654273,	Top-1 err = 98.660000,	Top-5 err = 94.360000,	val_time = 19.608854
TRAIN Iter 40: lr = 0.000957,	loss = 0.006228,	Top-1 err = 97.500000,	Top-5 err = 90.500000,	train_time = 2.693735
TEST Iter 40: loss = 5.569757,	Top-1 err = 98.630000,	Top-5 err = 93.540000,	val_time = 19.212251
TRAIN Iter 50: lr = 0.000933,	loss = 0.006433,	Top-1 err = 99.500000,	Top-5 err = 94.000000,	train_time = 2.725071
TEST Iter 50: loss = 5.869516,	Top-1 err = 98.610000,	Top-5 err = 93.790000,	val_time = 19.378412
TRAIN Iter 60: lr = 0.000905,	loss = 0.006003,	Top-1 err = 97.500000,	Top-5 err = 89.500000,	train_time = 2.719090
TEST Iter 60: loss = 5.605209,	Top-1 err = 98.330000,	Top-5 err = 93.460000,	val_time = 19.204744
TRAIN Iter 70: lr = 0.000872,	loss = 0.006253,	Top-1 err = 94.000000,	Top-5 err = 83.000000,	train_time = 2.697485
TEST Iter 70: loss = 5.722694,	Top-1 err = 98.470000,	Top-5 err = 93.500000,	val_time = 18.979444
TRAIN Iter 80: lr = 0.000835,	loss = 0.006079,	Top-1 err = 96.000000,	Top-5 err = 85.500000,	train_time = 2.698051
TEST Iter 80: loss = 5.646096,	Top-1 err = 98.320000,	Top-5 err = 92.980000,	val_time = 19.253062
TRAIN Iter 90: lr = 0.000794,	loss = 0.005669,	Top-1 err = 97.000000,	Top-5 err = 84.000000,	train_time = 2.671700
TEST Iter 90: loss = 5.792009,	Top-1 err = 98.340000,	Top-5 err = 93.240000,	val_time = 19.084329
TRAIN Iter 100: lr = 0.000750,	loss = 0.006913,	Top-1 err = 93.500000,	Top-5 err = 80.000000,	train_time = 2.672357
TEST Iter 100: loss = 5.606813,	Top-1 err = 98.170000,	Top-5 err = 91.390000,	val_time = 19.167052
TRAIN Iter 110: lr = 0.000703,	loss = 0.005632,	Top-1 err = 95.000000,	Top-5 err = 83.500000,	train_time = 2.640466
TEST Iter 110: loss = 5.763051,	Top-1 err = 98.010000,	Top-5 err = 91.910000,	val_time = 19.305922
TRAIN Iter 120: lr = 0.000655,	loss = 0.005174,	Top-1 err = 97.500000,	Top-5 err = 88.000000,	train_time = 2.660466
TEST Iter 120: loss = 5.569355,	Top-1 err = 97.720000,	Top-5 err = 91.220000,	val_time = 19.421681
TRAIN Iter 130: lr = 0.000604,	loss = 0.005778,	Top-1 err = 90.000000,	Top-5 err = 74.500000,	train_time = 2.651173
TEST Iter 130: loss = 5.487554,	Top-1 err = 97.180000,	Top-5 err = 89.870000,	val_time = 19.300490
TRAIN Iter 140: lr = 0.000552,	loss = 0.005129,	Top-1 err = 93.500000,	Top-5 err = 79.000000,	train_time = 2.668016
TEST Iter 140: loss = 5.543687,	Top-1 err = 97.360000,	Top-5 err = 90.100000,	val_time = 19.301708
TRAIN Iter 150: lr = 0.000500,	loss = 0.005559,	Top-1 err = 92.000000,	Top-5 err = 80.500000,	train_time = 2.703630
TEST Iter 150: loss = 5.623471,	Top-1 err = 97.460000,	Top-5 err = 90.290000,	val_time = 19.365413
TRAIN Iter 160: lr = 0.000448,	loss = 0.005690,	Top-1 err = 89.500000,	Top-5 err = 76.000000,	train_time = 2.636888
TEST Iter 160: loss = 5.416874,	Top-1 err = 97.240000,	Top-5 err = 89.470000,	val_time = 18.761946
TRAIN Iter 170: lr = 0.000396,	loss = 0.005491,	Top-1 err = 88.500000,	Top-5 err = 73.000000,	train_time = 2.632416
TEST Iter 170: loss = 5.319485,	Top-1 err = 97.130000,	Top-5 err = 88.860000,	val_time = 19.305510
TRAIN Iter 180: lr = 0.000345,	loss = 0.005083,	Top-1 err = 89.000000,	Top-5 err = 73.500000,	train_time = 2.729314
TEST Iter 180: loss = 5.437096,	Top-1 err = 96.930000,	Top-5 err = 88.880000,	val_time = 19.289495
TRAIN Iter 190: lr = 0.000297,	loss = 0.005214,	Top-1 err = 90.000000,	Top-5 err = 72.000000,	train_time = 2.685635
TEST Iter 190: loss = 5.311231,	Top-1 err = 96.400000,	Top-5 err = 87.020000,	val_time = 19.419213
TRAIN Iter 200: lr = 0.000250,	loss = 0.005776,	Top-1 err = 76.000000,	Top-5 err = 56.000000,	train_time = 2.680744
TEST Iter 200: loss = 5.314889,	Top-1 err = 96.740000,	Top-5 err = 87.170000,	val_time = 19.319412
TRAIN Iter 210: lr = 0.000206,	loss = 0.004956,	Top-1 err = 82.500000,	Top-5 err = 59.500000,	train_time = 2.584605
TEST Iter 210: loss = 5.266867,	Top-1 err = 96.410000,	Top-5 err = 86.850000,	val_time = 19.088501
TRAIN Iter 220: lr = 0.000165,	loss = 0.004462,	Top-1 err = 93.000000,	Top-5 err = 81.500000,	train_time = 2.658913
TEST Iter 220: loss = 5.213920,	Top-1 err = 96.270000,	Top-5 err = 86.370000,	val_time = 19.599223
TRAIN Iter 230: lr = 0.000128,	loss = 0.005028,	Top-1 err = 92.000000,	Top-5 err = 83.000000,	train_time = 2.653895
TEST Iter 230: loss = 5.268490,	Top-1 err = 96.300000,	Top-5 err = 86.410000,	val_time = 19.034691
TRAIN Iter 240: lr = 0.000095,	loss = 0.004669,	Top-1 err = 94.000000,	Top-5 err = 83.500000,	train_time = 2.686153
TEST Iter 240: loss = 5.168587,	Top-1 err = 96.020000,	Top-5 err = 85.830000,	val_time = 19.422488
TRAIN Iter 250: lr = 0.000067,	loss = 0.004832,	Top-1 err = 80.000000,	Top-5 err = 56.500000,	train_time = 2.658704
TEST Iter 250: loss = 5.170557,	Top-1 err = 95.980000,	Top-5 err = 85.780000,	val_time = 19.269228
TRAIN Iter 260: lr = 0.000043,	loss = 0.005168,	Top-1 err = 77.000000,	Top-5 err = 50.500000,	train_time = 2.653371
TEST Iter 260: loss = 5.187853,	Top-1 err = 95.920000,	Top-5 err = 85.620000,	val_time = 19.361420
TRAIN Iter 270: lr = 0.000024,	loss = 0.004627,	Top-1 err = 81.000000,	Top-5 err = 57.000000,	train_time = 2.677930
TEST Iter 270: loss = 5.204301,	Top-1 err = 96.060000,	Top-5 err = 85.830000,	val_time = 19.254503
TRAIN Iter 280: lr = 0.000011,	loss = 0.005084,	Top-1 err = 76.000000,	Top-5 err = 52.500000,	train_time = 2.668044
TEST Iter 280: loss = 5.193041,	Top-1 err = 96.010000,	Top-5 err = 85.400000,	val_time = 18.959783
TRAIN Iter 290: lr = 0.000003,	loss = 0.004974,	Top-1 err = 77.000000,	Top-5 err = 55.000000,	train_time = 2.636285
TEST Iter 290: loss = 5.189197,	Top-1 err = 96.060000,	Top-5 err = 85.470000,	val_time = 19.264689
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.019 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.019 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:  train/Top1 ▁▁▁▁▁▁▁▂▁▁▂▂▂▃▂▄▂▂▂▃▃▅▄▅▃▃▂▃▂▅▃▅▄▂▇▅█▄▆▄
wandb:  train/Top5 ▁▁▁▂▂▂▂▂▂▂▂▃▃▄▃▅▂▃▃▄▃▅▅▆▄▄▂▄▂▆▄▆▅▃▆▆█▅▆▅
wandb: train/epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  train/loss ▅▄▄█▇▅▅▅▃▃▄▄▃▄▃▄▂▂▃▂▂▃▃▃▁▂▂▁▂▁▁▂▁▁▁▁▂▁▂▁
wandb:    train/lr ███████▇▇▇▇▇▇▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁
wandb:   val/epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███
wandb:    val/loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    val/top1 ▁▂▂▃▃▃▃▃▃▃▄▄▄▆▅▅▅▆▆▇▆▇▇▇███████
wandb:    val/top5 ▁▂▂▃▃▃▃▃▄▃▅▄▅▅▅▅▆▆▆▇▇▇▇▇███████
wandb: 
wandb: Run summary:
wandb:  train/Top1 14.5
wandb:  train/Top5 36.0
wandb: train/epoch 299
wandb:  train/loss 0.0049
wandb:    train/lr 0.0
wandb:   val/epoch 299
wandb:    val/loss 5.19379
wandb:    val/top1 4.01
wandb:    val/top5 14.57
wandb: 
wandb: 🚀 View run ruby-armadillo-491 at: https://wandb.ai/hl57/final_rn18_fkd/runs/ro9kgg3b
wandb: ️⚡ View job at https://wandb.ai/hl57/final_rn18_fkd/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjk3MzkyNjU3/version_details/v47
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231023_152049-ro9kgg3b/logs
TEST Iter 299: loss = 5.193792,	Top-1 err = 95.990000,	Top-5 err = 85.430000,	val_time = 19.264611
