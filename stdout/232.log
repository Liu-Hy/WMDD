/home/techt/anaconda3/envs/hl/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/techt/anaconda3/envs/hl/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
/home/techt/anaconda3/envs/hl/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
r_bn:  1.0
lr:  0.1
bc shape torch.Size([200, 10, 512])
get_images call
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 503.64951107322423
main criterion 131.95853206931798
weighted_aux_loss 371.69097900390625
loss_r_bn_feature 371.69097900390625
Verifier accuracy:  0.0
------------iteration 100----------
total loss 210.96788064609862
main criterion 41.94340554844237
weighted_aux_loss 169.02447509765625
loss_r_bn_feature 169.02447509765625
Verifier accuracy:  1.0
------------iteration 200----------
total loss 163.0422053282112
main criterion 38.90696668075026
weighted_aux_loss 124.13523864746094
loss_r_bn_feature 124.13523864746094
Verifier accuracy:  0.0
------------iteration 300----------
total loss 151.41358022557105
main criterion 48.16284017430152
weighted_aux_loss 103.25074005126953
loss_r_bn_feature 103.25074005126953
Verifier accuracy:  0.0
------------iteration 400----------
total loss 130.7947952642264
main criterion 39.693560828191245
weighted_aux_loss 91.10123443603516
loss_r_bn_feature 91.10123443603516
Verifier accuracy:  0.0
------------iteration 500----------
total loss 319.3866950674277
main criterion 108.93491284086518
weighted_aux_loss 210.4517822265625
loss_r_bn_feature 210.4517822265625
Verifier accuracy:  0.0
------------iteration 600----------
total loss 99.55151010294762
main criterion 36.063209779461296
weighted_aux_loss 63.48830032348633
loss_r_bn_feature 63.48830032348633
Verifier accuracy:  0.0
------------iteration 700----------
total loss 95.23565226525133
main criterion 36.210776624382184
weighted_aux_loss 59.02487564086914
loss_r_bn_feature 59.02487564086914
Verifier accuracy:  0.0
------------iteration 800----------
total loss 104.94491417482166
main criterion 36.510954213884155
weighted_aux_loss 68.4339599609375
loss_r_bn_feature 68.4339599609375
Verifier accuracy:  0.0
------------iteration 900----------
total loss 85.63181712002049
main criterion 37.21728159755955
weighted_aux_loss 48.41453552246094
loss_r_bn_feature 48.41453552246094
Verifier accuracy:  0.0
------------iteration 1000----------
total loss 77.42078629395043
main criterion 31.177824410405506
weighted_aux_loss 46.24296188354492
loss_r_bn_feature 46.24296188354492
Verifier accuracy:  1.0
------------iteration 1100----------
total loss 142.27981640317412
main criterion 59.62884594418975
weighted_aux_loss 82.65097045898438
loss_r_bn_feature 82.65097045898438
Verifier accuracy:  1.0
------------iteration 1200----------
total loss 80.25892539184309
main criterion 34.8945340649388
weighted_aux_loss 45.3643913269043
loss_r_bn_feature 45.3643913269043
Verifier accuracy:  0.0
------------iteration 1300----------
total loss 82.69819000017185
main criterion 37.424618985767545
weighted_aux_loss 45.2735710144043
loss_r_bn_feature 45.2735710144043
Verifier accuracy:  0.0
------------iteration 1400----------
total loss 64.57281587263118
main criterion 27.20604417463313
weighted_aux_loss 37.36677169799805
loss_r_bn_feature 37.36677169799805
Verifier accuracy:  0.0
------------iteration 1500----------
total loss 66.2895468207293
main criterion 31.319583746998834
weighted_aux_loss 34.96996307373047
loss_r_bn_feature 34.96996307373047
Verifier accuracy:  0.0
------------iteration 1600----------
total loss 72.70644843535055
main criterion 35.661496042772434
weighted_aux_loss 37.044952392578125
loss_r_bn_feature 37.044952392578125
Verifier accuracy:  0.0
------------iteration 1700----------
total loss 55.64269973497023
main criterion 26.0361132786714
weighted_aux_loss 29.606586456298828
loss_r_bn_feature 29.606586456298828
Verifier accuracy:  0.0
------------iteration 1800----------
total loss 62.54547292508083
main criterion 29.27417547024685
weighted_aux_loss 33.271297454833984
loss_r_bn_feature 33.271297454833984
Verifier accuracy:  0.0
------------iteration 1900----------
total loss 55.24420540751532
main criterion 23.931955270186215
weighted_aux_loss 31.3122501373291
loss_r_bn_feature 31.3122501373291
Verifier accuracy:  0.0
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 578.1405264466396
main criterion 129.3187491028896
weighted_aux_loss 448.82177734375
loss_r_bn_feature 448.82177734375
Verifier accuracy:  0.0
------------iteration 100----------
total loss 205.46076820973855
main criterion 40.26710365895729
weighted_aux_loss 165.19366455078125
loss_r_bn_feature 165.19366455078125
Verifier accuracy:  0.0
------------iteration 200----------
total loss 178.17751536594096
main criterion 45.8070930026597
weighted_aux_loss 132.37042236328125
loss_r_bn_feature 132.37042236328125
Verifier accuracy:  0.0
------------iteration 300----------
total loss 171.65276328554376
main criterion 67.39152518739921
weighted_aux_loss 104.26123809814453
loss_r_bn_feature 104.26123809814453
Verifier accuracy:  0.0
------------iteration 400----------
total loss 155.59102406895744
main criterion 51.90415730870353
weighted_aux_loss 103.6868667602539
loss_r_bn_feature 103.6868667602539
Verifier accuracy:  0.0
------------iteration 500----------
total loss 162.48257775366062
main criterion 66.73569054662936
weighted_aux_loss 95.74688720703125
loss_r_bn_feature 95.74688720703125
Verifier accuracy:  0.0
------------iteration 600----------
total loss 145.54648386559595
main criterion 64.0369013460647
weighted_aux_loss 81.50958251953125
loss_r_bn_feature 81.50958251953125
Verifier accuracy:  0.0
------------iteration 700----------
total loss 115.48974849586425
main criterion 45.51049281959472
weighted_aux_loss 69.97925567626953
loss_r_bn_feature 69.97925567626953
Verifier accuracy:  0.0
------------iteration 800----------
total loss 94.07634121627746
main criterion 38.647901122283315
weighted_aux_loss 55.42844009399414
loss_r_bn_feature 55.42844009399414
Verifier accuracy:  0.0
------------iteration 900----------
total loss 97.38618348528681
main criterion 38.03254960467157
weighted_aux_loss 59.353633880615234
loss_r_bn_feature 59.353633880615234
Verifier accuracy:  0.0
------------iteration 1000----------
total loss 88.53113026898043
main criterion 37.572328999449184
weighted_aux_loss 50.95880126953125
loss_r_bn_feature 50.95880126953125
Verifier accuracy:  0.0
------------iteration 1100----------
total loss 75.9004373939832
main criterion 30.70718101580936
weighted_aux_loss 45.19325637817383
loss_r_bn_feature 45.19325637817383
Verifier accuracy:  0.0
------------iteration 1200----------
total loss 81.33739844778668
main criterion 38.09144584158551
weighted_aux_loss 43.24595260620117
loss_r_bn_feature 43.24595260620117
Verifier accuracy:  0.0
------------iteration 1300----------
total loss 68.66540612407263
main criterion 28.8670357627445
weighted_aux_loss 39.798370361328125
loss_r_bn_feature 39.798370361328125
Verifier accuracy:  0.0
------------iteration 1400----------
total loss 77.47997109669305
main criterion 36.81533639210321
weighted_aux_loss 40.664634704589844
loss_r_bn_feature 40.664634704589844
Verifier accuracy:  0.0
------------iteration 1500----------
total loss 86.77852285806807
main criterion 41.88140524332198
weighted_aux_loss 44.897117614746094
loss_r_bn_feature 44.897117614746094
Verifier accuracy:  0.0
------------iteration 1600----------
total loss 60.89689138821771
main criterion 27.167071136752867
weighted_aux_loss 33.729820251464844
loss_r_bn_feature 33.729820251464844
Verifier accuracy:  0.0
------------iteration 1700----------
total loss 58.81900566740671
main criterion 24.869386374193816
weighted_aux_loss 33.94961929321289
loss_r_bn_feature 33.94961929321289
Verifier accuracy:  0.0
------------iteration 1800----------
total loss 57.31168468160945
main criterion 24.159901692107493
weighted_aux_loss 33.15178298950195
loss_r_bn_feature 33.15178298950195
Verifier accuracy:  0.0
------------iteration 1900----------
total loss 59.78130285587445
main criterion 26.057921813393982
weighted_aux_loss 33.72338104248047
loss_r_bn_feature 33.72338104248047
Verifier accuracy:  0.0
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 539.5207084831102
main criterion 134.79014818037587
weighted_aux_loss 404.7305603027344
loss_r_bn_feature 404.7305603027344
Verifier accuracy:  0.0
------------iteration 100----------
total loss 232.22612457189095
main criterion 49.60524444493783
weighted_aux_loss 182.62088012695312
loss_r_bn_feature 182.62088012695312
Verifier accuracy:  0.0
------------iteration 200----------
total loss 184.9639855603874
main criterion 49.336330531090546
weighted_aux_loss 135.62765502929688
loss_r_bn_feature 135.62765502929688
Verifier accuracy:  0.0
------------iteration 300----------
total loss 152.07532674038785
main criterion 50.29966908657926
weighted_aux_loss 101.7756576538086
loss_r_bn_feature 101.7756576538086
Verifier accuracy:  0.0
------------iteration 400----------
total loss 183.71620824724283
main criterion 80.65785863786782
weighted_aux_loss 103.058349609375
loss_r_bn_feature 103.058349609375
Verifier accuracy:  0.0
------------iteration 500----------
total loss 119.31456467633654
main criterion 39.9555711460631
weighted_aux_loss 79.35899353027344
loss_r_bn_feature 79.35899353027344
Verifier accuracy:  0.0
------------iteration 600----------
total loss 119.19855636791705
main criterion 39.26815933422564
weighted_aux_loss 79.9303970336914
loss_r_bn_feature 79.9303970336914
Verifier accuracy:  0.0
------------iteration 700----------
total loss 96.34008537557207
main criterion 39.404992449546675
weighted_aux_loss 56.93509292602539
loss_r_bn_feature 56.93509292602539
Verifier accuracy:  0.0
------------iteration 800----------
total loss 96.01572542034351
main criterion 33.709424303400155
weighted_aux_loss 62.30630111694336
loss_r_bn_feature 62.30630111694336
Verifier accuracy:  0.0
------------iteration 900----------
total loss 85.07190711229366
main criterion 34.44439322679561
weighted_aux_loss 50.62751388549805
loss_r_bn_feature 50.62751388549805
Verifier accuracy:  0.0
------------iteration 1000----------
total loss 161.34085936621057
main criterion 68.59089751318324
weighted_aux_loss 92.74996185302734
loss_r_bn_feature 92.74996185302734
Verifier accuracy:  0.0
------------iteration 1100----------
total loss 74.81807966657406
main criterion 32.87977095075375
weighted_aux_loss 41.93830871582031
loss_r_bn_feature 41.93830871582031
Verifier accuracy:  0.0
------------iteration 1200----------
total loss 78.24854631949648
main criterion 34.87153597403749
weighted_aux_loss 43.377010345458984
loss_r_bn_feature 43.377010345458984
Verifier accuracy:  0.0
------------iteration 1300----------
total loss 74.0167030898649
main criterion 36.48726125636882
weighted_aux_loss 37.529441833496094
loss_r_bn_feature 37.529441833496094
Verifier accuracy:  0.0
------------iteration 1400----------
total loss 84.27860925544746
main criterion 43.77035425056465
weighted_aux_loss 40.50825500488281
loss_r_bn_feature 40.50825500488281
Verifier accuracy:  0.0
------------iteration 1500----------
total loss 66.27027062128776
main criterion 33.0998452062487
weighted_aux_loss 33.17042541503906
loss_r_bn_feature 33.17042541503906
Verifier accuracy:  0.0
------------iteration 1600----------
total loss 120.67328848252463
main criterion 53.14889730820822
weighted_aux_loss 67.5243911743164
loss_r_bn_feature 67.5243911743164
Verifier accuracy:  0.0
------------iteration 1700----------
total loss 52.73443289787981
main criterion 25.430325231864185
weighted_aux_loss 27.304107666015625
loss_r_bn_feature 27.304107666015625
Verifier accuracy:  0.0
------------iteration 1800----------
total loss 123.8935869717699
main criterion 49.01152215243396
weighted_aux_loss 74.88206481933594
loss_r_bn_feature 74.88206481933594
Verifier accuracy:  0.0
------------iteration 1900----------
total loss 100.0766289226016
main criterion 47.602381943841834
weighted_aux_loss 52.474246978759766
loss_r_bn_feature 52.474246978759766
Verifier accuracy:  0.0
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 540.355250461341
main criterion 136.60891257071597
weighted_aux_loss 403.746337890625
loss_r_bn_feature 403.746337890625
Verifier accuracy:  0.0
------------iteration 100----------
total loss 222.65442289074525
main criterion 55.51913846691712
weighted_aux_loss 167.13528442382812
loss_r_bn_feature 167.13528442382812
Verifier accuracy:  0.0
------------iteration 200----------
total loss 362.9404796233141
main criterion 99.24608265065783
weighted_aux_loss 263.69439697265625
loss_r_bn_feature 263.69439697265625
Verifier accuracy:  0.0
------------iteration 300----------
total loss 143.90398520604865
main criterion 43.45343131200567
weighted_aux_loss 100.45055389404297
loss_r_bn_feature 100.45055389404297
Verifier accuracy:  0.0
------------iteration 400----------
total loss 134.17585677957643
main criterion 40.57836837625612
weighted_aux_loss 93.59748840332031
loss_r_bn_feature 93.59748840332031
Verifier accuracy:  0.0
------------iteration 500----------
total loss 128.12138896821955
main criterion 51.62157970308284
weighted_aux_loss 76.49980926513672
loss_r_bn_feature 76.49980926513672
Verifier accuracy:  0.0
------------iteration 600----------
total loss 115.66072488698157
main criterion 47.18033243092688
weighted_aux_loss 68.48039245605469
loss_r_bn_feature 68.48039245605469
Verifier accuracy:  0.0
------------iteration 700----------
total loss 110.23716211804188
main criterion 46.70802927502431
weighted_aux_loss 63.52913284301758
loss_r_bn_feature 63.52913284301758
Verifier accuracy:  0.0
------------iteration 800----------
total loss 101.46102927616701
main criterion 40.38208793095216
weighted_aux_loss 61.078941345214844
loss_r_bn_feature 61.078941345214844
Verifier accuracy:  0.0
------------iteration 900----------
total loss 121.83346464542959
main criterion 55.9010458221874
weighted_aux_loss 65.93241882324219
loss_r_bn_feature 65.93241882324219
Verifier accuracy:  0.0
------------iteration 1000----------
total loss 75.7865495346642
main criterion 31.87549301610951
weighted_aux_loss 43.91105651855469
loss_r_bn_feature 43.91105651855469
Verifier accuracy:  0.0
------------iteration 1100----------
total loss 128.99658479231033
main criterion 59.77054872053298
weighted_aux_loss 69.22603607177734
loss_r_bn_feature 69.22603607177734
Verifier accuracy:  0.0
------------iteration 1200----------
total loss 75.4634510239789
main criterion 33.90428506938906
weighted_aux_loss 41.559165954589844
loss_r_bn_feature 41.559165954589844
Verifier accuracy:  0.0
------------iteration 1300----------
total loss 98.1125078494787
main criterion 48.33754828526972
weighted_aux_loss 49.774959564208984
loss_r_bn_feature 49.774959564208984
Verifier accuracy:  0.0
------------iteration 1400----------
total loss 66.24283171326937
main criterion 31.4154582086307
weighted_aux_loss 34.82737350463867
loss_r_bn_feature 34.82737350463867
Verifier accuracy:  0.0
------------iteration 1500----------
total loss 67.90467783680927
main criterion 30.574248759660826
weighted_aux_loss 37.33042907714844
loss_r_bn_feature 37.33042907714844
Verifier accuracy:  0.0
------------iteration 1600----------
total loss 58.645830489813974
main criterion 26.087652541815928
weighted_aux_loss 32.55817794799805
loss_r_bn_feature 32.55817794799805
Verifier accuracy:  0.0
------------iteration 1700----------
total loss 60.219349546026066
main criterion 27.884159726690125
weighted_aux_loss 32.33518981933594
loss_r_bn_feature 32.33518981933594
Verifier accuracy:  0.0
------------iteration 1800----------
total loss 78.51987770515983
main criterion 38.101981434407875
weighted_aux_loss 40.41789627075195
loss_r_bn_feature 40.41789627075195
Verifier accuracy:  0.0
------------iteration 1900----------
total loss 71.79182848095066
main criterion 33.17273363231784
weighted_aux_loss 38.61909484863281
loss_r_bn_feature 38.61909484863281
Verifier accuracy:  0.0
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 552.918878177055
main criterion 131.853448489555
weighted_aux_loss 421.0654296875
loss_r_bn_feature 421.0654296875
Verifier accuracy:  0.0
------------iteration 100----------
total loss 209.5058499771023
main criterion 48.48891272124292
weighted_aux_loss 161.01693725585938
loss_r_bn_feature 161.01693725585938
Verifier accuracy:  0.0
------------iteration 200----------
total loss 240.85402920212124
main criterion 91.71731045212124
weighted_aux_loss 149.13671875
loss_r_bn_feature 149.13671875
Verifier accuracy:  0.0
------------iteration 300----------
total loss 129.82212727765005
main criterion 45.34483235577506
weighted_aux_loss 84.477294921875
loss_r_bn_feature 84.477294921875
Verifier accuracy:  0.0
------------iteration 400----------
total loss 144.71595050015782
main criterion 41.71815539517736
weighted_aux_loss 102.99779510498047
loss_r_bn_feature 102.99779510498047
Verifier accuracy:  0.0
------------iteration 500----------
total loss 181.1942605567953
main criterion 84.521088559725
weighted_aux_loss 96.67317199707031
loss_r_bn_feature 96.67317199707031
Verifier accuracy:  0.0
------------iteration 600----------
total loss 103.98703452443007
main criterion 36.63863822316054
weighted_aux_loss 67.34839630126953
loss_r_bn_feature 67.34839630126953
Verifier accuracy:  0.0
------------iteration 700----------
total loss 117.48083819414057
main criterion 51.57309283281244
weighted_aux_loss 65.90774536132812
loss_r_bn_feature 65.90774536132812
Verifier accuracy:  0.0
------------iteration 800----------
total loss 92.2394804275245
main criterion 34.90859740140145
weighted_aux_loss 57.33088302612305
loss_r_bn_feature 57.33088302612305
Verifier accuracy:  0.0
------------iteration 900----------
total loss 151.28298361501922
main criterion 59.24318869314421
weighted_aux_loss 92.039794921875
loss_r_bn_feature 92.039794921875
Verifier accuracy:  0.0
------------iteration 1000----------
total loss 92.00439272086166
main criterion 40.842512227697604
weighted_aux_loss 51.16188049316406
loss_r_bn_feature 51.16188049316406
Verifier accuracy:  0.0
------------iteration 1100----------
total loss 85.92248955722428
main criterion 32.431747827487946
weighted_aux_loss 53.49074172973633
loss_r_bn_feature 53.49074172973633
Verifier accuracy:  1.0
------------iteration 1200----------
total loss 94.17217361022519
main criterion 44.36391555358456
weighted_aux_loss 49.808258056640625
loss_r_bn_feature 49.808258056640625
Verifier accuracy:  0.0
------------iteration 1300----------
total loss 75.52292229014702
main criterion 32.426162494004444
weighted_aux_loss 43.09675979614258
loss_r_bn_feature 43.09675979614258
Verifier accuracy:  1.0
------------iteration 1400----------
total loss 73.55851523073552
main criterion 35.626035372337086
weighted_aux_loss 37.93247985839844
loss_r_bn_feature 37.93247985839844
Verifier accuracy:  1.0
------------iteration 1500----------
total loss 58.89927822398826
main criterion 26.163415492054664
weighted_aux_loss 32.735862731933594
loss_r_bn_feature 32.735862731933594
Verifier accuracy:  1.0
------------iteration 1600----------
total loss 57.83957379694951
main criterion 26.81368153925908
weighted_aux_loss 31.02589225769043
loss_r_bn_feature 31.02589225769043
Verifier accuracy:  1.0
------------iteration 1700----------
total loss 74.49476167041271
main criterion 36.17228242236583
weighted_aux_loss 38.322479248046875
loss_r_bn_feature 38.322479248046875
Verifier accuracy:  1.0
------------iteration 1800----------
total loss 77.42323437971555
main criterion 37.08833066267453
weighted_aux_loss 40.334903717041016
loss_r_bn_feature 40.334903717041016
Verifier accuracy:  1.0
------------iteration 1900----------
total loss 50.30584786475009
main criterion 23.866192560795017
weighted_aux_loss 26.439655303955078
loss_r_bn_feature 26.439655303955078
Verifier accuracy:  1.0
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 503.70063694688236
main criterion 136.9137106773511
weighted_aux_loss 366.78692626953125
loss_r_bn_feature 366.78692626953125
Verifier accuracy:  0.0
------------iteration 100----------
total loss 222.3786437636546
main criterion 45.477795374982726
weighted_aux_loss 176.90084838867188
loss_r_bn_feature 176.90084838867188
Verifier accuracy:  0.0
------------iteration 200----------
total loss 185.41913919211783
main criterion 54.2336075759069
weighted_aux_loss 131.18553161621094
loss_r_bn_feature 131.18553161621094
Verifier accuracy:  0.0
------------iteration 300----------
total loss 132.8089248142179
main criterion 39.90153040503821
weighted_aux_loss 92.90739440917969
loss_r_bn_feature 92.90739440917969
Verifier accuracy:  0.0
------------iteration 400----------
total loss 136.31753440177764
main criterion 46.54903312003936
weighted_aux_loss 89.76850128173828
loss_r_bn_feature 89.76850128173828
Verifier accuracy:  0.0
------------iteration 500----------
total loss 114.04363257522505
main criterion 39.42205817336958
weighted_aux_loss 74.62157440185547
loss_r_bn_feature 74.62157440185547
Verifier accuracy:  0.0
------------iteration 600----------
total loss 110.34611084751779
main criterion 42.05941345982249
weighted_aux_loss 68.28669738769531
loss_r_bn_feature 68.28669738769531
Verifier accuracy:  0.0
------------iteration 700----------
total loss 175.5468209725446
main criterion 81.70259032068914
weighted_aux_loss 93.84423065185547
loss_r_bn_feature 93.84423065185547
Verifier accuracy:  0.0
------------iteration 800----------
total loss 104.99768379681478
main criterion 46.78013542645346
weighted_aux_loss 58.21754837036133
loss_r_bn_feature 58.21754837036133
Verifier accuracy:  0.0
------------iteration 900----------
total loss 78.86099884939773
main criterion 35.04604599905594
weighted_aux_loss 43.8149528503418
loss_r_bn_feature 43.8149528503418
Verifier accuracy:  0.0
------------iteration 1000----------
total loss 81.53496573931767
main criterion 34.777752146788366
weighted_aux_loss 46.7572135925293
loss_r_bn_feature 46.7572135925293
Verifier accuracy:  0.0
------------iteration 1100----------
total loss 97.92028027314694
main criterion 50.07211285371334
weighted_aux_loss 47.848167419433594
loss_r_bn_feature 47.848167419433594
Verifier accuracy:  0.0
------------iteration 1200----------
total loss 103.96668424050907
main criterion 52.51857175271609
weighted_aux_loss 51.44811248779297
loss_r_bn_feature 51.44811248779297
Verifier accuracy:  0.0
------------iteration 1300----------
total loss 73.06907117665538
main criterion 32.71182096303234
weighted_aux_loss 40.35725021362305
loss_r_bn_feature 40.35725021362305
Verifier accuracy:  0.0
------------iteration 1400----------
total loss 65.34076556628318
main criterion 29.765150637083966
weighted_aux_loss 35.57561492919922
loss_r_bn_feature 35.57561492919922
Verifier accuracy:  0.0
------------iteration 1500----------
total loss 80.9455658441542
main criterion 40.477285051917875
weighted_aux_loss 40.46828079223633
loss_r_bn_feature 40.46828079223633
Verifier accuracy:  0.0
------------iteration 1600----------
total loss 61.04124648460529
main criterion 27.54233367332599
weighted_aux_loss 33.4989128112793
loss_r_bn_feature 33.4989128112793
Verifier accuracy:  0.0
------------iteration 1700----------
total loss 56.62528632986367
main criterion 25.811928022978904
weighted_aux_loss 30.813358306884766
loss_r_bn_feature 30.813358306884766
Verifier accuracy:  0.0
------------iteration 1800----------
total loss 155.9845304628977
main criterion 63.17106152979223
weighted_aux_loss 92.81346893310547
loss_r_bn_feature 92.81346893310547
Verifier accuracy:  0.0
------------iteration 1900----------
total loss 148.66125143258805
main criterion 58.83541525094743
weighted_aux_loss 89.82583618164062
loss_r_bn_feature 89.82583618164062
Verifier accuracy:  0.0
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 548.444432174156
main criterion 142.97256938118727
weighted_aux_loss 405.47186279296875
loss_r_bn_feature 405.47186279296875
Verifier accuracy:  0.0
------------iteration 100----------
total loss 280.3949095975265
main criterion 68.25024101842499
weighted_aux_loss 212.14466857910156
loss_r_bn_feature 212.14466857910156
Verifier accuracy:  0.0
------------iteration 200----------
total loss 220.48626341025036
main criterion 65.60290159384411
weighted_aux_loss 154.88336181640625
loss_r_bn_feature 154.88336181640625
Verifier accuracy:  0.0
------------iteration 300----------
total loss 159.88991259448974
main criterion 39.36895464771239
weighted_aux_loss 120.52095794677734
loss_r_bn_feature 120.52095794677734
Verifier accuracy:  0.0
------------iteration 400----------
total loss 137.192726845939
main criterion 49.36701273461088
weighted_aux_loss 87.82571411132812
loss_r_bn_feature 87.82571411132812
Verifier accuracy:  0.0
------------iteration 500----------
total loss 103.96206116564234
main criterion 34.776666878532964
weighted_aux_loss 69.18539428710938
loss_r_bn_feature 69.18539428710938
Verifier accuracy:  0.0
------------iteration 600----------
total loss 123.20192051155009
main criterion 51.9361276648704
weighted_aux_loss 71.26579284667969
loss_r_bn_feature 71.26579284667969
Verifier accuracy:  0.0
------------iteration 700----------
total loss 110.56080942873331
main criterion 37.60418253664347
weighted_aux_loss 72.95662689208984
loss_r_bn_feature 72.95662689208984
Verifier accuracy:  0.0
------------iteration 800----------
total loss 96.84751874589477
main criterion 36.05960828446899
weighted_aux_loss 60.78791046142578
loss_r_bn_feature 60.78791046142578
Verifier accuracy:  0.0
------------iteration 900----------
total loss 90.47631617361353
main criterion 32.58053751760767
weighted_aux_loss 57.89577865600586
loss_r_bn_feature 57.89577865600586
Verifier accuracy:  0.0
------------iteration 1000----------
total loss 90.80579272276808
main criterion 38.49737063414503
weighted_aux_loss 52.30842208862305
loss_r_bn_feature 52.30842208862305
Verifier accuracy:  0.0
------------iteration 1100----------
total loss 83.26430175990846
main criterion 33.513088686177994
weighted_aux_loss 49.75121307373047
loss_r_bn_feature 49.75121307373047
Verifier accuracy:  0.0
------------iteration 1200----------
total loss 72.54306228497084
main criterion 29.79749114849624
weighted_aux_loss 42.74557113647461
loss_r_bn_feature 42.74557113647461
Verifier accuracy:  0.0
------------iteration 1300----------
total loss 70.50399822171804
main criterion 29.738941611610617
weighted_aux_loss 40.76505661010742
loss_r_bn_feature 40.76505661010742
Verifier accuracy:  0.0
------------iteration 1400----------
total loss 80.32023808517383
main criterion 36.77348134078906
weighted_aux_loss 43.546756744384766
loss_r_bn_feature 43.546756744384766
Verifier accuracy:  0.0
------------iteration 1500----------
total loss 63.16117268688902
main criterion 27.756612597777693
weighted_aux_loss 35.40456008911133
loss_r_bn_feature 35.40456008911133
Verifier accuracy:  0.0
------------iteration 1600----------
total loss 68.8919433240395
main criterion 32.56665264410786
weighted_aux_loss 36.32529067993164
loss_r_bn_feature 36.32529067993164
Verifier accuracy:  0.0
------------iteration 1700----------
total loss 65.75440616020177
main criterion 28.945743745650983
weighted_aux_loss 36.80866241455078
loss_r_bn_feature 36.80866241455078
Verifier accuracy:  0.0
------------iteration 1800----------
total loss 88.4174281021156
main criterion 39.435624208072625
weighted_aux_loss 48.98180389404297
loss_r_bn_feature 48.98180389404297
Verifier accuracy:  0.0
------------iteration 1900----------
total loss 91.56130480475444
main criterion 41.47781252570171
weighted_aux_loss 50.083492279052734
loss_r_bn_feature 50.083492279052734
Verifier accuracy:  0.0
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 583.2923322010256
main criterion 131.4900861072756
weighted_aux_loss 451.80224609375
loss_r_bn_feature 451.80224609375
Verifier accuracy:  0.0
------------iteration 100----------
total loss 269.5871930013868
main criterion 47.02827881681649
weighted_aux_loss 222.5589141845703
loss_r_bn_feature 222.5589141845703
Verifier accuracy:  0.0
------------iteration 200----------
total loss 175.640524335659
main criterion 55.751257367885565
weighted_aux_loss 119.88926696777344
loss_r_bn_feature 119.88926696777344
Verifier accuracy:  0.0
------------iteration 300----------
total loss 199.95409784062332
main criterion 59.582454774217055
weighted_aux_loss 140.37164306640625
loss_r_bn_feature 140.37164306640625
Verifier accuracy:  1.0
------------iteration 400----------
total loss 138.436128694942
main criterion 40.62468918078185
weighted_aux_loss 97.81143951416016
loss_r_bn_feature 97.81143951416016
Verifier accuracy:  0.0
------------iteration 500----------
total loss 112.95295877076735
main criterion 37.506677337661884
weighted_aux_loss 75.44628143310547
loss_r_bn_feature 75.44628143310547
Verifier accuracy:  0.0
------------iteration 600----------
total loss 113.08271132701893
main criterion 38.46419631237049
weighted_aux_loss 74.61851501464844
loss_r_bn_feature 74.61851501464844
Verifier accuracy:  0.0
------------iteration 700----------
total loss 112.95887734569554
main criterion 43.24581124462132
weighted_aux_loss 69.71306610107422
loss_r_bn_feature 69.71306610107422
Verifier accuracy:  0.0
------------iteration 800----------
total loss 89.7108905378957
main criterion 34.01088977495623
weighted_aux_loss 55.70000076293945
loss_r_bn_feature 55.70000076293945
Verifier accuracy:  0.0
------------iteration 900----------
total loss 178.78501715405451
main criterion 69.11112799389826
weighted_aux_loss 109.67388916015625
loss_r_bn_feature 109.67388916015625
Verifier accuracy:  0.0
------------iteration 1000----------
total loss 104.02637139427146
main criterion 43.33900490867576
weighted_aux_loss 60.6873664855957
loss_r_bn_feature 60.6873664855957
Verifier accuracy:  0.0
------------iteration 1100----------
total loss 128.07335708087356
main criterion 52.59268996661575
weighted_aux_loss 75.48066711425781
loss_r_bn_feature 75.48066711425781
Verifier accuracy:  0.0
------------iteration 1200----------
total loss 119.75175471343621
main criterion 53.974662733455745
weighted_aux_loss 65.77709197998047
loss_r_bn_feature 65.77709197998047
Verifier accuracy:  0.0
------------iteration 1300----------
total loss 70.93233361698348
main criterion 28.44782510257919
weighted_aux_loss 42.4845085144043
loss_r_bn_feature 42.4845085144043
Verifier accuracy:  0.0
------------iteration 1400----------
total loss 84.52138627974998
main criterion 39.89098848311912
weighted_aux_loss 44.63039779663086
loss_r_bn_feature 44.63039779663086
Verifier accuracy:  0.0
------------iteration 1500----------
total loss 131.75836615633852
main criterion 57.184246588467424
weighted_aux_loss 74.5741195678711
loss_r_bn_feature 74.5741195678711
Verifier accuracy:  0.0
------------iteration 1600----------
total loss 61.59239616124623
main criterion 28.652260205435685
weighted_aux_loss 32.94013595581055
loss_r_bn_feature 32.94013595581055
Verifier accuracy:  0.0
------------iteration 1700----------
total loss 56.86215809484409
main criterion 25.69256619115757
weighted_aux_loss 31.169591903686523
loss_r_bn_feature 31.169591903686523
Verifier accuracy:  0.0
------------iteration 1800----------
total loss 63.176136962545385
main criterion 28.65769671596335
weighted_aux_loss 34.51844024658203
loss_r_bn_feature 34.51844024658203
Verifier accuracy:  0.0
------------iteration 1900----------
total loss 142.74228241766156
main criterion 62.20034363592326
weighted_aux_loss 80.54193878173828
loss_r_bn_feature 80.54193878173828
Verifier accuracy:  0.0
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 584.0200106913238
main criterion 132.20430634562067
weighted_aux_loss 451.8157043457031
loss_r_bn_feature 451.8157043457031
Verifier accuracy:  0.0
------------iteration 100----------
total loss 380.21852879983027
main criterion 76.70534520608028
weighted_aux_loss 303.51318359375
loss_r_bn_feature 303.51318359375
Verifier accuracy:  0.0
------------iteration 200----------
total loss 194.4354572687611
main criterion 47.79579357247204
weighted_aux_loss 146.63966369628906
loss_r_bn_feature 146.63966369628906
Verifier accuracy:  0.0
------------iteration 300----------
total loss 155.2849821434868
main criterion 38.86174911126024
weighted_aux_loss 116.42323303222656
loss_r_bn_feature 116.42323303222656
Verifier accuracy:  0.0
------------iteration 400----------
total loss 147.0538567183719
main criterion 42.457589018176584
weighted_aux_loss 104.59626770019531
loss_r_bn_feature 104.59626770019531
Verifier accuracy:  0.0
------------iteration 500----------
total loss 125.86779094412586
main criterion 37.73927379324696
weighted_aux_loss 88.1285171508789
loss_r_bn_feature 88.1285171508789
Verifier accuracy:  0.0
------------iteration 600----------
total loss 123.65824342247649
main criterion 50.60968995567962
weighted_aux_loss 73.04855346679688
loss_r_bn_feature 73.04855346679688
Verifier accuracy:  0.0
------------iteration 700----------
total loss 147.57889450680847
main criterion 56.43068588864439
weighted_aux_loss 91.14820861816406
loss_r_bn_feature 91.14820861816406
Verifier accuracy:  0.0
------------iteration 800----------
total loss 239.53953153633236
main criterion 85.39814359687924
weighted_aux_loss 154.14138793945312
loss_r_bn_feature 154.14138793945312
Verifier accuracy:  0.0
------------iteration 900----------
total loss 154.6883104175759
main criterion 67.64376238290795
weighted_aux_loss 87.04454803466797
loss_r_bn_feature 87.04454803466797
Verifier accuracy:  0.0
------------iteration 1000----------
total loss 86.4554492508692
main criterion 36.19762149208013
weighted_aux_loss 50.25782775878906
loss_r_bn_feature 50.25782775878906
Verifier accuracy:  0.0
------------iteration 1100----------
total loss 108.98995097105492
main criterion 47.48022349302759
weighted_aux_loss 61.509727478027344
loss_r_bn_feature 61.509727478027344
Verifier accuracy:  0.0
------------iteration 1200----------
total loss 82.67400201236978
main criterion 36.804235777018214
weighted_aux_loss 45.86976623535156
loss_r_bn_feature 45.86976623535156
Verifier accuracy:  0.0
------------iteration 1300----------
total loss 74.20684104381067
main criterion 34.832703165392694
weighted_aux_loss 39.37413787841797
loss_r_bn_feature 39.37413787841797
Verifier accuracy:  0.0
------------iteration 1400----------
total loss 73.47607956150084
main criterion 33.52974853733092
weighted_aux_loss 39.94633102416992
loss_r_bn_feature 39.94633102416992
Verifier accuracy:  0.0
------------iteration 1500----------
total loss 72.6931429482461
main criterion 33.267506656742206
weighted_aux_loss 39.425636291503906
loss_r_bn_feature 39.425636291503906
Verifier accuracy:  1.0
------------iteration 1600----------
total loss 76.52783587710418
main criterion 36.91508868472137
weighted_aux_loss 39.61274719238281
loss_r_bn_feature 39.61274719238281
Verifier accuracy:  0.0
------------iteration 1700----------
total loss 60.36036400406989
main criterion 27.667363210612862
weighted_aux_loss 32.69300079345703
loss_r_bn_feature 32.69300079345703
Verifier accuracy:  0.0
------------iteration 1800----------
total loss 54.66365616755901
main criterion 24.376346520952563
weighted_aux_loss 30.287309646606445
loss_r_bn_feature 30.287309646606445
Verifier accuracy:  1.0
------------iteration 1900----------
total loss 138.57597254309684
main criterion 59.03781031165152
weighted_aux_loss 79.53816223144531
loss_r_bn_feature 79.53816223144531
Verifier accuracy:  1.0
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 593.4783160503071
main criterion 131.62245057179146
weighted_aux_loss 461.8558654785156
loss_r_bn_feature 461.8558654785156
Verifier accuracy:  0.0
------------iteration 100----------
total loss 255.80709831088544
main criterion 45.09780876010418
weighted_aux_loss 210.70928955078125
loss_r_bn_feature 210.70928955078125
Verifier accuracy:  0.0
------------iteration 200----------
total loss 180.07134621647987
main criterion 45.457836084643944
weighted_aux_loss 134.61351013183594
loss_r_bn_feature 134.61351013183594
Verifier accuracy:  0.0
------------iteration 300----------
total loss 139.67605950923144
main criterion 42.42196252436814
weighted_aux_loss 97.25409698486328
loss_r_bn_feature 97.25409698486328
Verifier accuracy:  0.0
------------iteration 400----------
total loss 142.01482936702558
main criterion 41.695996969564646
weighted_aux_loss 100.31883239746094
loss_r_bn_feature 100.31883239746094
Verifier accuracy:  0.0
------------iteration 500----------
total loss 118.42238899211776
main criterion 40.506930312918534
weighted_aux_loss 77.91545867919922
loss_r_bn_feature 77.91545867919922
Verifier accuracy:  0.0
------------iteration 600----------
total loss 105.91193391675344
main criterion 43.571075457280784
weighted_aux_loss 62.340858459472656
loss_r_bn_feature 62.340858459472656
Verifier accuracy:  0.0
------------iteration 700----------
total loss 198.02819181556788
main criterion 69.86136747474757
weighted_aux_loss 128.1668243408203
loss_r_bn_feature 128.1668243408203
Verifier accuracy:  0.0
------------iteration 800----------
total loss 145.4978871030124
main criterion 57.9765247983249
weighted_aux_loss 87.5213623046875
loss_r_bn_feature 87.5213623046875
Verifier accuracy:  0.0
------------iteration 900----------
total loss 155.08111317819782
main criterion 62.41727956003376
weighted_aux_loss 92.66383361816406
loss_r_bn_feature 92.66383361816406
Verifier accuracy:  0.0
------------iteration 1000----------
total loss 81.3280496983359
main criterion 31.66029838608004
weighted_aux_loss 49.66775131225586
loss_r_bn_feature 49.66775131225586
Verifier accuracy:  0.0
------------iteration 1100----------
total loss 98.07752044033168
main criterion 42.68240172695277
weighted_aux_loss 55.395118713378906
loss_r_bn_feature 55.395118713378906
Verifier accuracy:  0.0
------------iteration 1200----------
total loss 80.95587816481265
main criterion 32.27173128370914
weighted_aux_loss 48.684146881103516
loss_r_bn_feature 48.684146881103516
Verifier accuracy:  0.0
------------iteration 1300----------
total loss 70.34778680068611
main criterion 29.490990536037675
weighted_aux_loss 40.85679626464844
loss_r_bn_feature 40.85679626464844
Verifier accuracy:  0.0
------------iteration 1400----------
total loss 93.24015912741645
main criterion 41.68788633078559
weighted_aux_loss 51.55227279663086
loss_r_bn_feature 51.55227279663086
Verifier accuracy:  0.0
------------iteration 1500----------
total loss 100.88807715654353
main criterion 45.61689032793025
weighted_aux_loss 55.27118682861328
loss_r_bn_feature 55.27118682861328
Verifier accuracy:  0.0
------------iteration 1600----------
total loss 75.55189710043922
main criterion 30.43748670004859
weighted_aux_loss 45.114410400390625
loss_r_bn_feature 45.114410400390625
Verifier accuracy:  0.0
------------iteration 1700----------
total loss 94.88515656325563
main criterion 44.0833911213611
weighted_aux_loss 50.80176544189453
loss_r_bn_feature 50.80176544189453
Verifier accuracy:  0.0
------------iteration 1800----------
total loss 60.15546231377054
main criterion 27.2965031157725
weighted_aux_loss 32.85895919799805
loss_r_bn_feature 32.85895919799805
Verifier accuracy:  0.0
------------iteration 1900----------
total loss 138.6862060729445
main criterion 50.33986207880386
weighted_aux_loss 88.34634399414062
loss_r_bn_feature 88.34634399414062
Verifier accuracy:  0.0
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 573.5262691756424
main criterion 134.53298304282987
weighted_aux_loss 438.9932861328125
loss_r_bn_feature 438.9932861328125
Verifier accuracy:  0.0
------------iteration 100----------
total loss 282.2691262891933
main criterion 45.57556854993552
weighted_aux_loss 236.6935577392578
loss_r_bn_feature 236.6935577392578
Verifier accuracy:  0.0
------------iteration 200----------
total loss 196.40461892049302
main criterion 40.21176308553209
weighted_aux_loss 156.19285583496094
loss_r_bn_feature 156.19285583496094
Verifier accuracy:  0.0
------------iteration 300----------
total loss 156.75249422791964
main criterion 44.53588808778293
weighted_aux_loss 112.21660614013672
loss_r_bn_feature 112.21660614013672
Verifier accuracy:  0.0
------------iteration 400----------
total loss 152.5126332052204
main criterion 51.95447433070868
weighted_aux_loss 100.55815887451172
loss_r_bn_feature 100.55815887451172
Verifier accuracy:  0.0
------------iteration 500----------
total loss 334.6824545163309
main criterion 94.72795622531525
weighted_aux_loss 239.95449829101562
loss_r_bn_feature 239.95449829101562
Verifier accuracy:  0.0
------------iteration 600----------
total loss 111.43016370968647
main criterion 36.39574751095601
weighted_aux_loss 75.03441619873047
loss_r_bn_feature 75.03441619873047
Verifier accuracy:  0.0
------------iteration 700----------
total loss 98.64741562436117
main criterion 35.90635346005454
weighted_aux_loss 62.74106216430664
loss_r_bn_feature 62.74106216430664
Verifier accuracy:  0.0
------------iteration 800----------
total loss 100.97352330105718
main criterion 40.7520534219068
weighted_aux_loss 60.22146987915039
loss_r_bn_feature 60.22146987915039
Verifier accuracy:  0.0
------------iteration 900----------
total loss 91.50129952722895
main criterion 34.07665124231684
weighted_aux_loss 57.42464828491211
loss_r_bn_feature 57.42464828491211
Verifier accuracy:  0.0
------------iteration 1000----------
total loss 122.08013477081315
main criterion 51.40459003204361
weighted_aux_loss 70.67554473876953
loss_r_bn_feature 70.67554473876953
Verifier accuracy:  0.0
------------iteration 1100----------
total loss 115.7551307611684
main criterion 50.57145308782855
weighted_aux_loss 65.18367767333984
loss_r_bn_feature 65.18367767333984
Verifier accuracy:  0.0
------------iteration 1200----------
total loss 83.2783489123145
main criterion 40.3707904711524
weighted_aux_loss 42.90755844116211
loss_r_bn_feature 42.90755844116211
Verifier accuracy:  0.0
------------iteration 1300----------
total loss 102.29345255737951
main criterion 46.767222698981065
weighted_aux_loss 55.52622985839844
loss_r_bn_feature 55.52622985839844
Verifier accuracy:  0.0
------------iteration 1400----------
total loss 125.1393951083432
main criterion 54.007559170843194
weighted_aux_loss 71.1318359375
loss_r_bn_feature 71.1318359375
Verifier accuracy:  0.0
------------iteration 1500----------
total loss 76.7747639373005
main criterion 33.91867339164621
weighted_aux_loss 42.8560905456543
loss_r_bn_feature 42.8560905456543
Verifier accuracy:  0.0
------------iteration 1600----------
total loss 76.60637861724182
main criterion 38.05915266509338
weighted_aux_loss 38.54722595214844
loss_r_bn_feature 38.54722595214844
Verifier accuracy:  0.0
------------iteration 1700----------
total loss 61.96704433506755
main criterion 29.226123314559736
weighted_aux_loss 32.74092102050781
loss_r_bn_feature 32.74092102050781
Verifier accuracy:  0.0
------------iteration 1800----------
total loss 57.38841072804486
main criterion 24.152765433855404
weighted_aux_loss 33.23564529418945
loss_r_bn_feature 33.23564529418945
Verifier accuracy:  0.0
------------iteration 1900----------
total loss 79.53475076154703
main criterion 40.40120583967203
weighted_aux_loss 39.133544921875
loss_r_bn_feature 39.133544921875
Verifier accuracy:  0.0
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 573.7804315179709
main criterion 136.75494934023655
weighted_aux_loss 437.0254821777344
loss_r_bn_feature 437.0254821777344
Verifier accuracy:  0.0
------------iteration 100----------
total loss 248.40934478715974
main criterion 43.70834075883942
weighted_aux_loss 204.7010040283203
loss_r_bn_feature 204.7010040283203
Verifier accuracy:  0.0
------------iteration 200----------
total loss 166.00121503336504
main criterion 44.25311475260331
weighted_aux_loss 121.74810028076172
loss_r_bn_feature 121.74810028076172
Verifier accuracy:  0.0
------------iteration 300----------
total loss 194.2090008791193
main criterion 59.83244448263491
weighted_aux_loss 134.37655639648438
loss_r_bn_feature 134.37655639648438
Verifier accuracy:  0.0
------------iteration 400----------
total loss 151.3880384643081
main criterion 60.2698972899917
weighted_aux_loss 91.1181411743164
loss_r_bn_feature 91.1181411743164
Verifier accuracy:  0.0
------------iteration 500----------
total loss 158.06991695141437
main criterion 58.21063350414876
weighted_aux_loss 99.85928344726562
loss_r_bn_feature 99.85928344726562
Verifier accuracy:  0.0
------------iteration 600----------
total loss 167.06422158960814
main criterion 65.74769326929564
weighted_aux_loss 101.3165283203125
loss_r_bn_feature 101.3165283203125
Verifier accuracy:  0.0
------------iteration 700----------
total loss 119.70329173512167
main criterion 46.66707499928183
weighted_aux_loss 73.03621673583984
loss_r_bn_feature 73.03621673583984
Verifier accuracy:  0.0
------------iteration 800----------
total loss 105.5710055041582
main criterion 45.39619700196093
weighted_aux_loss 60.174808502197266
loss_r_bn_feature 60.174808502197266
Verifier accuracy:  0.0
------------iteration 900----------
total loss 117.24022945016054
main criterion 49.146250568324604
weighted_aux_loss 68.09397888183594
loss_r_bn_feature 68.09397888183594
Verifier accuracy:  0.0
------------iteration 1000----------
total loss 111.0862334109396
main criterion 48.939127240285295
weighted_aux_loss 62.1471061706543
loss_r_bn_feature 62.1471061706543
Verifier accuracy:  0.0
------------iteration 1100----------
total loss 80.05178585208645
main criterion 31.39305248416654
weighted_aux_loss 48.65873336791992
loss_r_bn_feature 48.65873336791992
Verifier accuracy:  0.0
------------iteration 1200----------
total loss 81.1529959786288
main criterion 30.690620337759654
weighted_aux_loss 50.46237564086914
loss_r_bn_feature 50.46237564086914
Verifier accuracy:  0.0
------------iteration 1300----------
total loss 81.3492582651601
main criterion 38.343826136253846
weighted_aux_loss 43.00543212890625
loss_r_bn_feature 43.00543212890625
Verifier accuracy:  0.0
------------iteration 1400----------
total loss 82.6132425950065
main criterion 35.26330134134439
weighted_aux_loss 47.34994125366211
loss_r_bn_feature 47.34994125366211
Verifier accuracy:  0.0
------------iteration 1500----------
total loss 68.87170917490018
main criterion 30.592976875095488
weighted_aux_loss 38.27873229980469
loss_r_bn_feature 38.27873229980469
Verifier accuracy:  0.0
------------iteration 1600----------
total loss 61.02265068020538
main criterion 26.958636245390927
weighted_aux_loss 34.06401443481445
loss_r_bn_feature 34.06401443481445
Verifier accuracy:  0.0
------------iteration 1700----------
total loss 231.4659180454039
main criterion 74.08255622899763
weighted_aux_loss 157.38336181640625
loss_r_bn_feature 157.38336181640625
Verifier accuracy:  0.0
------------iteration 1800----------
total loss 69.55194274983968
main criterion 31.838418885093592
weighted_aux_loss 37.713523864746094
loss_r_bn_feature 37.713523864746094
Verifier accuracy:  0.0
------------iteration 1900----------
total loss 61.411449954836954
main criterion 25.940086887210004
weighted_aux_loss 35.47136306762695
loss_r_bn_feature 35.47136306762695
Verifier accuracy:  0.0
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 581.8954561682286
main criterion 127.97394737916616
weighted_aux_loss 453.9215087890625
loss_r_bn_feature 453.9215087890625
Verifier accuracy:  0.0
------------iteration 100----------
total loss 348.4050684437171
main criterion 89.47657112926396
weighted_aux_loss 258.9284973144531
loss_r_bn_feature 258.9284973144531
Verifier accuracy:  0.0
------------iteration 200----------
total loss 190.8506312456053
main criterion 41.86286879443345
weighted_aux_loss 148.98776245117188
loss_r_bn_feature 148.98776245117188
Verifier accuracy:  0.0
------------iteration 300----------
total loss 165.50036276969783
main criterion 39.73813285026424
weighted_aux_loss 125.7622299194336
loss_r_bn_feature 125.7622299194336
Verifier accuracy:  0.0
------------iteration 400----------
total loss 144.57432627383886
main criterion 37.432724711338864
weighted_aux_loss 107.1416015625
loss_r_bn_feature 107.1416015625
Verifier accuracy:  0.0
------------iteration 500----------
total loss 112.52298031429771
main criterion 34.90908108334068
weighted_aux_loss 77.61389923095703
loss_r_bn_feature 77.61389923095703
Verifier accuracy:  0.0
------------iteration 600----------
total loss 120.85909477443414
main criterion 43.90873924464899
weighted_aux_loss 76.95035552978516
loss_r_bn_feature 76.95035552978516
Verifier accuracy:  0.0
------------iteration 700----------
total loss 106.32998795444061
main criterion 33.47110123569062
weighted_aux_loss 72.85888671875
loss_r_bn_feature 72.85888671875
Verifier accuracy:  0.0
------------iteration 800----------
total loss 105.97599199260642
main criterion 38.91964891399314
weighted_aux_loss 67.05634307861328
loss_r_bn_feature 67.05634307861328
Verifier accuracy:  0.0
------------iteration 900----------
total loss 86.88353578848282
main criterion 33.01957933706681
weighted_aux_loss 53.863956451416016
loss_r_bn_feature 53.863956451416016
Verifier accuracy:  0.0
------------iteration 1000----------
total loss 99.89354684438038
main criterion 45.675864959614756
weighted_aux_loss 54.217681884765625
loss_r_bn_feature 54.217681884765625
Verifier accuracy:  0.0
------------iteration 1100----------
total loss 81.0689084542396
main criterion 33.295261145889995
weighted_aux_loss 47.77364730834961
loss_r_bn_feature 47.77364730834961
Verifier accuracy:  0.0
------------iteration 1200----------
total loss 81.20735718944609
main criterion 31.359750530510542
weighted_aux_loss 49.84760665893555
loss_r_bn_feature 49.84760665893555
Verifier accuracy:  0.0
------------iteration 1300----------
total loss 67.04813243711652
main criterion 27.436739462263
weighted_aux_loss 39.611392974853516
loss_r_bn_feature 39.611392974853516
Verifier accuracy:  0.0
------------iteration 1400----------
total loss 138.84391928681316
main criterion 55.555215368356144
weighted_aux_loss 83.28870391845703
loss_r_bn_feature 83.28870391845703
Verifier accuracy:  0.0
------------iteration 1500----------
total loss 74.68232616276475
main criterion 31.018675650069437
weighted_aux_loss 43.66365051269531
loss_r_bn_feature 43.66365051269531
Verifier accuracy:  0.0
------------iteration 1600----------
total loss 147.43871067871873
main criterion 58.69656895508594
weighted_aux_loss 88.74214172363281
loss_r_bn_feature 88.74214172363281
Verifier accuracy:  0.0
------------iteration 1700----------
total loss 66.38755031000782
main criterion 28.76340098749806
weighted_aux_loss 37.624149322509766
loss_r_bn_feature 37.624149322509766
Verifier accuracy:  0.0
------------iteration 1800----------
total loss 89.436073019253
main criterion 43.01007051681159
weighted_aux_loss 46.426002502441406
loss_r_bn_feature 46.426002502441406
Verifier accuracy:  0.0
------------iteration 1900----------
total loss 117.8625219873025
main criterion 52.23559175048609
weighted_aux_loss 65.6269302368164
loss_r_bn_feature 65.6269302368164
Verifier accuracy:  0.0
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 584.4246505619608
main criterion 135.16110075727332
weighted_aux_loss 449.2635498046875
loss_r_bn_feature 449.2635498046875
Verifier accuracy:  0.0
------------iteration 100----------
total loss 472.4994792190854
main criterion 102.35903732455415
weighted_aux_loss 370.14044189453125
loss_r_bn_feature 370.14044189453125
Verifier accuracy:  0.0
------------iteration 200----------
total loss 198.81891011091864
main criterion 40.90631245466862
weighted_aux_loss 157.91259765625
loss_r_bn_feature 157.91259765625
Verifier accuracy:  0.0
------------iteration 300----------
total loss 233.83795233748918
main criterion 78.52635260604389
weighted_aux_loss 155.3115997314453
loss_r_bn_feature 155.3115997314453
Verifier accuracy:  0.0
------------iteration 400----------
total loss 134.93368602578474
main criterion 40.10528636758161
weighted_aux_loss 94.82839965820312
loss_r_bn_feature 94.82839965820312
Verifier accuracy:  0.0
------------iteration 500----------
total loss 124.26321623382395
main criterion 38.515291429136454
weighted_aux_loss 85.7479248046875
loss_r_bn_feature 85.7479248046875
Verifier accuracy:  0.0
------------iteration 600----------
total loss 136.11911420127723
main criterion 51.80966595908972
weighted_aux_loss 84.3094482421875
loss_r_bn_feature 84.3094482421875
Verifier accuracy:  0.0
------------iteration 700----------
total loss 178.19040623997347
main criterion 75.53854314182894
weighted_aux_loss 102.65186309814453
loss_r_bn_feature 102.65186309814453
Verifier accuracy:  0.0
------------iteration 800----------
total loss 123.78717532518259
main criterion 49.363507417467744
weighted_aux_loss 74.42366790771484
loss_r_bn_feature 74.42366790771484
Verifier accuracy:  0.0
------------iteration 900----------
total loss 94.48586275681646
main criterion 39.411220575419975
weighted_aux_loss 55.074642181396484
loss_r_bn_feature 55.074642181396484
Verifier accuracy:  0.0
------------iteration 1000----------
total loss 91.69005097185607
main criterion 38.93076599871154
weighted_aux_loss 52.75928497314453
loss_r_bn_feature 52.75928497314453
Verifier accuracy:  0.0
------------iteration 1100----------
total loss 159.78119545295465
main criterion 64.83953743293513
weighted_aux_loss 94.94165802001953
loss_r_bn_feature 94.94165802001953
Verifier accuracy:  0.0
------------iteration 1200----------
total loss 67.94153365391654
main criterion 27.63053282994192
weighted_aux_loss 40.31100082397461
loss_r_bn_feature 40.31100082397461
Verifier accuracy:  0.0
------------iteration 1300----------
total loss 75.21072818468244
main criterion 30.036667363149242
weighted_aux_loss 45.1740608215332
loss_r_bn_feature 45.1740608215332
Verifier accuracy:  0.0
------------iteration 1400----------
total loss 83.3207898609354
main criterion 39.42578940317173
weighted_aux_loss 43.89500045776367
loss_r_bn_feature 43.89500045776367
Verifier accuracy:  0.0
------------iteration 1500----------
total loss 87.27886178466835
main criterion 42.2592351672367
weighted_aux_loss 45.01962661743164
loss_r_bn_feature 45.01962661743164
Verifier accuracy:  0.0
------------iteration 1600----------
total loss 81.59686510770868
main criterion 38.79362337797235
weighted_aux_loss 42.80324172973633
loss_r_bn_feature 42.80324172973633
Verifier accuracy:  0.0
------------iteration 1700----------
total loss 74.67223530061555
main criterion 34.3241746927054
weighted_aux_loss 40.348060607910156
loss_r_bn_feature 40.348060607910156
Verifier accuracy:  0.0
------------iteration 1800----------
total loss 152.1991279210485
main criterion 59.6423881139196
weighted_aux_loss 92.5567398071289
loss_r_bn_feature 92.5567398071289
Verifier accuracy:  0.0
------------iteration 1900----------
total loss 59.278949399743695
main criterion 26.605586667810105
weighted_aux_loss 32.673362731933594
loss_r_bn_feature 32.673362731933594
Verifier accuracy:  0.0
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 584.4269259894588
main criterion 134.14656099922445
weighted_aux_loss 450.2803649902344
loss_r_bn_feature 450.2803649902344
Verifier accuracy:  0.0
------------iteration 100----------
total loss 246.90846634536678
main criterion 44.819126135405824
weighted_aux_loss 202.08934020996094
loss_r_bn_feature 202.08934020996094
Verifier accuracy:  0.0
------------iteration 200----------
total loss 200.52295614949458
main criterion 41.02159811726801
weighted_aux_loss 159.50135803222656
loss_r_bn_feature 159.50135803222656
Verifier accuracy:  0.0
------------iteration 300----------
total loss 146.22453221422126
main criterion 42.702803698596256
weighted_aux_loss 103.521728515625
loss_r_bn_feature 103.521728515625
Verifier accuracy:  0.0
------------iteration 400----------
total loss 128.4958690572629
main criterion 47.29593314417696
weighted_aux_loss 81.19993591308594
loss_r_bn_feature 81.19993591308594
Verifier accuracy:  0.0
------------iteration 500----------
total loss 111.71865337995274
main criterion 36.17039745954258
weighted_aux_loss 75.54825592041016
loss_r_bn_feature 75.54825592041016
Verifier accuracy:  0.0
------------iteration 600----------
total loss 120.43656676680439
main criterion 47.49371093184345
weighted_aux_loss 72.94285583496094
loss_r_bn_feature 72.94285583496094
Verifier accuracy:  0.0
------------iteration 700----------
total loss 126.5772062761977
main criterion 51.96100296809224
weighted_aux_loss 74.61620330810547
loss_r_bn_feature 74.61620330810547
Verifier accuracy:  0.0
------------iteration 800----------
total loss 90.88522311108312
main criterion 36.560870846678824
weighted_aux_loss 54.3243522644043
loss_r_bn_feature 54.3243522644043
Verifier accuracy:  0.0
------------iteration 900----------
total loss 90.22778971095374
main criterion 33.8612621535807
weighted_aux_loss 56.36652755737305
loss_r_bn_feature 56.36652755737305
Verifier accuracy:  0.0
------------iteration 1000----------
total loss 93.03627500544569
main criterion 34.27251157771131
weighted_aux_loss 58.763763427734375
loss_r_bn_feature 58.763763427734375
Verifier accuracy:  0.0
------------iteration 1100----------
total loss 86.03244271025228
main criterion 36.544317862840174
weighted_aux_loss 49.48812484741211
loss_r_bn_feature 49.48812484741211
Verifier accuracy:  0.0
------------iteration 1200----------
total loss 98.17734480294791
main criterion 44.02923536691276
weighted_aux_loss 54.148109436035156
loss_r_bn_feature 54.148109436035156
Verifier accuracy:  0.0
------------iteration 1300----------
total loss 82.39015508041751
main criterion 37.63503194198979
weighted_aux_loss 44.755123138427734
loss_r_bn_feature 44.755123138427734
Verifier accuracy:  0.0
------------iteration 1400----------
total loss 68.16635249192124
main criterion 28.4848606401146
weighted_aux_loss 39.68149185180664
loss_r_bn_feature 39.68149185180664
Verifier accuracy:  0.0
------------iteration 1500----------
total loss 70.85829714155821
main criterion 30.60267046309142
weighted_aux_loss 40.2556266784668
loss_r_bn_feature 40.2556266784668
Verifier accuracy:  0.0
------------iteration 1600----------
total loss 73.92655579687965
main criterion 36.3765451157273
weighted_aux_loss 37.550010681152344
loss_r_bn_feature 37.550010681152344
Verifier accuracy:  0.0
------------iteration 1700----------
total loss 62.961978836513126
main criterion 27.094539566493598
weighted_aux_loss 35.86743927001953
loss_r_bn_feature 35.86743927001953
Verifier accuracy:  0.0
------------iteration 1800----------
total loss 68.73315584340959
main criterion 30.940484639796306
weighted_aux_loss 37.79267120361328
loss_r_bn_feature 37.79267120361328
Verifier accuracy:  0.0
------------iteration 1900----------
total loss 54.81367602964202
main criterion 24.290976673562916
weighted_aux_loss 30.5226993560791
loss_r_bn_feature 30.5226993560791
Verifier accuracy:  0.0
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 594.3115075232301
main criterion 129.49931269901145
weighted_aux_loss 464.81219482421875
loss_r_bn_feature 464.81219482421875
Verifier accuracy:  0.0
------------iteration 100----------
total loss 379.2770324808372
main criterion 105.2932983499778
weighted_aux_loss 273.9837341308594
loss_r_bn_feature 273.9837341308594
Verifier accuracy:  0.0
------------iteration 200----------
total loss 186.2265687197876
main criterion 39.15630199615479
weighted_aux_loss 147.0702667236328
loss_r_bn_feature 147.0702667236328
Verifier accuracy:  0.0
------------iteration 300----------
total loss 132.40742012966544
main criterion 43.90850350368887
weighted_aux_loss 88.49891662597656
loss_r_bn_feature 88.49891662597656
Verifier accuracy:  0.0
------------iteration 400----------
total loss 276.37471076228735
main criterion 91.99481269099826
weighted_aux_loss 184.37989807128906
loss_r_bn_feature 184.37989807128906
Verifier accuracy:  0.0
------------iteration 500----------
total loss 110.76268037973742
main criterion 38.570747701514755
weighted_aux_loss 72.19193267822266
loss_r_bn_feature 72.19193267822266
Verifier accuracy:  0.0
------------iteration 600----------
total loss 115.15400998459988
main criterion 42.80660550461941
weighted_aux_loss 72.34740447998047
loss_r_bn_feature 72.34740447998047
Verifier accuracy:  0.0
------------iteration 700----------
total loss 97.5542398300822
main criterion 37.700056633060704
weighted_aux_loss 59.854183197021484
loss_r_bn_feature 59.854183197021484
Verifier accuracy:  0.0
------------iteration 800----------
total loss 163.1140823609772
main criterion 68.78233339857485
weighted_aux_loss 94.33174896240234
loss_r_bn_feature 94.33174896240234
Verifier accuracy:  0.0
------------iteration 900----------
total loss 91.30245474861243
main criterion 38.27667502449134
weighted_aux_loss 53.025779724121094
loss_r_bn_feature 53.025779724121094
Verifier accuracy:  0.0
------------iteration 1000----------
total loss 90.21549658487905
main criterion 40.108933016763814
weighted_aux_loss 50.106563568115234
loss_r_bn_feature 50.106563568115234
Verifier accuracy:  0.0
------------iteration 1100----------
total loss 109.61551769962347
main criterion 49.27747448673284
weighted_aux_loss 60.338043212890625
loss_r_bn_feature 60.338043212890625
Verifier accuracy:  0.0
------------iteration 1200----------
total loss 68.59394961036783
main criterion 27.728791529313146
weighted_aux_loss 40.86515808105469
loss_r_bn_feature 40.86515808105469
Verifier accuracy:  0.0
------------iteration 1300----------
total loss 67.94135066746391
main criterion 28.242917745100627
weighted_aux_loss 39.69843292236328
loss_r_bn_feature 39.69843292236328
Verifier accuracy:  0.0
------------iteration 1400----------
total loss 67.90729488830998
main criterion 28.191661495975996
weighted_aux_loss 39.715633392333984
loss_r_bn_feature 39.715633392333984
Verifier accuracy:  0.0
------------iteration 1500----------
total loss 80.67184018391832
main criterion 38.60423230427965
weighted_aux_loss 42.06760787963867
loss_r_bn_feature 42.06760787963867
Verifier accuracy:  0.0
------------iteration 1600----------
total loss 64.4449306715523
main criterion 26.650405525067928
weighted_aux_loss 37.794525146484375
loss_r_bn_feature 37.794525146484375
Verifier accuracy:  0.0
------------iteration 1700----------
total loss 62.85378388894242
main criterion 26.670716567165076
weighted_aux_loss 36.183067321777344
loss_r_bn_feature 36.183067321777344
Verifier accuracy:  0.0
------------iteration 1800----------
total loss 95.57314900704031
main criterion 39.69394139595632
weighted_aux_loss 55.879207611083984
loss_r_bn_feature 55.879207611083984
Verifier accuracy:  0.0
------------iteration 1900----------
total loss 76.85013692637517
main criterion 36.29640310069158
weighted_aux_loss 40.553733825683594
loss_r_bn_feature 40.553733825683594
Verifier accuracy:  0.0
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 570.9987552455977
main criterion 136.83472326317582
weighted_aux_loss 434.1640319824219
loss_r_bn_feature 434.1640319824219
Verifier accuracy:  0.0
------------iteration 100----------
total loss 252.55979644081606
main criterion 59.92336760780825
weighted_aux_loss 192.6364288330078
loss_r_bn_feature 192.6364288330078
Verifier accuracy:  0.0
------------iteration 200----------
total loss 191.4465726676829
main criterion 39.33168924483133
weighted_aux_loss 152.11488342285156
loss_r_bn_feature 152.11488342285156
Verifier accuracy:  0.0
------------iteration 300----------
total loss 163.04651671614963
main criterion 53.32856780257541
weighted_aux_loss 109.71794891357422
loss_r_bn_feature 109.71794891357422
Verifier accuracy:  0.0
------------iteration 400----------
total loss 124.7862739708103
main criterion 39.17860032579076
weighted_aux_loss 85.60767364501953
loss_r_bn_feature 85.60767364501953
Verifier accuracy:  0.0
------------iteration 500----------
total loss 131.19347901177946
main criterion 41.46166748834196
weighted_aux_loss 89.7318115234375
loss_r_bn_feature 89.7318115234375
Verifier accuracy:  0.0
------------iteration 600----------
total loss 106.58910920681643
main criterion 42.27675416531253
weighted_aux_loss 64.3123550415039
loss_r_bn_feature 64.3123550415039
Verifier accuracy:  0.0
------------iteration 700----------
total loss 138.25435968684587
main criterion 60.90854212092792
weighted_aux_loss 77.34581756591797
loss_r_bn_feature 77.34581756591797
Verifier accuracy:  0.0
------------iteration 800----------
total loss 90.01581812974896
main criterion 33.884726058215755
weighted_aux_loss 56.1310920715332
loss_r_bn_feature 56.1310920715332
Verifier accuracy:  0.0
------------iteration 900----------
total loss 180.51031405137422
main criterion 65.27833468125702
weighted_aux_loss 115.23197937011719
loss_r_bn_feature 115.23197937011719
Verifier accuracy:  0.0
------------iteration 1000----------
total loss 135.11147507797168
main criterion 50.912759868010745
weighted_aux_loss 84.19871520996094
loss_r_bn_feature 84.19871520996094
Verifier accuracy:  0.0
------------iteration 1100----------
total loss 85.22583782753975
main criterion 32.723991514063194
weighted_aux_loss 52.50184631347656
loss_r_bn_feature 52.50184631347656
Verifier accuracy:  0.0
------------iteration 1200----------
total loss 76.48651267104447
main criterion 36.5142073731929
weighted_aux_loss 39.97230529785156
loss_r_bn_feature 39.97230529785156
Verifier accuracy:  0.0
------------iteration 1300----------
total loss 96.25741907378563
main criterion 44.59514948150047
weighted_aux_loss 51.662269592285156
loss_r_bn_feature 51.662269592285156
Verifier accuracy:  0.0
------------iteration 1400----------
total loss 98.75276851723581
main criterion 45.785368920067846
weighted_aux_loss 52.96739959716797
loss_r_bn_feature 52.96739959716797
Verifier accuracy:  0.0
------------iteration 1500----------
total loss 63.459010222344055
main criterion 28.66427526751007
weighted_aux_loss 34.794734954833984
loss_r_bn_feature 34.794734954833984
Verifier accuracy:  0.0
------------iteration 1600----------
total loss 62.142477923909254
main criterion 28.86247914461238
weighted_aux_loss 33.279998779296875
loss_r_bn_feature 33.279998779296875
Verifier accuracy:  0.0
------------iteration 1700----------
total loss 56.81164889783222
main criterion 25.110057406133002
weighted_aux_loss 31.70159149169922
loss_r_bn_feature 31.70159149169922
Verifier accuracy:  0.0
------------iteration 1800----------
total loss 59.29797204959698
main criterion 26.228823032262994
weighted_aux_loss 33.069149017333984
loss_r_bn_feature 33.069149017333984
Verifier accuracy:  0.0
------------iteration 1900----------
total loss 76.97024591375938
main criterion 33.580799655214456
weighted_aux_loss 43.38944625854492
loss_r_bn_feature 43.38944625854492
Verifier accuracy:  0.0
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 592.9879409238085
main criterion 136.17989649021484
weighted_aux_loss 456.80804443359375
loss_r_bn_feature 456.80804443359375
Verifier accuracy:  0.0
------------iteration 100----------
total loss 238.57188513640412
main criterion 44.727860478201
weighted_aux_loss 193.84402465820312
loss_r_bn_feature 193.84402465820312
Verifier accuracy:  0.0
------------iteration 200----------
total loss 182.79770493623337
main criterion 45.4693663131865
weighted_aux_loss 137.32833862304688
loss_r_bn_feature 137.32833862304688
Verifier accuracy:  0.0
------------iteration 300----------
total loss 172.4244018448075
main criterion 59.56376036531533
weighted_aux_loss 112.86064147949219
loss_r_bn_feature 112.86064147949219
Verifier accuracy:  0.0
------------iteration 400----------
total loss 109.75464128247808
main criterion 37.14006303540776
weighted_aux_loss 72.61457824707031
loss_r_bn_feature 72.61457824707031
Verifier accuracy:  0.0
------------iteration 500----------
total loss 125.97553455983925
main criterion 38.54159748708535
weighted_aux_loss 87.4339370727539
loss_r_bn_feature 87.4339370727539
Verifier accuracy:  0.0
------------iteration 600----------
total loss 123.46012953886982
main criterion 44.82370833525654
weighted_aux_loss 78.63642120361328
loss_r_bn_feature 78.63642120361328
Verifier accuracy:  0.0
------------iteration 700----------
total loss 100.64805733501376
main criterion 36.97942482768954
weighted_aux_loss 63.66863250732422
loss_r_bn_feature 63.66863250732422
Verifier accuracy:  0.0
------------iteration 800----------
total loss 230.19531470708642
main criterion 82.44882422857079
weighted_aux_loss 147.74649047851562
loss_r_bn_feature 147.74649047851562
Verifier accuracy:  0.0
------------iteration 900----------
total loss 110.23871907128391
main criterion 44.23934468163547
weighted_aux_loss 65.99937438964844
loss_r_bn_feature 65.99937438964844
Verifier accuracy:  0.0
------------iteration 1000----------
total loss 91.5107359525137
main criterion 39.17443987097073
weighted_aux_loss 52.33629608154297
loss_r_bn_feature 52.33629608154297
Verifier accuracy:  0.0
------------iteration 1100----------
total loss 88.00326521408633
main criterion 32.9403913737543
weighted_aux_loss 55.06287384033203
loss_r_bn_feature 55.06287384033203
Verifier accuracy:  0.0
------------iteration 1200----------
total loss 81.4932921376691
main criterion 32.06079320578434
weighted_aux_loss 49.432498931884766
loss_r_bn_feature 49.432498931884766
Verifier accuracy:  0.0
------------iteration 1300----------
total loss 234.10624203896072
main criterion 83.16300473427322
weighted_aux_loss 150.9432373046875
loss_r_bn_feature 150.9432373046875
Verifier accuracy:  0.0
------------iteration 1400----------
total loss 67.09300148950193
main criterion 28.124610071044902
weighted_aux_loss 38.96839141845703
loss_r_bn_feature 38.96839141845703
Verifier accuracy:  0.0
------------iteration 1500----------
total loss 77.78941912373699
main criterion 32.981294581500656
weighted_aux_loss 44.80812454223633
loss_r_bn_feature 44.80812454223633
Verifier accuracy:  0.0
------------iteration 1600----------
total loss 87.77050928311814
main criterion 39.330117742590794
weighted_aux_loss 48.440391540527344
loss_r_bn_feature 48.440391540527344
Verifier accuracy:  0.0
------------iteration 1700----------
total loss 56.384715548495635
main criterion 26.755963793734896
weighted_aux_loss 29.628751754760742
loss_r_bn_feature 29.628751754760742
Verifier accuracy:  0.0
------------iteration 1800----------
total loss 66.74217557350244
main criterion 32.81162213722313
weighted_aux_loss 33.9305534362793
loss_r_bn_feature 33.9305534362793
Verifier accuracy:  0.0
------------iteration 1900----------
total loss 57.702872057570374
main criterion 27.954342623366276
weighted_aux_loss 29.7485294342041
loss_r_bn_feature 29.7485294342041
Verifier accuracy:  0.0
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 674.9462183706469
main criterion 137.38182628080307
weighted_aux_loss 537.5643920898438
loss_r_bn_feature 537.5643920898438
Verifier accuracy:  0.0
------------iteration 100----------
total loss 221.26066220466262
main criterion 47.122661716381366
weighted_aux_loss 174.13800048828125
loss_r_bn_feature 174.13800048828125
Verifier accuracy:  0.0
------------iteration 200----------
total loss 211.22396560086815
main criterion 68.33268447293845
weighted_aux_loss 142.8912811279297
loss_r_bn_feature 142.8912811279297
Verifier accuracy:  0.0
------------iteration 300----------
total loss 171.70109544192377
main criterion 50.661475996122974
weighted_aux_loss 121.03961944580078
loss_r_bn_feature 121.03961944580078
Verifier accuracy:  0.0
------------iteration 400----------
total loss 126.5403979920155
main criterion 39.750336041331906
weighted_aux_loss 86.7900619506836
loss_r_bn_feature 86.7900619506836
Verifier accuracy:  0.0
------------iteration 500----------
total loss 178.94712453172025
main criterion 63.87871938035307
weighted_aux_loss 115.06840515136719
loss_r_bn_feature 115.06840515136719
Verifier accuracy:  0.0
------------iteration 600----------
total loss 156.8101818256672
main criterion 59.333588808089075
weighted_aux_loss 97.47659301757812
loss_r_bn_feature 97.47659301757812
Verifier accuracy:  0.0
------------iteration 700----------
total loss 135.87729844843585
main criterion 51.644258592478806
weighted_aux_loss 84.23303985595703
loss_r_bn_feature 84.23303985595703
Verifier accuracy:  0.0
------------iteration 800----------
total loss 94.69213113899164
main criterion 34.12841043586665
weighted_aux_loss 60.563720703125
loss_r_bn_feature 60.563720703125
Verifier accuracy:  0.0
------------iteration 900----------
total loss 97.6559419769564
main criterion 34.34537602847006
weighted_aux_loss 63.31056594848633
loss_r_bn_feature 63.31056594848633
Verifier accuracy:  0.0
------------iteration 1000----------
total loss 93.38371104377764
main criterion 42.3139135279085
weighted_aux_loss 51.06979751586914
loss_r_bn_feature 51.06979751586914
Verifier accuracy:  0.0
------------iteration 1100----------
total loss 78.62577221532449
main criterion 30.917016722160426
weighted_aux_loss 47.70875549316406
loss_r_bn_feature 47.70875549316406
Verifier accuracy:  0.0
------------iteration 1200----------
total loss 82.94712130563823
main criterion 33.93698565500346
weighted_aux_loss 49.010135650634766
loss_r_bn_feature 49.010135650634766
Verifier accuracy:  0.0
------------iteration 1300----------
total loss 64.49978723329943
main criterion 27.729386232322867
weighted_aux_loss 36.77040100097656
loss_r_bn_feature 36.77040100097656
Verifier accuracy:  0.0
------------iteration 1400----------
total loss 81.39305085999484
main criterion 37.634505938119844
weighted_aux_loss 43.758544921875
loss_r_bn_feature 43.758544921875
Verifier accuracy:  0.0
------------iteration 1500----------
total loss 63.43020876653921
main criterion 29.322268092711088
weighted_aux_loss 34.107940673828125
loss_r_bn_feature 34.107940673828125
Verifier accuracy:  0.0
------------iteration 1600----------
total loss 61.43140379382983
main criterion 28.92269483997241
weighted_aux_loss 32.50870895385742
loss_r_bn_feature 32.50870895385742
Verifier accuracy:  0.0
------------iteration 1700----------
total loss 56.650148534533926
main criterion 26.021635198352282
weighted_aux_loss 30.62851333618164
loss_r_bn_feature 30.62851333618164
Verifier accuracy:  0.0
------------iteration 1800----------
total loss 55.283999468827275
main criterion 23.95438673689368
weighted_aux_loss 31.329612731933594
loss_r_bn_feature 31.329612731933594
Verifier accuracy:  0.0
------------iteration 1900----------
total loss 98.1838505236357
main criterion 45.432328459426714
weighted_aux_loss 52.751522064208984
loss_r_bn_feature 52.751522064208984
Verifier accuracy:  0.0
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 560.5048328923882
main criterion 145.74018445488824
weighted_aux_loss 414.7646484375
loss_r_bn_feature 414.7646484375
Verifier accuracy:  0.0
------------iteration 100----------
total loss 214.5717989593403
main criterion 41.61636988219187
weighted_aux_loss 172.95542907714844
loss_r_bn_feature 172.95542907714844
Verifier accuracy:  0.0
------------iteration 200----------
total loss 228.52035224255278
main criterion 66.58366095837309
weighted_aux_loss 161.9366912841797
loss_r_bn_feature 161.9366912841797
Verifier accuracy:  0.0
------------iteration 300----------
total loss 166.68778482759035
main criterion 65.19218698823487
weighted_aux_loss 101.49559783935547
loss_r_bn_feature 101.49559783935547
Verifier accuracy:  0.0
------------iteration 400----------
total loss 111.48238167493722
main criterion 42.348409506968466
weighted_aux_loss 69.13397216796875
loss_r_bn_feature 69.13397216796875
Verifier accuracy:  0.0
------------iteration 500----------
total loss 106.27585115913192
main criterion 34.714739708936605
weighted_aux_loss 71.56111145019531
loss_r_bn_feature 71.56111145019531
Verifier accuracy:  0.0
------------iteration 600----------
total loss 100.4298486663544
main criterion 33.47287845151065
weighted_aux_loss 66.95697021484375
loss_r_bn_feature 66.95697021484375
Verifier accuracy:  0.0
------------iteration 700----------
total loss 105.66279730735192
main criterion 38.49633154807457
weighted_aux_loss 67.16646575927734
loss_r_bn_feature 67.16646575927734
Verifier accuracy:  0.0
------------iteration 800----------
total loss 95.03656739268882
main criterion 37.329551401477886
weighted_aux_loss 57.70701599121094
loss_r_bn_feature 57.70701599121094
Verifier accuracy:  0.0
------------iteration 900----------
total loss 77.97224278753094
main criterion 30.812879994562202
weighted_aux_loss 47.15936279296875
loss_r_bn_feature 47.15936279296875
Verifier accuracy:  0.0
------------iteration 1000----------
total loss 90.86481254386781
main criterion 39.95353477287172
weighted_aux_loss 50.911277770996094
loss_r_bn_feature 50.911277770996094
Verifier accuracy:  0.0
------------iteration 1100----------
total loss 78.19981315466279
main criterion 32.221617964233104
weighted_aux_loss 45.97819519042969
loss_r_bn_feature 45.97819519042969
Verifier accuracy:  0.0
------------iteration 1200----------
total loss 91.62380905777027
main criterion 40.71828003555348
weighted_aux_loss 50.9055290222168
loss_r_bn_feature 50.9055290222168
Verifier accuracy:  0.0
------------iteration 1300----------
total loss 71.49787645780648
main criterion 28.76453905546274
weighted_aux_loss 42.73333740234375
loss_r_bn_feature 42.73333740234375
Verifier accuracy:  0.0
------------iteration 1400----------
total loss 134.40084535354072
main criterion 56.038121049341505
weighted_aux_loss 78.36272430419922
loss_r_bn_feature 78.36272430419922
Verifier accuracy:  0.0
------------iteration 1500----------
total loss 69.72820311390015
main criterion 32.475208576546635
weighted_aux_loss 37.252994537353516
loss_r_bn_feature 37.252994537353516
Verifier accuracy:  0.0
------------iteration 1600----------
total loss 97.02324280320605
main criterion 44.700858922590804
weighted_aux_loss 52.322383880615234
loss_r_bn_feature 52.322383880615234
Verifier accuracy:  0.0
------------iteration 1700----------
total loss 82.96941601260866
main criterion 39.67936932071413
weighted_aux_loss 43.29004669189453
loss_r_bn_feature 43.29004669189453
Verifier accuracy:  0.0
------------iteration 1800----------
total loss 85.93891125464249
main criterion 40.07309323096085
weighted_aux_loss 45.86581802368164
loss_r_bn_feature 45.86581802368164
Verifier accuracy:  0.0
------------iteration 1900----------
total loss 61.41351586661948
main criterion 27.952517331463223
weighted_aux_loss 33.46099853515625
loss_r_bn_feature 33.46099853515625
Verifier accuracy:  0.0
=> using pytorch pre-trained model 'resnet18'
process data from ../recover/syn_data/tiny-imagenet/232
  0%|          | 0/300 [00:00<?, ?it/s]  0%|          | 1/300 [00:05<27:47,  5.58s/it]  1%|          | 2/300 [00:07<17:44,  3.57s/it]  1%|          | 3/300 [00:10<14:43,  2.98s/it]  1%|▏         | 4/300 [00:12<12:55,  2.62s/it]  2%|▏         | 5/300 [00:14<12:17,  2.50s/it]  2%|▏         | 6/300 [00:16<11:39,  2.38s/it]  2%|▏         | 7/300 [00:18<11:13,  2.30s/it]  3%|▎         | 8/300 [00:20<10:59,  2.26s/it]  3%|▎         | 9/300 [00:22<10:45,  2.22s/it]  3%|▎         | 10/300 [00:25<10:31,  2.18s/it]  4%|▎         | 11/300 [00:27<10:25,  2.16s/it]  4%|▍         | 12/300 [00:29<10:27,  2.18s/it]  4%|▍         | 13/300 [00:31<10:29,  2.19s/it]  5%|▍         | 14/300 [00:33<10:20,  2.17s/it]  5%|▌         | 15/300 [00:35<10:19,  2.17s/it]  5%|▌         | 16/300 [00:38<10:16,  2.17s/it]  6%|▌         | 17/300 [00:40<10:07,  2.15s/it]  6%|▌         | 18/300 [00:42<10:03,  2.14s/it]  6%|▋         | 19/300 [00:44<10:05,  2.15s/it]  7%|▋         | 20/300 [00:46<09:56,  2.13s/it]  7%|▋         | 21/300 [00:48<09:53,  2.13s/it]  7%|▋         | 22/300 [00:50<09:53,  2.14s/it]  8%|▊         | 23/300 [00:52<09:51,  2.13s/it]  8%|▊         | 24/300 [00:55<09:53,  2.15s/it]  8%|▊         | 25/300 [00:57<09:48,  2.14s/it]  9%|▊         | 26/300 [00:59<09:43,  2.13s/it]  9%|▉         | 27/300 [01:01<09:38,  2.12s/it]  9%|▉         | 28/300 [01:03<09:37,  2.12s/it] 10%|▉         | 29/300 [01:05<09:38,  2.13s/it] 10%|█         | 30/300 [01:07<09:33,  2.12s/it] 10%|█         | 31/300 [01:09<09:29,  2.12s/it] 11%|█         | 32/300 [01:12<09:34,  2.15s/it] 11%|█         | 33/300 [01:14<09:39,  2.17s/it] 11%|█▏        | 34/300 [01:16<09:33,  2.16s/it] 12%|█▏        | 35/300 [01:18<09:27,  2.14s/it] 12%|█▏        | 36/300 [01:20<09:29,  2.16s/it] 12%|█▏        | 37/300 [01:22<09:23,  2.14s/it] 13%|█▎        | 38/300 [01:25<09:34,  2.19s/it] 13%|█▎        | 39/300 [01:28<10:33,  2.43s/it] 13%|█▎        | 40/300 [01:30<10:30,  2.42s/it] 14%|█▎        | 41/300 [01:32<10:19,  2.39s/it] 14%|█▍        | 42/300 [01:35<09:56,  2.31s/it] 14%|█▍        | 43/300 [01:37<09:57,  2.33s/it] 15%|█▍        | 44/300 [01:39<09:45,  2.29s/it] 15%|█▌        | 45/300 [01:41<09:39,  2.27s/it] 15%|█▌        | 46/300 [01:44<09:30,  2.24s/it] 16%|█▌        | 47/300 [01:46<09:24,  2.23s/it] 16%|█▌        | 48/300 [01:48<09:21,  2.23s/it] 16%|█▋        | 49/300 [01:50<09:19,  2.23s/it] 17%|█▋        | 50/300 [01:52<09:14,  2.22s/it] 17%|█▋        | 51/300 [01:55<09:08,  2.20s/it] 17%|█▋        | 52/300 [01:57<08:56,  2.16s/it] 18%|█▊        | 53/300 [01:59<08:50,  2.15s/it] 18%|█▊        | 54/300 [02:01<08:54,  2.17s/it] 18%|█▊        | 55/300 [02:03<08:52,  2.17s/it] 19%|█▊        | 56/300 [02:05<08:43,  2.15s/it] 19%|█▉        | 57/300 [02:07<08:47,  2.17s/it] 19%|█▉        | 58/300 [02:10<08:54,  2.21s/it] 20%|█▉        | 59/300 [02:12<08:43,  2.17s/it] 20%|██        | 60/300 [02:14<08:34,  2.14s/it] 20%|██        | 61/300 [02:16<08:30,  2.14s/it] 21%|██        | 62/300 [02:18<08:28,  2.14s/it] 21%|██        | 63/300 [02:20<08:32,  2.16s/it] 21%|██▏       | 64/300 [02:23<08:32,  2.17s/it] 22%|██▏       | 65/300 [02:25<08:33,  2.18s/it] 22%|██▏       | 66/300 [02:27<08:30,  2.18s/it] 22%|██▏       | 67/300 [02:29<08:27,  2.18s/it] 23%|██▎       | 68/300 [02:31<08:22,  2.17s/it] 23%|██▎       | 69/300 [02:33<08:14,  2.14s/it] 23%|██▎       | 70/300 [02:36<08:12,  2.14s/it] 24%|██▎       | 71/300 [02:38<08:12,  2.15s/it] 24%|██▍       | 72/300 [02:40<08:08,  2.14s/it] 24%|██▍       | 73/300 [02:42<08:02,  2.13s/it] 25%|██▍       | 74/300 [02:44<08:18,  2.21s/it] 25%|██▌       | 75/300 [02:46<08:09,  2.17s/it] 25%|██▌       | 76/300 [02:49<08:06,  2.17s/it] 26%|██▌       | 77/300 [02:51<08:02,  2.16s/it] 26%|██▌       | 78/300 [02:53<08:02,  2.17s/it] 26%|██▋       | 79/300 [02:55<07:56,  2.16s/it] 27%|██▋       | 80/300 [02:57<07:50,  2.14s/it] 27%|██▋       | 81/300 [02:59<07:56,  2.18s/it] 27%|██▋       | 82/300 [03:02<07:54,  2.18s/it] 28%|██▊       | 83/300 [03:04<07:51,  2.17s/it] 28%|██▊       | 84/300 [03:06<07:47,  2.16s/it] 28%|██▊       | 85/300 [03:08<07:39,  2.14s/it] 29%|██▊       | 86/300 [03:10<07:31,  2.11s/it] 29%|██▉       | 87/300 [03:12<07:35,  2.14s/it] 29%|██▉       | 88/300 [03:14<07:31,  2.13s/it] 30%|██▉       | 89/300 [03:16<07:31,  2.14s/it] 30%|███       | 90/300 [03:19<07:38,  2.18s/it] 30%|███       | 91/300 [03:21<07:43,  2.22s/it] 31%|███       | 92/300 [03:23<07:34,  2.19s/it] 31%|███       | 93/300 [03:25<07:27,  2.16s/it] 31%|███▏      | 94/300 [03:27<07:20,  2.14s/it] 32%|███▏      | 95/300 [03:29<07:18,  2.14s/it] 32%|███▏      | 96/300 [03:32<07:18,  2.15s/it] 32%|███▏      | 97/300 [03:34<07:13,  2.13s/it] 33%|███▎      | 98/300 [03:36<07:08,  2.12s/it] 33%|███▎      | 99/300 [03:38<07:04,  2.11s/it] 33%|███▎      | 100/300 [03:40<07:06,  2.13s/it] 34%|███▎      | 101/300 [03:42<07:14,  2.19s/it] 34%|███▍      | 102/300 [03:45<07:27,  2.26s/it] 34%|███▍      | 103/300 [03:47<07:28,  2.28s/it] 35%|███▍      | 104/300 [03:49<07:27,  2.28s/it] 35%|███▌      | 105/300 [03:52<07:34,  2.33s/it] 35%|███▌      | 106/300 [03:54<07:20,  2.27s/it] 36%|███▌      | 107/300 [03:56<07:22,  2.29s/it] 36%|███▌      | 108/300 [03:59<07:11,  2.25s/it] 36%|███▋      | 109/300 [04:01<06:59,  2.20s/it] 37%|███▋      | 110/300 [04:03<06:54,  2.18s/it] 37%|███▋      | 111/300 [04:05<06:47,  2.16s/it] 37%|███▋      | 112/300 [04:07<06:45,  2.15s/it] 38%|███▊      | 113/300 [04:10<07:14,  2.32s/it] 38%|███▊      | 114/300 [04:12<07:31,  2.43s/it] 38%|███▊      | 115/300 [04:15<07:15,  2.36s/it] 39%|███▊      | 116/300 [04:17<07:17,  2.38s/it] 39%|███▉      | 117/300 [04:19<07:02,  2.31s/it] 39%|███▉      | 118/300 [04:21<07:01,  2.32s/it] 40%|███▉      | 119/300 [04:24<06:48,  2.26s/it] 40%|████      | 120/300 [04:26<06:40,  2.22s/it] 40%|████      | 121/300 [04:28<06:39,  2.23s/it] 41%|████      | 122/300 [04:30<06:37,  2.23s/it] 41%|████      | 123/300 [04:33<06:43,  2.28s/it] 41%|████▏     | 124/300 [04:35<06:34,  2.24s/it] 42%|████▏     | 125/300 [04:37<06:34,  2.25s/it] 42%|████▏     | 126/300 [04:39<06:31,  2.25s/it] 42%|████▏     | 127/300 [04:42<06:32,  2.27s/it] 43%|████▎     | 128/300 [04:44<06:20,  2.21s/it] 43%|████▎     | 129/300 [04:46<06:14,  2.19s/it] 43%|████▎     | 130/300 [04:48<06:12,  2.19s/it] 44%|████▎     | 131/300 [04:50<06:19,  2.24s/it] 44%|████▍     | 132/300 [04:52<06:08,  2.19s/it] 44%|████▍     | 133/300 [04:55<06:02,  2.17s/it] 45%|████▍     | 134/300 [04:57<06:07,  2.21s/it] 45%|████▌     | 135/300 [04:59<06:03,  2.20s/it] 45%|████▌     | 136/300 [05:01<06:02,  2.21s/it] 46%|████▌     | 137/300 [05:03<05:58,  2.20s/it] 46%|████▌     | 138/300 [05:06<05:59,  2.22s/it] 46%|████▋     | 139/300 [05:08<06:02,  2.25s/it] 47%|████▋     | 140/300 [05:10<05:52,  2.20s/it] 47%|████▋     | 141/300 [05:12<05:47,  2.18s/it] 47%|████▋     | 142/300 [05:15<05:49,  2.21s/it] 48%|████▊     | 143/300 [05:17<05:48,  2.22s/it] 48%|████▊     | 144/300 [05:19<05:42,  2.20s/it] 48%|████▊     | 145/300 [05:21<05:38,  2.19s/it] 49%|████▊     | 146/300 [05:23<05:42,  2.22s/it] 49%|████▉     | 147/300 [05:26<05:42,  2.24s/it] 49%|████▉     | 148/300 [05:28<05:38,  2.23s/it] 50%|████▉     | 149/300 [05:30<05:37,  2.24s/it] 50%|█████     | 150/300 [05:32<05:38,  2.26s/it] 50%|█████     | 151/300 [05:35<05:30,  2.22s/it] 51%|█████     | 152/300 [05:37<05:25,  2.20s/it] 51%|█████     | 153/300 [05:39<05:23,  2.20s/it] 51%|█████▏    | 154/300 [05:41<05:18,  2.18s/it] 52%|█████▏    | 155/300 [05:43<05:11,  2.15s/it] 52%|█████▏    | 156/300 [05:45<05:11,  2.16s/it] 52%|█████▏    | 157/300 [05:47<05:06,  2.15s/it] 53%|█████▎    | 158/300 [05:50<05:08,  2.17s/it] 53%|█████▎    | 159/300 [05:52<05:10,  2.20s/it] 53%|█████▎    | 160/300 [05:54<05:13,  2.24s/it] 54%|█████▎    | 161/300 [05:56<05:06,  2.20s/it] 54%|█████▍    | 162/300 [05:59<05:09,  2.25s/it] 54%|█████▍    | 163/300 [06:01<05:03,  2.22s/it] 55%|█████▍    | 164/300 [06:03<05:03,  2.23s/it] 55%|█████▌    | 165/300 [06:05<04:55,  2.19s/it] 55%|█████▌    | 166/300 [06:08<04:56,  2.21s/it] 56%|█████▌    | 167/300 [06:10<04:58,  2.25s/it] 56%|█████▌    | 168/300 [06:12<04:58,  2.26s/it] 56%|█████▋    | 169/300 [06:15<04:59,  2.28s/it] 57%|█████▋    | 170/300 [06:17<04:59,  2.30s/it] 57%|█████▋    | 171/300 [06:19<04:56,  2.30s/it] 57%|█████▋    | 172/300 [06:21<04:49,  2.26s/it] 58%|█████▊    | 173/300 [06:24<04:46,  2.26s/it] 58%|█████▊    | 174/300 [06:26<04:42,  2.24s/it] 58%|█████▊    | 175/300 [06:28<04:39,  2.23s/it] 59%|█████▊    | 176/300 [06:30<04:32,  2.19s/it] 59%|█████▉    | 177/300 [06:32<04:33,  2.23s/it] 59%|█████▉    | 178/300 [06:34<04:27,  2.19s/it] 60%|█████▉    | 179/300 [06:37<04:22,  2.17s/it] 60%|██████    | 180/300 [06:39<04:21,  2.18s/it] 60%|██████    | 181/300 [06:41<04:20,  2.19s/it] 61%|██████    | 182/300 [06:43<04:22,  2.23s/it] 61%|██████    | 183/300 [06:45<04:17,  2.20s/it] 61%|██████▏   | 184/300 [06:48<04:15,  2.20s/it] 62%|██████▏   | 185/300 [06:50<04:12,  2.20s/it] 62%|██████▏   | 186/300 [06:52<04:08,  2.18s/it] 62%|██████▏   | 187/300 [06:54<04:09,  2.20s/it] 63%|██████▎   | 188/300 [06:56<04:07,  2.21s/it] 63%|██████▎   | 189/300 [06:59<04:04,  2.20s/it] 63%|██████▎   | 190/300 [07:01<04:04,  2.22s/it] 64%|██████▎   | 191/300 [07:03<04:00,  2.20s/it] 64%|██████▍   | 192/300 [07:05<04:02,  2.25s/it] 64%|██████▍   | 193/300 [07:08<04:02,  2.27s/it] 65%|██████▍   | 194/300 [07:10<03:58,  2.25s/it] 65%|██████▌   | 195/300 [07:12<03:59,  2.28s/it] 65%|██████▌   | 196/300 [07:14<03:51,  2.23s/it] 66%|██████▌   | 197/300 [07:17<03:51,  2.25s/it] 66%|██████▌   | 198/300 [07:19<03:51,  2.27s/it] 66%|██████▋   | 199/300 [07:21<03:49,  2.27s/it] 67%|██████▋   | 200/300 [07:24<03:44,  2.24s/it] 67%|██████▋   | 201/300 [07:26<03:41,  2.23s/it] 67%|██████▋   | 202/300 [07:28<03:36,  2.21s/it] 68%|██████▊   | 203/300 [07:30<03:37,  2.24s/it] 68%|██████▊   | 204/300 [07:33<03:37,  2.27s/it] 68%|██████▊   | 205/300 [07:35<03:33,  2.25s/it] 69%|██████▊   | 206/300 [07:37<03:27,  2.21s/it] 69%|██████▉   | 207/300 [07:39<03:24,  2.20s/it] 69%|██████▉   | 208/300 [07:41<03:25,  2.23s/it] 70%|██████▉   | 209/300 [07:44<03:24,  2.24s/it] 70%|███████   | 210/300 [07:46<03:20,  2.22s/it] 70%|███████   | 211/300 [07:48<03:15,  2.19s/it] 71%|███████   | 212/300 [07:50<03:13,  2.20s/it] 71%|███████   | 213/300 [07:52<03:09,  2.17s/it] 71%|███████▏  | 214/300 [07:55<03:11,  2.22s/it] 72%|███████▏  | 215/300 [07:57<03:08,  2.22s/it] 72%|███████▏  | 216/300 [07:59<03:08,  2.24s/it] 72%|███████▏  | 217/300 [08:01<03:03,  2.21s/it] 73%|███████▎  | 218/300 [08:03<03:02,  2.22s/it] 73%|███████▎  | 219/300 [08:06<02:58,  2.21s/it] 73%|███████▎  | 220/300 [08:08<02:55,  2.20s/it] 74%|███████▎  | 221/300 [08:10<02:53,  2.20s/it] 74%|███████▍  | 222/300 [08:12<02:50,  2.19s/it] 74%|███████▍  | 223/300 [08:14<02:51,  2.23s/it] 75%|███████▍  | 224/300 [08:17<02:47,  2.21s/it] 75%|███████▌  | 225/300 [08:19<02:48,  2.25s/it] 75%|███████▌  | 226/300 [08:21<02:43,  2.20s/it] 76%|███████▌  | 227/300 [08:23<02:39,  2.19s/it] 76%|███████▌  | 228/300 [08:25<02:37,  2.19s/it] 76%|███████▋  | 229/300 [08:28<02:34,  2.18s/it] 77%|███████▋  | 230/300 [08:30<02:35,  2.22s/it] 77%|███████▋  | 231/300 [08:32<02:32,  2.21s/it] 77%|███████▋  | 232/300 [08:34<02:32,  2.24s/it] 78%|███████▊  | 233/300 [08:37<02:31,  2.27s/it] 78%|███████▊  | 234/300 [08:39<02:29,  2.26s/it] 78%|███████▊  | 235/300 [08:41<02:24,  2.23s/it] 79%|███████▊  | 236/300 [08:43<02:20,  2.20s/it] 79%|███████▉  | 237/300 [08:45<02:18,  2.19s/it] 79%|███████▉  | 238/300 [08:48<02:15,  2.19s/it] 80%|███████▉  | 239/300 [08:50<02:14,  2.21s/it] 80%|████████  | 240/300 [08:52<02:13,  2.22s/it] 80%|████████  | 241/300 [08:54<02:11,  2.23s/it] 81%|████████  | 242/300 [08:56<02:07,  2.20s/it] 81%|████████  | 243/300 [08:59<02:04,  2.19s/it] 81%|████████▏ | 244/300 [09:01<02:02,  2.18s/it] 82%|████████▏ | 245/300 [09:03<02:00,  2.20s/it] 82%|████████▏ | 246/300 [09:05<02:00,  2.24s/it] 82%|████████▏ | 247/300 [09:08<01:58,  2.24s/it] 83%|████████▎ | 248/300 [09:10<01:54,  2.21s/it] 83%|████████▎ | 249/300 [09:12<01:52,  2.21s/it] 83%|████████▎ | 250/300 [09:14<01:49,  2.18s/it] 84%|████████▎ | 251/300 [09:16<01:47,  2.19s/it] 84%|████████▍ | 252/300 [09:19<01:45,  2.20s/it] 84%|████████▍ | 253/300 [09:21<01:45,  2.24s/it] 85%|████████▍ | 254/300 [09:23<01:43,  2.24s/it] 85%|████████▌ | 255/300 [09:25<01:41,  2.25s/it] 85%|████████▌ | 256/300 [09:28<01:38,  2.24s/it] 86%|████████▌ | 257/300 [09:30<01:37,  2.26s/it] 86%|████████▌ | 258/300 [09:32<01:34,  2.24s/it] 86%|████████▋ | 259/300 [09:34<01:33,  2.27s/it] 87%|████████▋ | 260/300 [09:37<01:29,  2.23s/it] 87%|████████▋ | 261/300 [09:39<01:26,  2.22s/it] 87%|████████▋ | 262/300 [09:41<01:24,  2.22s/it] 88%|████████▊ | 263/300 [09:43<01:21,  2.21s/it] 88%|████████▊ | 264/300 [09:45<01:20,  2.24s/it] 88%|████████▊ | 265/300 [09:48<01:18,  2.25s/it] 89%|████████▊ | 266/300 [09:50<01:15,  2.22s/it] 89%|████████▉ | 267/300 [09:52<01:12,  2.20s/it] 89%|████████▉ | 268/300 [09:54<01:11,  2.22s/it] 90%|████████▉ | 269/300 [09:57<01:09,  2.25s/it] 90%|█████████ | 270/300 [09:59<01:08,  2.27s/it] 90%|█████████ | 271/300 [10:01<01:06,  2.28s/it] 91%|█████████ | 272/300 [10:04<01:03,  2.28s/it] 91%|█████████ | 273/300 [10:06<01:01,  2.26s/it] 91%|█████████▏| 274/300 [10:08<00:58,  2.25s/it] 92%|█████████▏| 275/300 [10:10<00:56,  2.27s/it] 92%|█████████▏| 276/300 [10:13<00:54,  2.27s/it] 92%|█████████▏| 277/300 [10:15<00:51,  2.26s/it] 93%|█████████▎| 278/300 [10:17<00:49,  2.24s/it] 93%|█████████▎| 279/300 [10:19<00:46,  2.23s/it] 93%|█████████▎| 280/300 [10:22<00:44,  2.24s/it] 94%|█████████▎| 281/300 [10:24<00:42,  2.23s/it] 94%|█████████▍| 282/300 [10:26<00:40,  2.23s/it] 94%|█████████▍| 283/300 [10:28<00:37,  2.22s/it] 95%|█████████▍| 284/300 [10:30<00:35,  2.23s/it] 95%|█████████▌| 285/300 [10:33<00:33,  2.23s/it] 95%|█████████▌| 286/300 [10:35<00:31,  2.25s/it] 96%|█████████▌| 287/300 [10:37<00:29,  2.23s/it] 96%|█████████▌| 288/300 [10:39<00:26,  2.20s/it] 96%|█████████▋| 289/300 [10:42<00:24,  2.22s/it] 97%|█████████▋| 290/300 [10:44<00:21,  2.19s/it] 97%|█████████▋| 291/300 [10:46<00:19,  2.20s/it] 97%|█████████▋| 292/300 [10:48<00:17,  2.19s/it] 98%|█████████▊| 293/300 [10:50<00:15,  2.18s/it] 98%|█████████▊| 294/300 [10:52<00:12,  2.14s/it] 98%|█████████▊| 295/300 [10:54<00:10,  2.16s/it] 99%|█████████▊| 296/300 [10:57<00:08,  2.19s/it] 99%|█████████▉| 297/300 [10:59<00:06,  2.18s/it] 99%|█████████▉| 298/300 [11:01<00:04,  2.24s/it]100%|█████████▉| 299/300 [11:03<00:02,  2.21s/it]100%|██████████| 300/300 [11:06<00:00,  2.21s/it]100%|██████████| 300/300 [11:06<00:00,  2.22s/it]
W&B disabled.
W&B online. Running your script from this directory will now sync to the cloud.
wandb: Currently logged in as: hl57. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.12 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.10
wandb: Run data is saved locally in /media/techt/One Touch/DD/SRe2L/train/wandb/run-20231006_202205-q5aogz4t
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rosy-terrain-400
wandb: ⭐️ View project at https://wandb.ai/hl57/final_rn18_fkd
wandb: 🚀 View run at https://wandb.ai/hl57/final_rn18_fkd/runs/q5aogz4t
======= FKD: dataset info ======
path: ../relabel/FKD_cutmix_fp16/tiny-imagenet/232/
num img: 2000
batch size: 100
max epoch: 300
================================
load data successfully
=> loading student model 'resnet18'

Epoch: 0
TRAIN Iter 0: lr = 0.001000,	loss = 0.007797,	Top-1 err = 99.400000,	Top-5 err = 96.500000,	train_time = 16.078362
TEST Iter 0: loss = 7.985452,	Top-1 err = 99.350000,	Top-5 err = 95.740000,	val_time = 18.663135

Epoch: 1

Epoch: 2

Epoch: 3

Epoch: 4

Epoch: 5

Epoch: 6

Epoch: 7

Epoch: 8

Epoch: 9

Epoch: 10
TRAIN Iter 10: lr = 0.000997,	loss = 0.006232,	Top-1 err = 97.200000,	Top-5 err = 89.600000,	train_time = 15.039721
TEST Iter 10: loss = 7.006927,	Top-1 err = 97.880000,	Top-5 err = 90.190000,	val_time = 18.388402

Epoch: 11

Epoch: 12

Epoch: 13

Epoch: 14

Epoch: 15

Epoch: 16

Epoch: 17

Epoch: 18

Epoch: 19

Epoch: 20
TRAIN Iter 20: lr = 0.000989,	loss = 0.005987,	Top-1 err = 94.600000,	Top-5 err = 79.000000,	train_time = 15.315940
TEST Iter 20: loss = 5.273078,	Top-1 err = 95.520000,	Top-5 err = 83.340000,	val_time = 18.296495

Epoch: 21

Epoch: 22

Epoch: 23

Epoch: 24

Epoch: 25

Epoch: 26

Epoch: 27

Epoch: 28

Epoch: 29

Epoch: 30
TRAIN Iter 30: lr = 0.000976,	loss = 0.005007,	Top-1 err = 92.800000,	Top-5 err = 77.550000,	train_time = 15.128410
TEST Iter 30: loss = 4.757521,	Top-1 err = 93.030000,	Top-5 err = 76.450000,	val_time = 18.046110

Epoch: 31

Epoch: 32

Epoch: 33

Epoch: 34

Epoch: 35

Epoch: 36

Epoch: 37

Epoch: 38

Epoch: 39

Epoch: 40
TRAIN Iter 40: lr = 0.000957,	loss = 0.004704,	Top-1 err = 90.350000,	Top-5 err = 71.100000,	train_time = 15.065287
TEST Iter 40: loss = 5.102330,	Top-1 err = 93.390000,	Top-5 err = 77.800000,	val_time = 18.357486

Epoch: 41

Epoch: 42

Epoch: 43

Epoch: 44

Epoch: 45

Epoch: 46

Epoch: 47

Epoch: 48

Epoch: 49

Epoch: 50
TRAIN Iter 50: lr = 0.000933,	loss = 0.004573,	Top-1 err = 85.500000,	Top-5 err = 63.850000,	train_time = 15.915310
TEST Iter 50: loss = 5.313874,	Top-1 err = 93.130000,	Top-5 err = 77.630000,	val_time = 18.142942

Epoch: 51

Epoch: 52

Epoch: 53

Epoch: 54

Epoch: 55

Epoch: 56

Epoch: 57

Epoch: 58

Epoch: 59

Epoch: 60
TRAIN Iter 60: lr = 0.000905,	loss = 0.004222,	Top-1 err = 85.800000,	Top-5 err = 62.850000,	train_time = 15.375179
TEST Iter 60: loss = 4.631824,	Top-1 err = 89.310000,	Top-5 err = 70.290000,	val_time = 17.928552

Epoch: 61

Epoch: 62

Epoch: 63

Epoch: 64

Epoch: 65

Epoch: 66

Epoch: 67

Epoch: 68

Epoch: 69

Epoch: 70
TRAIN Iter 70: lr = 0.000872,	loss = 0.004116,	Top-1 err = 78.400000,	Top-5 err = 52.800000,	train_time = 15.395719
TEST Iter 70: loss = 4.450979,	Top-1 err = 88.100000,	Top-5 err = 67.050000,	val_time = 17.848499

Epoch: 71

Epoch: 72

Epoch: 73

Epoch: 74

Epoch: 75

Epoch: 76

Epoch: 77

Epoch: 78

Epoch: 79

Epoch: 80
TRAIN Iter 80: lr = 0.000835,	loss = 0.003827,	Top-1 err = 79.650000,	Top-5 err = 55.050000,	train_time = 15.385622
TEST Iter 80: loss = 4.435603,	Top-1 err = 87.960000,	Top-5 err = 67.670000,	val_time = 17.989500

Epoch: 81

Epoch: 82

Epoch: 83

Epoch: 84

Epoch: 85

Epoch: 86

Epoch: 87

Epoch: 88

Epoch: 89

Epoch: 90
TRAIN Iter 90: lr = 0.000794,	loss = 0.003496,	Top-1 err = 83.600000,	Top-5 err = 62.550000,	train_time = 15.060918
TEST Iter 90: loss = 4.683684,	Top-1 err = 86.840000,	Top-5 err = 64.790000,	val_time = 17.877290

Epoch: 91

Epoch: 92

Epoch: 93

Epoch: 94

Epoch: 95

Epoch: 96

Epoch: 97

Epoch: 98

Epoch: 99

Epoch: 100
TRAIN Iter 100: lr = 0.000750,	loss = 0.003422,	Top-1 err = 70.250000,	Top-5 err = 42.500000,	train_time = 14.966165
TEST Iter 100: loss = 4.244259,	Top-1 err = 83.860000,	Top-5 err = 60.320000,	val_time = 17.932952

Epoch: 101

Epoch: 102

Epoch: 103

Epoch: 104

Epoch: 105

Epoch: 106

Epoch: 107

Epoch: 108

Epoch: 109

Epoch: 110
TRAIN Iter 110: lr = 0.000703,	loss = 0.003293,	Top-1 err = 77.950000,	Top-5 err = 55.850000,	train_time = 14.912474
TEST Iter 110: loss = 4.309929,	Top-1 err = 83.300000,	Top-5 err = 58.810000,	val_time = 17.804614

Epoch: 111

Epoch: 112

Epoch: 113

Epoch: 114

Epoch: 115

Epoch: 116

Epoch: 117

Epoch: 118

Epoch: 119

Epoch: 120
TRAIN Iter 120: lr = 0.000655,	loss = 0.003215,	Top-1 err = 76.600000,	Top-5 err = 54.550000,	train_time = 14.963402
TEST Iter 120: loss = 4.155973,	Top-1 err = 81.280000,	Top-5 err = 56.370000,	val_time = 17.820529

Epoch: 121

Epoch: 122

Epoch: 123

Epoch: 124

Epoch: 125

Epoch: 126

Epoch: 127

Epoch: 128

Epoch: 129

Epoch: 130
TRAIN Iter 130: lr = 0.000604,	loss = 0.003000,	Top-1 err = 68.350000,	Top-5 err = 44.950000,	train_time = 14.941975
TEST Iter 130: loss = 3.919400,	Top-1 err = 79.770000,	Top-5 err = 55.080000,	val_time = 17.819609

Epoch: 131

Epoch: 132

Epoch: 133

Epoch: 134

Epoch: 135

Epoch: 136

Epoch: 137

Epoch: 138

Epoch: 139

Epoch: 140
TRAIN Iter 140: lr = 0.000552,	loss = 0.002856,	Top-1 err = 66.050000,	Top-5 err = 40.800000,	train_time = 14.968655
TEST Iter 140: loss = 3.904531,	Top-1 err = 77.460000,	Top-5 err = 51.090000,	val_time = 17.753814

Epoch: 141

Epoch: 142

Epoch: 143

Epoch: 144

Epoch: 145

Epoch: 146

Epoch: 147

Epoch: 148

Epoch: 149

Epoch: 150
TRAIN Iter 150: lr = 0.000500,	loss = 0.002859,	Top-1 err = 64.450000,	Top-5 err = 42.100000,	train_time = 14.967163
TEST Iter 150: loss = 4.131060,	Top-1 err = 78.000000,	Top-5 err = 52.460000,	val_time = 17.741004

Epoch: 151

Epoch: 152

Epoch: 153

Epoch: 154

Epoch: 155

Epoch: 156

Epoch: 157

Epoch: 158

Epoch: 159

Epoch: 160
TRAIN Iter 160: lr = 0.000448,	loss = 0.002650,	Top-1 err = 62.900000,	Top-5 err = 39.700000,	train_time = 14.790878
TEST Iter 160: loss = 3.490275,	Top-1 err = 74.510000,	Top-5 err = 46.040000,	val_time = 17.722433

Epoch: 161

Epoch: 162

Epoch: 163

Epoch: 164

Epoch: 165

Epoch: 166

Epoch: 167

Epoch: 168

Epoch: 169

Epoch: 170
TRAIN Iter 170: lr = 0.000396,	loss = 0.002637,	Top-1 err = 69.450000,	Top-5 err = 49.500000,	train_time = 15.409443
TEST Iter 170: loss = 3.553504,	Top-1 err = 73.360000,	Top-5 err = 46.250000,	val_time = 18.149281

Epoch: 171

Epoch: 172

Epoch: 173

Epoch: 174

Epoch: 175

Epoch: 176

Epoch: 177

Epoch: 178

Epoch: 179

Epoch: 180
TRAIN Iter 180: lr = 0.000345,	loss = 0.002607,	Top-1 err = 57.450000,	Top-5 err = 34.450000,	train_time = 15.280732
TEST Iter 180: loss = 3.456957,	Top-1 err = 72.730000,	Top-5 err = 44.000000,	val_time = 18.174600

Epoch: 181

Epoch: 182

Epoch: 183

Epoch: 184

Epoch: 185

Epoch: 186

Epoch: 187

Epoch: 188

Epoch: 189

Epoch: 190
TRAIN Iter 190: lr = 0.000297,	loss = 0.002473,	Top-1 err = 56.600000,	Top-5 err = 36.550000,	train_time = 15.227890
TEST Iter 190: loss = 3.184177,	Top-1 err = 69.850000,	Top-5 err = 41.570000,	val_time = 18.800379

Epoch: 191

Epoch: 192

Epoch: 193

Epoch: 194

Epoch: 195

Epoch: 196

Epoch: 197

Epoch: 198

Epoch: 199

Epoch: 200
TRAIN Iter 200: lr = 0.000250,	loss = 0.002432,	Top-1 err = 59.600000,	Top-5 err = 39.700000,	train_time = 14.930180
TEST Iter 200: loss = 3.052129,	Top-1 err = 68.780000,	Top-5 err = 40.090000,	val_time = 18.342588

Epoch: 201

Epoch: 202

Epoch: 203

Epoch: 204

Epoch: 205

Epoch: 206

Epoch: 207

Epoch: 208

Epoch: 209

Epoch: 210
TRAIN Iter 210: lr = 0.000206,	loss = 0.002353,	Top-1 err = 48.700000,	Top-5 err = 27.750000,	train_time = 15.002646
TEST Iter 210: loss = 3.067910,	Top-1 err = 68.220000,	Top-5 err = 38.810000,	val_time = 17.829837

Epoch: 211

Epoch: 212

Epoch: 213

Epoch: 214

Epoch: 215

Epoch: 216

Epoch: 217

Epoch: 218

Epoch: 219

Epoch: 220
TRAIN Iter 220: lr = 0.000165,	loss = 0.002316,	Top-1 err = 59.700000,	Top-5 err = 38.700000,	train_time = 15.105110
TEST Iter 220: loss = 3.004726,	Top-1 err = 67.170000,	Top-5 err = 38.210000,	val_time = 17.914568

Epoch: 221

Epoch: 222

Epoch: 223

Epoch: 224

Epoch: 225

Epoch: 226

Epoch: 227

Epoch: 228

Epoch: 229

Epoch: 230
TRAIN Iter 230: lr = 0.000128,	loss = 0.002312,	Top-1 err = 65.250000,	Top-5 err = 46.500000,	train_time = 15.528099
TEST Iter 230: loss = 3.019869,	Top-1 err = 66.870000,	Top-5 err = 38.210000,	val_time = 18.628665

Epoch: 231

Epoch: 232

Epoch: 233

Epoch: 234

Epoch: 235

Epoch: 236

Epoch: 237

Epoch: 238

Epoch: 239

Epoch: 240
TRAIN Iter 240: lr = 0.000095,	loss = 0.002256,	Top-1 err = 57.800000,	Top-5 err = 38.750000,	train_time = 14.917272
TEST Iter 240: loss = 3.141733,	Top-1 err = 68.910000,	Top-5 err = 40.280000,	val_time = 17.933511

Epoch: 241

Epoch: 242

Epoch: 243

Epoch: 244

Epoch: 245

Epoch: 246

Epoch: 247

Epoch: 248

Epoch: 249

Epoch: 250
TRAIN Iter 250: lr = 0.000067,	loss = 0.002186,	Top-1 err = 53.000000,	Top-5 err = 36.000000,	train_time = 15.171665
TEST Iter 250: loss = 2.946256,	Top-1 err = 66.510000,	Top-5 err = 37.390000,	val_time = 18.357142

Epoch: 251

Epoch: 252

Epoch: 253

Epoch: 254

Epoch: 255

Epoch: 256

Epoch: 257

Epoch: 258

Epoch: 259

Epoch: 260
TRAIN Iter 260: lr = 0.000043,	loss = 0.002213,	Top-1 err = 50.100000,	Top-5 err = 27.700000,	train_time = 15.262114
TEST Iter 260: loss = 2.949482,	Top-1 err = 65.930000,	Top-5 err = 37.280000,	val_time = 17.837543

Epoch: 261

Epoch: 262

Epoch: 263

Epoch: 264

Epoch: 265

Epoch: 266

Epoch: 267

Epoch: 268

Epoch: 269

Epoch: 270
TRAIN Iter 270: lr = 0.000024,	loss = 0.002213,	Top-1 err = 59.100000,	Top-5 err = 36.350000,	train_time = 15.597858
TEST Iter 270: loss = 2.894094,	Top-1 err = 65.210000,	Top-5 err = 36.450000,	val_time = 19.115739

Epoch: 271

Epoch: 272

Epoch: 273

Epoch: 274

Epoch: 275

Epoch: 276

Epoch: 277

Epoch: 278

Epoch: 279

Epoch: 280
TRAIN Iter 280: lr = 0.000011,	loss = 0.002156,	Top-1 err = 51.600000,	Top-5 err = 32.600000,	train_time = 15.611244
TEST Iter 280: loss = 2.911015,	Top-1 err = 64.980000,	Top-5 err = 36.550000,	val_time = 18.401326

Epoch: 281

Epoch: 282

Epoch: 283

Epoch: 284

Epoch: 285

Epoch: 286

Epoch: 287

Epoch: 288

Epoch: 289

Epoch: 290
TRAIN Iter 290: lr = 0.000003,	loss = 0.002116,	Top-1 err = 57.800000,	Top-5 err = 38.550000,	train_time = 15.914810
TEST Iter 290: loss = 2.912919,	Top-1 err = 65.110000,	Top-5 err = 36.400000,	val_time = 18.095720

Epoch: 291

Epoch: 292

Epoch: 293

Epoch: 294

Epoch: 295

Epoch: 296

Epoch: 297

Epoch: 298

Epoch: 299
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.004 MB of 0.029 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.029 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:  train/Top1 ▁▁▁▂▂▂▂▂▃▄▃▄▅▅▄▅▄▆▅▅▆▆▇▇▆▆▆▇██▅█▆▇▇▆▆▇▇▆
wandb:  train/Top5 ▁▁▂▃▃▃▄▄▅▅▅▅▇▇▅▆▅▇▆▇▇▇██▆▆▆▇██▆█▆█▇▇▆▇▇▆
wandb: train/epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  train/loss █▆▆▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    train/lr ███████▇▇▇▇▇▇▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁
wandb:   val/epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███
wandb:    val/loss █▇▄▄▄▄▃▃▃▃▃▃▃▂▂▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    val/top1 ▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇██▇██████
wandb:    val/top5 ▁▂▂▃▃▃▄▄▄▅▅▅▆▆▆▆▇▇▇▇███████████
wandb: 
wandb: Run summary:
wandb:  train/Top1 37.55
wandb:  train/Top5 58.2
wandb: train/epoch 299
wandb:  train/loss 0.00216
wandb:    train/lr 0.0
wandb:   val/epoch 299
wandb:    val/loss 2.90155
wandb:    val/top1 35.02
wandb:    val/top5 63.67
wandb: 
wandb: 🚀 View run rosy-terrain-400 at: https://wandb.ai/hl57/final_rn18_fkd/runs/q5aogz4t
wandb: ️⚡ View job at https://wandb.ai/hl57/final_rn18_fkd/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjk3MzkyNjU3/version_details/v37
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231006_202205-q5aogz4t/logs
TEST Iter 299: loss = 2.901547,	Top-1 err = 64.980000,	Top-5 err = 36.330000,	val_time = 17.966973
