/home/techt/anaconda3/envs/hl/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/techt/anaconda3/envs/hl/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
/home/techt/anaconda3/envs/hl/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
r_bn:  1.0
lr:  0.01
bc shape torch.Size([200, 10, 512])
get_images call
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 503.1302581263499
main criterion 133.03730158338115
weighted_aux_loss 370.09295654296875
loss_r_bn_feature 370.09295654296875
------------iteration 100----------
total loss 350.0497065767464
main criterion 87.67397415487142
weighted_aux_loss 262.375732421875
loss_r_bn_feature 262.375732421875
------------iteration 200----------
total loss 300.64619973342815
main criterion 62.20708230178751
weighted_aux_loss 238.43911743164062
loss_r_bn_feature 238.43911743164062
------------iteration 300----------
total loss 289.90222119436334
main criterion 76.15040539846488
weighted_aux_loss 213.75181579589844
loss_r_bn_feature 213.75181579589844
------------iteration 400----------
total loss 219.51403176903432
main criterion 46.927132965323366
weighted_aux_loss 172.58689880371094
loss_r_bn_feature 172.58689880371094
------------iteration 500----------
total loss 202.96264657387948
main criterion 43.17225655923104
weighted_aux_loss 159.79039001464844
loss_r_bn_feature 159.79039001464844
------------iteration 600----------
total loss 204.75416196905095
main criterion 36.75851072393377
weighted_aux_loss 167.9956512451172
loss_r_bn_feature 167.9956512451172
------------iteration 700----------
total loss 203.00758719725923
main criterion 47.292224648431116
weighted_aux_loss 155.71536254882812
loss_r_bn_feature 155.71536254882812
------------iteration 800----------
total loss 216.81285917822396
main criterion 52.85488188330208
weighted_aux_loss 163.95797729492188
loss_r_bn_feature 163.95797729492188
------------iteration 900----------
total loss 281.3372530554103
main criterion 82.3820528600978
weighted_aux_loss 198.9552001953125
loss_r_bn_feature 198.9552001953125
------------iteration 1000----------
total loss 169.6187299831251
main criterion 40.285691653046975
weighted_aux_loss 129.33303833007812
loss_r_bn_feature 129.33303833007812
------------iteration 1100----------
total loss 168.29893606684175
main criterion 31.387284455513623
weighted_aux_loss 136.91165161132812
loss_r_bn_feature 136.91165161132812
------------iteration 1200----------
total loss 172.0379022128441
main criterion 44.16107115815661
weighted_aux_loss 127.8768310546875
loss_r_bn_feature 127.8768310546875
------------iteration 1300----------
total loss 311.6446518745263
main criterion 81.08961342237785
weighted_aux_loss 230.55503845214844
loss_r_bn_feature 230.55503845214844
------------iteration 1400----------
total loss 486.16236430765355
main criterion 127.9724839365598
weighted_aux_loss 358.18988037109375
loss_r_bn_feature 358.18988037109375
------------iteration 1500----------
total loss 165.65768584338267
main criterion 36.018647757445166
weighted_aux_loss 129.6390380859375
loss_r_bn_feature 129.6390380859375
------------iteration 1600----------
total loss 174.577143011586
main criterion 38.19974738170319
weighted_aux_loss 136.3773956298828
loss_r_bn_feature 136.3773956298828
------------iteration 1700----------
total loss 158.72377742651966
main criterion 33.00178493384387
weighted_aux_loss 125.72199249267578
loss_r_bn_feature 125.72199249267578
------------iteration 1800----------
total loss 158.3440610526412
main criterion 35.571249162992764
weighted_aux_loss 122.77281188964844
loss_r_bn_feature 122.77281188964844
------------iteration 1900----------
total loss 440.1122039806874
main criterion 116.18663635373429
weighted_aux_loss 323.9255676269531
loss_r_bn_feature 323.9255676269531
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 578.3672435154623
main criterion 129.65175889632164
weighted_aux_loss 448.7154846191406
loss_r_bn_feature 448.7154846191406
------------iteration 100----------
total loss 430.37164293613
main criterion 89.22970568027063
weighted_aux_loss 341.1419372558594
loss_r_bn_feature 341.1419372558594
------------iteration 200----------
total loss 316.90668609166613
main criterion 60.126839900259874
weighted_aux_loss 256.77984619140625
loss_r_bn_feature 256.77984619140625
------------iteration 300----------
total loss 269.0217115820895
main criterion 49.9389021338473
weighted_aux_loss 219.0828094482422
loss_r_bn_feature 219.0828094482422
------------iteration 400----------
total loss 420.5894310714292
main criterion 96.89375236049172
weighted_aux_loss 323.6956787109375
loss_r_bn_feature 323.6956787109375
------------iteration 500----------
total loss 280.9782287417898
main criterion 67.64995115389915
weighted_aux_loss 213.32827758789062
loss_r_bn_feature 213.32827758789062
------------iteration 600----------
total loss 208.6695668829018
main criterion 45.794078601651776
weighted_aux_loss 162.87548828125
loss_r_bn_feature 162.87548828125
------------iteration 700----------
total loss 251.67757389020352
main criterion 68.97998783063322
weighted_aux_loss 182.6975860595703
loss_r_bn_feature 182.6975860595703
------------iteration 800----------
total loss 207.02619358584184
main criterion 36.02906223818559
weighted_aux_loss 170.99713134765625
loss_r_bn_feature 170.99713134765625
------------iteration 900----------
total loss 396.4712329915344
main criterion 99.92310677083125
weighted_aux_loss 296.5481262207031
loss_r_bn_feature 296.5481262207031
------------iteration 1000----------
total loss 399.2747948389997
main criterion 96.0975792628278
weighted_aux_loss 303.1772155761719
loss_r_bn_feature 303.1772155761719
------------iteration 1100----------
total loss 189.57798634037653
main criterion 33.98916492924373
weighted_aux_loss 155.5888214111328
loss_r_bn_feature 155.5888214111328
------------iteration 1200----------
total loss 178.03639586601838
main criterion 36.32612975273713
weighted_aux_loss 141.71026611328125
loss_r_bn_feature 141.71026611328125
------------iteration 1300----------
total loss 245.8237368414196
main criterion 61.72984951231802
weighted_aux_loss 184.09388732910156
loss_r_bn_feature 184.09388732910156
------------iteration 1400----------
total loss 234.19402497054722
main criterion 58.224176337734725
weighted_aux_loss 175.9698486328125
loss_r_bn_feature 175.9698486328125
------------iteration 1500----------
total loss 174.03739250007595
main criterion 44.00896537605252
weighted_aux_loss 130.02842712402344
loss_r_bn_feature 130.02842712402344
------------iteration 1600----------
total loss 181.0573976793167
main criterion 31.59896262072294
weighted_aux_loss 149.45843505859375
loss_r_bn_feature 149.45843505859375
------------iteration 1700----------
total loss 171.2627588369526
main criterion 44.86965428372995
weighted_aux_loss 126.39310455322266
loss_r_bn_feature 126.39310455322266
------------iteration 1800----------
total loss 187.9166560945848
main criterion 43.80924947837386
weighted_aux_loss 144.10740661621094
loss_r_bn_feature 144.10740661621094
------------iteration 1900----------
total loss 173.11184839059442
main criterion 31.255830324188178
weighted_aux_loss 141.85601806640625
loss_r_bn_feature 141.85601806640625
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 537.1535284383233
main criterion 137.14913390707324
weighted_aux_loss 400.00439453125
loss_r_bn_feature 400.00439453125
------------iteration 100----------
total loss 396.35291392628255
main criterion 95.60031993214194
weighted_aux_loss 300.7525939941406
loss_r_bn_feature 300.7525939941406
------------iteration 200----------
total loss 315.0208563753985
main criterion 66.36222600430476
weighted_aux_loss 248.65863037109375
loss_r_bn_feature 248.65863037109375
------------iteration 300----------
total loss 289.7728448907102
main criterion 66.05800114071016
weighted_aux_loss 223.71484375
loss_r_bn_feature 223.71484375
------------iteration 400----------
total loss 265.54282305544984
main criterion 46.69625017947328
weighted_aux_loss 218.84657287597656
loss_r_bn_feature 218.84657287597656
------------iteration 500----------
total loss 249.09761485230334
main criterion 44.85231455933459
weighted_aux_loss 204.24530029296875
loss_r_bn_feature 204.24530029296875
------------iteration 600----------
total loss 268.1418874588078
main criterion 64.31234339142496
weighted_aux_loss 203.8295440673828
loss_r_bn_feature 203.8295440673828
------------iteration 700----------
total loss 211.2322847069309
main criterion 39.767379921774655
weighted_aux_loss 171.46490478515625
loss_r_bn_feature 171.46490478515625
------------iteration 800----------
total loss 207.56347016113332
main criterion 36.403817451172394
weighted_aux_loss 171.15965270996094
loss_r_bn_feature 171.15965270996094
------------iteration 900----------
total loss 212.0533426220204
main criterion 48.02276400873917
weighted_aux_loss 164.03057861328125
loss_r_bn_feature 164.03057861328125
------------iteration 1000----------
total loss 195.99534669446138
main criterion 36.604386001102014
weighted_aux_loss 159.39096069335938
loss_r_bn_feature 159.39096069335938
------------iteration 1100----------
total loss 253.6327124220286
main criterion 65.3297339064036
weighted_aux_loss 188.302978515625
loss_r_bn_feature 188.302978515625
------------iteration 1200----------
total loss 187.85489024408582
main criterion 36.15837229974988
weighted_aux_loss 151.69651794433594
loss_r_bn_feature 151.69651794433594
------------iteration 1300----------
total loss 191.40318018533563
main criterion 36.11005884744501
weighted_aux_loss 155.29312133789062
loss_r_bn_feature 155.29312133789062
------------iteration 1400----------
total loss 188.04686498733145
main criterion 47.564061642604884
weighted_aux_loss 140.48280334472656
loss_r_bn_feature 140.48280334472656
------------iteration 1500----------
total loss 186.35862410436508
main criterion 32.78384077916976
weighted_aux_loss 153.5747833251953
loss_r_bn_feature 153.5747833251953
------------iteration 1600----------
total loss 204.82804138960876
main criterion 49.01604492964781
weighted_aux_loss 155.81199645996094
loss_r_bn_feature 155.81199645996094
------------iteration 1700----------
total loss 183.96673573862748
main criterion 35.32615651499466
weighted_aux_loss 148.6405792236328
loss_r_bn_feature 148.6405792236328
------------iteration 1800----------
total loss 206.78715413746434
main criterion 49.040907799573716
weighted_aux_loss 157.74624633789062
loss_r_bn_feature 157.74624633789062
------------iteration 1900----------
total loss 178.80498064172718
main criterion 35.55545366418813
weighted_aux_loss 143.24952697753906
loss_r_bn_feature 143.24952697753906
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 554.6362972790309
main criterion 149.1036739880153
weighted_aux_loss 405.5326232910156
loss_r_bn_feature 405.5326232910156
------------iteration 100----------
total loss 417.24345117090525
main criterion 103.00111108301464
weighted_aux_loss 314.2423400878906
loss_r_bn_feature 314.2423400878906
------------iteration 200----------
total loss 301.81833409778164
main criterion 67.06552648059417
weighted_aux_loss 234.7528076171875
loss_r_bn_feature 234.7528076171875
------------iteration 300----------
total loss 283.6105658033833
main criterion 67.9975286940083
weighted_aux_loss 215.613037109375
loss_r_bn_feature 215.613037109375
------------iteration 400----------
total loss 243.47463288070742
main criterion 45.91382660629336
weighted_aux_loss 197.56080627441406
loss_r_bn_feature 197.56080627441406
------------iteration 500----------
total loss 229.79551414584807
main criterion 51.84338096713715
weighted_aux_loss 177.95213317871094
loss_r_bn_feature 177.95213317871094
------------iteration 600----------
total loss 358.02362352875906
main criterion 96.63690477875905
weighted_aux_loss 261.38671875
loss_r_bn_feature 261.38671875
------------iteration 700----------
total loss 231.5722091774723
main criterion 56.438099680401976
weighted_aux_loss 175.1341094970703
loss_r_bn_feature 175.1341094970703
------------iteration 800----------
total loss 204.15695034244385
main criterion 45.18401943424073
weighted_aux_loss 158.97293090820312
loss_r_bn_feature 158.97293090820312
------------iteration 900----------
total loss 200.57237135407462
main criterion 36.21044813630118
weighted_aux_loss 164.36192321777344
loss_r_bn_feature 164.36192321777344
------------iteration 1000----------
total loss 267.3690155373579
main criterion 70.34258731470163
weighted_aux_loss 197.02642822265625
loss_r_bn_feature 197.02642822265625
------------iteration 1100----------
total loss 196.15495052941154
main criterion 49.3621191085131
weighted_aux_loss 146.79283142089844
loss_r_bn_feature 146.79283142089844
------------iteration 1200----------
total loss 186.66945215869723
main criterion 31.837497202642535
weighted_aux_loss 154.8319549560547
loss_r_bn_feature 154.8319549560547
------------iteration 1300----------
total loss 178.32159123750807
main criterion 30.824688771687747
weighted_aux_loss 147.4969024658203
loss_r_bn_feature 147.4969024658203
------------iteration 1400----------
total loss 177.33092591904926
main criterion 33.93370912217428
weighted_aux_loss 143.397216796875
loss_r_bn_feature 143.397216796875
------------iteration 1500----------
total loss 171.21153568952428
main criterion 38.748309371164915
weighted_aux_loss 132.46322631835938
loss_r_bn_feature 132.46322631835938
------------iteration 1600----------
total loss 173.50699212734995
main criterion 36.67296197598276
weighted_aux_loss 136.8340301513672
loss_r_bn_feature 136.8340301513672
------------iteration 1700----------
total loss 181.23518062302833
main criterion 50.03228450486428
weighted_aux_loss 131.20289611816406
loss_r_bn_feature 131.20289611816406
------------iteration 1800----------
total loss 176.69971694135512
main criterion 42.457956687448885
weighted_aux_loss 134.24176025390625
loss_r_bn_feature 134.24176025390625
------------iteration 1900----------
total loss 172.90397084404657
main criterion 42.016306049124694
weighted_aux_loss 130.88766479492188
loss_r_bn_feature 130.88766479492188
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 554.8421329521461
main criterion 132.20559730761485
weighted_aux_loss 422.63653564453125
loss_r_bn_feature 422.63653564453125
------------iteration 100----------
total loss 387.55452669853605
main criterion 89.29549349541107
weighted_aux_loss 298.259033203125
loss_r_bn_feature 298.259033203125
------------iteration 200----------
total loss 289.64489830634886
main criterion 60.74704064033324
weighted_aux_loss 228.89785766601562
loss_r_bn_feature 228.89785766601562
------------iteration 300----------
total loss 275.6908458458413
main criterion 62.94017445912257
weighted_aux_loss 212.75067138671875
loss_r_bn_feature 212.75067138671875
------------iteration 400----------
total loss 261.0463973520875
main criterion 48.04513087259533
weighted_aux_loss 213.0012664794922
loss_r_bn_feature 213.0012664794922
------------iteration 500----------
total loss 216.3507784471867
main criterion 38.15831934074141
weighted_aux_loss 178.1924591064453
loss_r_bn_feature 178.1924591064453
------------iteration 600----------
total loss 210.96446561837598
main criterion 38.59083890939162
weighted_aux_loss 172.37362670898438
loss_r_bn_feature 172.37362670898438
------------iteration 700----------
total loss 209.372262158346
main criterion 36.99701801772099
weighted_aux_loss 172.375244140625
loss_r_bn_feature 172.375244140625
------------iteration 800----------
total loss 211.30853053879986
main criterion 37.340406149151406
weighted_aux_loss 173.96812438964844
loss_r_bn_feature 173.96812438964844
------------iteration 900----------
total loss 195.45238196048925
main criterion 43.68695532474705
weighted_aux_loss 151.7654266357422
loss_r_bn_feature 151.7654266357422
------------iteration 1000----------
total loss 188.2514984403596
main criterion 44.338381985281465
weighted_aux_loss 143.91311645507812
loss_r_bn_feature 143.91311645507812
------------iteration 1100----------
total loss 190.91116424210205
main criterion 38.371216732336414
weighted_aux_loss 152.53994750976562
loss_r_bn_feature 152.53994750976562
------------iteration 1200----------
total loss 399.0223003307651
main criterion 115.48613700068698
weighted_aux_loss 283.5361633300781
loss_r_bn_feature 283.5361633300781
------------iteration 1300----------
total loss 370.4627010043317
main criterion 96.65874592620666
weighted_aux_loss 273.803955078125
loss_r_bn_feature 273.803955078125
------------iteration 1400----------
total loss 174.0074123749924
main criterion 35.09470790721898
weighted_aux_loss 138.91270446777344
loss_r_bn_feature 138.91270446777344
------------iteration 1500----------
total loss 184.8079942513255
main criterion 49.5535692024974
weighted_aux_loss 135.25442504882812
loss_r_bn_feature 135.25442504882812
------------iteration 1600----------
total loss 182.1170160748007
main criterion 38.156292197847556
weighted_aux_loss 143.96072387695312
loss_r_bn_feature 143.96072387695312
------------iteration 1700----------
total loss 172.09826003135282
main criterion 31.793068991313753
weighted_aux_loss 140.30519104003906
loss_r_bn_feature 140.30519104003906
------------iteration 1800----------
total loss 181.83281346481655
main criterion 43.449405872043116
weighted_aux_loss 138.38340759277344
loss_r_bn_feature 138.38340759277344
------------iteration 1900----------
total loss 176.04727018636882
main criterion 32.31405485433757
weighted_aux_loss 143.73321533203125
loss_r_bn_feature 143.73321533203125
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 499.6335244483945
main criterion 135.837503940582
weighted_aux_loss 363.7960205078125
loss_r_bn_feature 363.7960205078125
------------iteration 100----------
total loss 372.53518433130336
main criterion 93.74853271997522
weighted_aux_loss 278.7866516113281
loss_r_bn_feature 278.7866516113281
------------iteration 200----------
total loss 315.7216376788275
main criterion 80.34685130187437
weighted_aux_loss 235.37478637695312
loss_r_bn_feature 235.37478637695312
------------iteration 300----------
total loss 251.06173592589766
main criterion 53.38235360167892
weighted_aux_loss 197.67938232421875
loss_r_bn_feature 197.67938232421875
------------iteration 400----------
total loss 252.39686123960104
main criterion 47.90539090268698
weighted_aux_loss 204.49147033691406
loss_r_bn_feature 204.49147033691406
------------iteration 500----------
total loss 321.9726180235989
main criterion 90.03713218375515
weighted_aux_loss 231.93548583984375
loss_r_bn_feature 231.93548583984375
------------iteration 600----------
total loss 231.10079610227686
main criterion 43.17489278196437
weighted_aux_loss 187.9259033203125
loss_r_bn_feature 187.9259033203125
------------iteration 700----------
total loss 197.61807794975633
main criterion 40.64906855034227
weighted_aux_loss 156.96900939941406
loss_r_bn_feature 156.96900939941406
------------iteration 800----------
total loss 191.10001912998092
main criterion 39.42016378330123
weighted_aux_loss 151.6798553466797
loss_r_bn_feature 151.6798553466797
------------iteration 900----------
total loss 186.92711774342632
main criterion 37.58580914967632
weighted_aux_loss 149.34130859375
loss_r_bn_feature 149.34130859375
------------iteration 1000----------
total loss 180.89655097713873
main criterion 34.60330756893561
weighted_aux_loss 146.29324340820312
loss_r_bn_feature 146.29324340820312
------------iteration 1100----------
total loss 201.80477458894578
main criterion 51.008845633867644
weighted_aux_loss 150.79592895507812
loss_r_bn_feature 150.79592895507812
------------iteration 1200----------
total loss 201.79499616770815
main criterion 37.60059919505189
weighted_aux_loss 164.19439697265625
loss_r_bn_feature 164.19439697265625
------------iteration 1300----------
total loss 189.16789937817964
main criterion 35.11571431958589
weighted_aux_loss 154.05218505859375
loss_r_bn_feature 154.05218505859375
------------iteration 1400----------
total loss 174.3601009450482
main criterion 38.03789635520445
weighted_aux_loss 136.32220458984375
loss_r_bn_feature 136.32220458984375
------------iteration 1500----------
total loss 166.88814494643793
main criterion 36.83953044448481
weighted_aux_loss 130.04861450195312
loss_r_bn_feature 130.04861450195312
------------iteration 1600----------
total loss 181.6758470905152
main criterion 44.78183463934331
weighted_aux_loss 136.89401245117188
loss_r_bn_feature 136.89401245117188
------------iteration 1700----------
total loss 173.68000181418478
main criterion 35.06888731223166
weighted_aux_loss 138.61111450195312
loss_r_bn_feature 138.61111450195312
------------iteration 1800----------
total loss 179.50002585242012
main criterion 35.14853964636543
weighted_aux_loss 144.3514862060547
loss_r_bn_feature 144.3514862060547
------------iteration 1900----------
total loss 178.981227943772
main criterion 34.60380179631106
weighted_aux_loss 144.37742614746094
loss_r_bn_feature 144.37742614746094
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 547.1797797149482
main criterion 132.51858586729196
weighted_aux_loss 414.66119384765625
loss_r_bn_feature 414.66119384765625
------------iteration 100----------
total loss 427.6064744618189
main criterion 87.12124496963139
weighted_aux_loss 340.4852294921875
loss_r_bn_feature 340.4852294921875
------------iteration 200----------
total loss 335.0692756829537
main criterion 69.44888383725056
weighted_aux_loss 265.6203918457031
loss_r_bn_feature 265.6203918457031
------------iteration 300----------
total loss 308.05647795665266
main criterion 51.830464773058935
weighted_aux_loss 256.22601318359375
loss_r_bn_feature 256.22601318359375
------------iteration 400----------
total loss 333.9316867352068
main criterion 81.49588046079275
weighted_aux_loss 252.43580627441406
loss_r_bn_feature 252.43580627441406
------------iteration 500----------
total loss 255.18838514970378
main criterion 39.52645888017253
weighted_aux_loss 215.66192626953125
loss_r_bn_feature 215.66192626953125
------------iteration 600----------
total loss 252.57670918749852
main criterion 41.09059468554538
weighted_aux_loss 211.48611450195312
loss_r_bn_feature 211.48611450195312
------------iteration 700----------
total loss 227.57320003879306
main criterion 36.12301998508211
weighted_aux_loss 191.45018005371094
loss_r_bn_feature 191.45018005371094
------------iteration 800----------
total loss 223.77594469163958
main criterion 35.188343983631775
weighted_aux_loss 188.5876007080078
loss_r_bn_feature 188.5876007080078
------------iteration 900----------
total loss 207.72197120548083
main criterion 35.77391212344957
weighted_aux_loss 171.94805908203125
loss_r_bn_feature 171.94805908203125
------------iteration 1000----------
total loss 212.80113465154014
main criterion 34.47758728825888
weighted_aux_loss 178.32354736328125
loss_r_bn_feature 178.32354736328125
------------iteration 1100----------
total loss 410.7440466594635
main criterion 109.66637942313535
weighted_aux_loss 301.0776672363281
loss_r_bn_feature 301.0776672363281
------------iteration 1200----------
total loss 197.67169525543574
main criterion 47.555072330631056
weighted_aux_loss 150.1166229248047
loss_r_bn_feature 150.1166229248047
------------iteration 1300----------
total loss 206.8236448357483
main criterion 37.857916075982665
weighted_aux_loss 168.96572875976562
loss_r_bn_feature 168.96572875976562
------------iteration 1400----------
total loss 455.8513422782793
main criterion 117.4354487235918
weighted_aux_loss 338.4158935546875
loss_r_bn_feature 338.4158935546875
------------iteration 1500----------
total loss 197.246070879361
main criterion 38.89585115279852
weighted_aux_loss 158.3502197265625
loss_r_bn_feature 158.3502197265625
------------iteration 1600----------
total loss 191.774003691263
main criterion 43.371431059427074
weighted_aux_loss 148.40257263183594
loss_r_bn_feature 148.40257263183594
------------iteration 1700----------
total loss 234.68915363962446
main criterion 52.91448017771039
weighted_aux_loss 181.77467346191406
loss_r_bn_feature 181.77467346191406
------------iteration 1800----------
total loss 199.1950108247037
main criterion 31.385669394039628
weighted_aux_loss 167.80934143066406
loss_r_bn_feature 167.80934143066406
------------iteration 1900----------
total loss 193.25067597134736
main criterion 43.12570648892549
weighted_aux_loss 150.12496948242188
loss_r_bn_feature 150.12496948242188
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 576.5441356011405
main criterion 132.60456040582798
weighted_aux_loss 443.9395751953125
loss_r_bn_feature 443.9395751953125
------------iteration 100----------
total loss 445.1089581829458
main criterion 102.11545842708641
weighted_aux_loss 342.9934997558594
loss_r_bn_feature 342.9934997558594
------------iteration 200----------
total loss 362.13501841990546
main criterion 64.7030726191242
weighted_aux_loss 297.43194580078125
loss_r_bn_feature 297.43194580078125
------------iteration 300----------
total loss 318.6620380633856
main criterion 52.47859690127622
weighted_aux_loss 266.1834411621094
loss_r_bn_feature 266.1834411621094
------------iteration 400----------
total loss 276.2293683720005
main criterion 50.618757410086474
weighted_aux_loss 225.61061096191406
loss_r_bn_feature 225.61061096191406
------------iteration 500----------
total loss 332.2109150667691
main criterion 87.33887527184721
weighted_aux_loss 244.87203979492188
loss_r_bn_feature 244.87203979492188
------------iteration 600----------
total loss 259.3227609951239
main criterion 39.76422828028009
weighted_aux_loss 219.55853271484375
loss_r_bn_feature 219.55853271484375
------------iteration 700----------
total loss 253.85270633801787
main criterion 37.19102420911162
weighted_aux_loss 216.66168212890625
loss_r_bn_feature 216.66168212890625
------------iteration 800----------
total loss 236.08609008356325
main criterion 35.650344844305444
weighted_aux_loss 200.4357452392578
loss_r_bn_feature 200.4357452392578
------------iteration 900----------
total loss 260.70009702509714
main criterion 41.80654233759713
weighted_aux_loss 218.8935546875
loss_r_bn_feature 218.8935546875
------------iteration 1000----------
total loss 240.31777765856253
main criterion 36.25404169664848
weighted_aux_loss 204.06373596191406
loss_r_bn_feature 204.06373596191406
------------iteration 1100----------
total loss 220.07190137816548
main criterion 48.134553966056096
weighted_aux_loss 171.93734741210938
loss_r_bn_feature 171.93734741210938
------------iteration 1200----------
total loss 212.73115904454647
main criterion 32.311633898062105
weighted_aux_loss 180.41952514648438
loss_r_bn_feature 180.41952514648438
------------iteration 1300----------
total loss 222.93569722253682
main criterion 47.104398394411824
weighted_aux_loss 175.831298828125
loss_r_bn_feature 175.831298828125
------------iteration 1400----------
total loss 218.94893926595748
main criterion 32.32946294759811
weighted_aux_loss 186.61947631835938
loss_r_bn_feature 186.61947631835938
------------iteration 1500----------
total loss 207.41577107078996
main criterion 36.95089680321183
weighted_aux_loss 170.46487426757812
loss_r_bn_feature 170.46487426757812
------------iteration 1600----------
total loss 207.83065617159565
main criterion 31.622694135462837
weighted_aux_loss 176.2079620361328
loss_r_bn_feature 176.2079620361328
------------iteration 1700----------
total loss 224.48408894467153
main criterion 34.90164570736684
weighted_aux_loss 189.5824432373047
loss_r_bn_feature 189.5824432373047
------------iteration 1800----------
total loss 232.9192024005875
main criterion 48.32587964668125
weighted_aux_loss 184.59332275390625
loss_r_bn_feature 184.59332275390625
------------iteration 1900----------
total loss 228.09588054275622
main criterion 34.72336772537341
weighted_aux_loss 193.3725128173828
loss_r_bn_feature 193.3725128173828
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 571.243187765818
main criterion 131.81502614472424
weighted_aux_loss 439.42816162109375
loss_r_bn_feature 439.42816162109375
------------iteration 100----------
total loss 418.3059736683478
main criterion 83.95630325819155
weighted_aux_loss 334.34967041015625
loss_r_bn_feature 334.34967041015625
------------iteration 200----------
total loss 343.7899755652152
main criterion 62.524838846465165
weighted_aux_loss 281.26513671875
loss_r_bn_feature 281.26513671875
------------iteration 300----------
total loss 319.6932216225939
main criterion 55.18083148587516
weighted_aux_loss 264.51239013671875
loss_r_bn_feature 264.51239013671875
------------iteration 400----------
total loss 291.7400931309211
main criterion 48.638317007874214
weighted_aux_loss 243.10177612304688
loss_r_bn_feature 243.10177612304688
------------iteration 500----------
total loss 257.0803947338028
main criterion 40.237178791419964
weighted_aux_loss 216.8432159423828
loss_r_bn_feature 216.8432159423828
------------iteration 600----------
total loss 265.16845206779317
main criterion 52.26189689201189
weighted_aux_loss 212.90655517578125
loss_r_bn_feature 212.90655517578125
------------iteration 700----------
total loss 251.01686179473188
main criterion 54.19909751250532
weighted_aux_loss 196.81776428222656
loss_r_bn_feature 196.81776428222656
------------iteration 800----------
total loss 226.70581651367593
main criterion 42.08104539551186
weighted_aux_loss 184.62477111816406
loss_r_bn_feature 184.62477111816406
------------iteration 900----------
total loss 369.29680367550606
main criterion 94.99361153683418
weighted_aux_loss 274.3031921386719
loss_r_bn_feature 274.3031921386719
------------iteration 1000----------
total loss 252.51156310361335
main criterion 56.3167999200196
weighted_aux_loss 196.19476318359375
loss_r_bn_feature 196.19476318359375
------------iteration 1100----------
total loss 227.507994988953
main criterion 45.61511168817174
weighted_aux_loss 181.89288330078125
loss_r_bn_feature 181.89288330078125
------------iteration 1200----------
total loss 215.02237234505444
main criterion 36.828356842124755
weighted_aux_loss 178.1940155029297
loss_r_bn_feature 178.1940155029297
------------iteration 1300----------
total loss 208.35275858935108
main criterion 31.459753218257344
weighted_aux_loss 176.89300537109375
loss_r_bn_feature 176.89300537109375
------------iteration 1400----------
total loss 207.9405933596536
main criterion 35.81862985867705
weighted_aux_loss 172.12196350097656
loss_r_bn_feature 172.12196350097656
------------iteration 1500----------
total loss 215.8362684654727
main criterion 32.219492952777394
weighted_aux_loss 183.6167755126953
loss_r_bn_feature 183.6167755126953
------------iteration 1600----------
total loss 197.89101344320216
main criterion 36.32129603597561
weighted_aux_loss 161.56971740722656
loss_r_bn_feature 161.56971740722656
------------iteration 1700----------
total loss 197.64654111436067
main criterion 36.00031613877475
weighted_aux_loss 161.64622497558594
loss_r_bn_feature 161.64622497558594
------------iteration 1800----------
total loss 211.01704659628012
main criterion 37.00760140585043
weighted_aux_loss 174.0094451904297
loss_r_bn_feature 174.0094451904297
------------iteration 1900----------
total loss 208.97881519989522
main criterion 41.64241993622335
weighted_aux_loss 167.33639526367188
loss_r_bn_feature 167.33639526367188
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 565.7219877551264
main criterion 140.64422896606393
weighted_aux_loss 425.0777587890625
loss_r_bn_feature 425.0777587890625
------------iteration 100----------
total loss 433.0462909870423
main criterion 84.25973092844855
weighted_aux_loss 348.78656005859375
loss_r_bn_feature 348.78656005859375
------------iteration 200----------
total loss 385.6227805424574
main criterion 69.86618874558239
weighted_aux_loss 315.756591796875
loss_r_bn_feature 315.756591796875
------------iteration 300----------
total loss 457.06664973856357
main criterion 113.52120906473544
weighted_aux_loss 343.5454406738281
loss_r_bn_feature 343.5454406738281
------------iteration 400----------
total loss 277.9977805765171
main criterion 47.67069317417333
weighted_aux_loss 230.32708740234375
loss_r_bn_feature 230.32708740234375
------------iteration 500----------
total loss 255.89356901601002
main criterion 49.60114958241626
weighted_aux_loss 206.29241943359375
loss_r_bn_feature 206.29241943359375
------------iteration 600----------
total loss 262.7923404094218
main criterion 40.73957551684369
weighted_aux_loss 222.05276489257812
loss_r_bn_feature 222.05276489257812
------------iteration 700----------
total loss 234.43973295448927
main criterion 35.19397489784866
weighted_aux_loss 199.24575805664062
loss_r_bn_feature 199.24575805664062
------------iteration 800----------
total loss 239.21673166043863
main criterion 39.727641694618306
weighted_aux_loss 199.4890899658203
loss_r_bn_feature 199.4890899658203
------------iteration 900----------
total loss 223.41809644584848
main criterion 34.26120557670785
weighted_aux_loss 189.15689086914062
loss_r_bn_feature 189.15689086914062
------------iteration 1000----------
total loss 227.41097611549486
main criterion 34.18682450416673
weighted_aux_loss 193.22415161132812
loss_r_bn_feature 193.22415161132812
------------iteration 1100----------
total loss 206.55532532221872
main criterion 33.08416443354686
weighted_aux_loss 173.47116088867188
loss_r_bn_feature 173.47116088867188
------------iteration 1200----------
total loss 204.16231721255838
main criterion 35.35369294498026
weighted_aux_loss 168.80862426757812
loss_r_bn_feature 168.80862426757812
------------iteration 1300----------
total loss 201.20724418618408
main criterion 37.26959159829346
weighted_aux_loss 163.93765258789062
loss_r_bn_feature 163.93765258789062
------------iteration 1400----------
total loss 199.43435922309604
main criterion 32.201494843213226
weighted_aux_loss 167.2328643798828
loss_r_bn_feature 167.2328643798828
------------iteration 1500----------
total loss 343.48695760056376
main criterion 75.80879597946998
weighted_aux_loss 267.67816162109375
loss_r_bn_feature 267.67816162109375
------------iteration 1600----------
total loss 214.22922787688958
main criterion 37.48020443938957
weighted_aux_loss 176.7490234375
loss_r_bn_feature 176.7490234375
------------iteration 1700----------
total loss 194.7390246612282
main criterion 36.14489319150162
weighted_aux_loss 158.59413146972656
loss_r_bn_feature 158.59413146972656
------------iteration 1800----------
total loss 363.4630829865075
main criterion 79.38663645330442
weighted_aux_loss 284.0764465332031
loss_r_bn_feature 284.0764465332031
------------iteration 1900----------
total loss 198.19195908458386
main criterion 41.62034958751354
weighted_aux_loss 156.5716094970703
loss_r_bn_feature 156.5716094970703
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 590.9116018279014
main criterion 130.6238515837608
weighted_aux_loss 460.2877502441406
loss_r_bn_feature 460.2877502441406
------------iteration 100----------
total loss 453.2347540304444
main criterion 100.28910583708502
weighted_aux_loss 352.9456481933594
loss_r_bn_feature 352.9456481933594
------------iteration 200----------
total loss 348.26515313267913
main criterion 61.19169732213226
weighted_aux_loss 287.0734558105469
loss_r_bn_feature 287.0734558105469
------------iteration 300----------
total loss 399.33762873833354
main criterion 88.1435064238804
weighted_aux_loss 311.1941223144531
loss_r_bn_feature 311.1941223144531
------------iteration 400----------
total loss 301.50982278814445
main criterion 47.22684854498038
weighted_aux_loss 254.28297424316406
loss_r_bn_feature 254.28297424316406
------------iteration 500----------
total loss 279.60368656966233
main criterion 41.19890141341232
weighted_aux_loss 238.40478515625
loss_r_bn_feature 238.40478515625
------------iteration 600----------
total loss 259.351974931946
main criterion 41.458969560852275
weighted_aux_loss 217.89300537109375
loss_r_bn_feature 217.89300537109375
------------iteration 700----------
total loss 318.4622098172141
main criterion 70.13196384553444
weighted_aux_loss 248.3302459716797
loss_r_bn_feature 248.3302459716797
------------iteration 800----------
total loss 251.0203852078361
main criterion 39.08967536896893
weighted_aux_loss 211.9307098388672
loss_r_bn_feature 211.9307098388672
------------iteration 900----------
total loss 236.177441105001
main criterion 41.85999725734475
weighted_aux_loss 194.31744384765625
loss_r_bn_feature 194.31744384765625
------------iteration 1000----------
total loss 253.07192407476484
main criterion 36.763513430233594
weighted_aux_loss 216.30841064453125
loss_r_bn_feature 216.30841064453125
------------iteration 1100----------
total loss 231.6794477616387
main criterion 36.07255994425588
weighted_aux_loss 195.6068878173828
loss_r_bn_feature 195.6068878173828
------------iteration 1200----------
total loss 224.99494670336668
main criterion 34.22872661059325
weighted_aux_loss 190.76622009277344
loss_r_bn_feature 190.76622009277344
------------iteration 1300----------
total loss 222.71618664302386
main criterion 43.648407102008235
weighted_aux_loss 179.06777954101562
loss_r_bn_feature 179.06777954101562
------------iteration 1400----------
total loss 219.4033828109011
main criterion 32.57402184898703
weighted_aux_loss 186.82936096191406
loss_r_bn_feature 186.82936096191406
------------iteration 1500----------
total loss 375.84218334543107
main criterion 80.85616039621229
weighted_aux_loss 294.98602294921875
loss_r_bn_feature 294.98602294921875
------------iteration 1600----------
total loss 214.01771899925814
main criterion 38.52753040062534
weighted_aux_loss 175.4901885986328
loss_r_bn_feature 175.4901885986328
------------iteration 1700----------
total loss 291.8927762713122
main criterion 61.27127053400751
weighted_aux_loss 230.6215057373047
loss_r_bn_feature 230.6215057373047
------------iteration 1800----------
total loss 221.40123959038976
main criterion 36.40546627496008
weighted_aux_loss 184.9957733154297
loss_r_bn_feature 184.9957733154297
------------iteration 1900----------
total loss 216.29949325280865
main criterion 38.45751327233989
weighted_aux_loss 177.84197998046875
loss_r_bn_feature 177.84197998046875
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 574.5118865382987
main criterion 138.35063165548618
weighted_aux_loss 436.1612548828125
loss_r_bn_feature 436.1612548828125
------------iteration 100----------
total loss 464.53915730081115
main criterion 110.3268159922174
weighted_aux_loss 354.21234130859375
loss_r_bn_feature 354.21234130859375
------------iteration 200----------
total loss 434.3866218272177
main criterion 104.83132397565522
weighted_aux_loss 329.5552978515625
loss_r_bn_feature 329.5552978515625
------------iteration 300----------
total loss 302.0202973292296
main criterion 49.223544399542135
weighted_aux_loss 252.7967529296875
loss_r_bn_feature 252.7967529296875
------------iteration 400----------
total loss 300.33308717340657
main criterion 48.386157241765936
weighted_aux_loss 251.94692993164062
loss_r_bn_feature 251.94692993164062
------------iteration 500----------
total loss 271.3201939862635
main criterion 46.3699681561854
weighted_aux_loss 224.95022583007812
loss_r_bn_feature 224.95022583007812
------------iteration 600----------
total loss 276.6731934887953
main criterion 42.487692390162486
weighted_aux_loss 234.1855010986328
loss_r_bn_feature 234.1855010986328
------------iteration 700----------
total loss 245.18232728101157
main criterion 36.87933350659752
weighted_aux_loss 208.30299377441406
loss_r_bn_feature 208.30299377441406
------------iteration 800----------
total loss 240.74427249344586
main criterion 39.016916536414605
weighted_aux_loss 201.72735595703125
loss_r_bn_feature 201.72735595703125
------------iteration 900----------
total loss 245.89239025925082
main criterion 38.21461010788363
weighted_aux_loss 207.6777801513672
loss_r_bn_feature 207.6777801513672
------------iteration 1000----------
total loss 228.45381165705615
main criterion 36.01107789240772
weighted_aux_loss 192.44273376464844
loss_r_bn_feature 192.44273376464844
------------iteration 1100----------
total loss 425.2970036802576
main criterion 100.63510792830445
weighted_aux_loss 324.6618957519531
loss_r_bn_feature 324.6618957519531
------------iteration 1200----------
total loss 205.4273523410222
main criterion 38.51164189180347
weighted_aux_loss 166.91571044921875
loss_r_bn_feature 166.91571044921875
------------iteration 1300----------
total loss 211.14155032062192
main criterion 37.15270449542661
weighted_aux_loss 173.9888458251953
loss_r_bn_feature 173.9888458251953
------------iteration 1400----------
total loss 224.74595498780914
main criterion 35.5682816479654
weighted_aux_loss 189.17767333984375
loss_r_bn_feature 189.17767333984375
------------iteration 1500----------
total loss 236.57668255763812
main criterion 51.36246441798969
weighted_aux_loss 185.21421813964844
loss_r_bn_feature 185.21421813964844
------------iteration 1600----------
total loss 212.46400409258774
main criterion 32.773726992978375
weighted_aux_loss 179.69027709960938
loss_r_bn_feature 179.69027709960938
------------iteration 1700----------
total loss 196.5039728882114
main criterion 44.46812999270358
weighted_aux_loss 152.0358428955078
loss_r_bn_feature 152.0358428955078
------------iteration 1800----------
total loss 216.23492015680958
main criterion 34.534328115793954
weighted_aux_loss 181.70059204101562
loss_r_bn_feature 181.70059204101562
------------iteration 1900----------
total loss 207.48275862885865
main criterion 32.24650679780397
weighted_aux_loss 175.2362518310547
loss_r_bn_feature 175.2362518310547
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 582.1031734094486
main criterion 128.1170283899173
weighted_aux_loss 453.98614501953125
loss_r_bn_feature 453.98614501953125
------------iteration 100----------
total loss 430.2251184751876
main criterion 84.94859869979699
weighted_aux_loss 345.2765197753906
loss_r_bn_feature 345.2765197753906
------------iteration 200----------
total loss 340.97303615664646
main criterion 65.31162868594332
weighted_aux_loss 275.6614074707031
loss_r_bn_feature 275.6614074707031
------------iteration 300----------
total loss 431.84780973222394
main criterion 98.4085397126927
weighted_aux_loss 333.43927001953125
loss_r_bn_feature 333.43927001953125
------------iteration 400----------
total loss 280.22680735930106
main criterion 45.47497630461357
weighted_aux_loss 234.7518310546875
loss_r_bn_feature 234.7518310546875
------------iteration 500----------
total loss 346.132822885807
main criterion 76.03730286627577
weighted_aux_loss 270.09552001953125
loss_r_bn_feature 270.09552001953125
------------iteration 600----------
total loss 280.2526197526691
main criterion 53.38163281419253
weighted_aux_loss 226.87098693847656
loss_r_bn_feature 226.87098693847656
------------iteration 700----------
total loss 240.0067881818693
main criterion 33.898481297103686
weighted_aux_loss 206.10830688476562
loss_r_bn_feature 206.10830688476562
------------iteration 800----------
total loss 266.5393252494781
main criterion 54.69429351119687
weighted_aux_loss 211.84503173828125
loss_r_bn_feature 211.84503173828125
------------iteration 900----------
total loss 224.39995914498948
main criterion 38.56142765084886
weighted_aux_loss 185.83853149414062
loss_r_bn_feature 185.83853149414062
------------iteration 1000----------
total loss 228.86458677233665
main criterion 34.23813718737572
weighted_aux_loss 194.62644958496094
loss_r_bn_feature 194.62644958496094
------------iteration 1100----------
total loss 293.1978197134789
main criterion 64.8818254507836
weighted_aux_loss 228.3159942626953
loss_r_bn_feature 228.3159942626953
------------iteration 1200----------
total loss 230.31446027600853
main criterion 47.12935590589134
weighted_aux_loss 183.1851043701172
loss_r_bn_feature 183.1851043701172
------------iteration 1300----------
total loss 212.4544481421357
main criterion 36.66280690678415
weighted_aux_loss 175.79164123535156
loss_r_bn_feature 175.79164123535156
------------iteration 1400----------
total loss 203.99609074411978
main criterion 36.973492477518214
weighted_aux_loss 167.02259826660156
loss_r_bn_feature 167.02259826660156
------------iteration 1500----------
total loss 214.51518174527325
main criterion 46.72791978238264
weighted_aux_loss 167.78726196289062
loss_r_bn_feature 167.78726196289062
------------iteration 1600----------
total loss 212.66103978794266
main criterion 36.06667943638016
weighted_aux_loss 176.5943603515625
loss_r_bn_feature 176.5943603515625
------------iteration 1700----------
total loss 207.92292394936752
main criterion 31.16590490639876
weighted_aux_loss 176.75701904296875
loss_r_bn_feature 176.75701904296875
------------iteration 1800----------
total loss 200.2008346932647
main criterion 39.449308814358474
weighted_aux_loss 160.75152587890625
loss_r_bn_feature 160.75152587890625
------------iteration 1900----------
total loss 218.26981985619685
main criterion 46.883345246821854
weighted_aux_loss 171.386474609375
loss_r_bn_feature 171.386474609375
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
per-class-batchnorm:  True
------------iteration 0----------
total loss 594.4071191769242
main criterion 130.90156497770545
weighted_aux_loss 463.50555419921875
loss_r_bn_feature 463.50555419921875
------------iteration 100----------
total loss 451.6871185352453
main criterion 88.28956604501093
weighted_aux_loss 363.3975524902344
loss_r_bn_feature 363.3975524902344
------------iteration 200----------
total loss 359.22955331012156
main criterion 61.53088387652778
weighted_aux_loss 297.69866943359375
loss_r_bn_feature 297.69866943359375
------------iteration 300----------
total loss 363.18948976570914
main criterion 79.28632204109978
weighted_aux_loss 283.9031677246094
loss_r_bn_feature 283.9031677246094
------------iteration 400----------
total loss 357.20327653602004
main criterion 75.7906178446138
weighted_aux_loss 281.41265869140625
loss_r_bn_feature 281.41265869140625
------------iteration 500----------
total loss 270.35678612363154
main criterion 42.38139855038932
weighted_aux_loss 227.9753875732422
loss_r_bn_feature 227.9753875732422
------------iteration 600----------
total loss 275.54201588083004
main criterion 56.113716930634716
weighted_aux_loss 219.4282989501953
loss_r_bn_feature 219.4282989501953
------------iteration 700----------
total loss 265.42794251765895
main criterion 39.48525452937767
weighted_aux_loss 225.94268798828125
loss_r_bn_feature 225.94268798828125
------------iteration 800----------
total loss 244.44841732622288
main criterion 51.30493893266819
weighted_aux_loss 193.1434783935547
loss_r_bn_feature 193.1434783935547
------------iteration 900----------
total loss 232.95754452682655
main criterion 51.74318905807655
weighted_aux_loss 181.21435546875
loss_r_bn_feature 181.21435546875
Traceback (most recent call last):
  File "data_synthesis.py", line 371, in <module>
    main_syn(args, bc)
  File "data_synthesis.py", line 295, in main_syn
    get_images(args, model_teacher, hook_for_display, bc=bc)
  File "data_synthesis.py", line 168, in get_images
    if best_cost > loss.item() or iteration == 1:
KeyboardInterrupt
